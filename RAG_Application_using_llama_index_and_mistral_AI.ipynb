{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6043ffa6f2ab48a5a3ad300068105335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d18a3ae0b97047e38f2863d274daca0b",
              "IPY_MODEL_a7256eeeb67045d2922786c603d45c8b",
              "IPY_MODEL_2618025450084e509d50862e59e72a19"
            ],
            "layout": "IPY_MODEL_a18f832d261d41f1885c8a218f8ad770"
          }
        },
        "d18a3ae0b97047e38f2863d274daca0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d6507c379c640ac98e75c250cda13c8",
            "placeholder": "​",
            "style": "IPY_MODEL_c2e7c9ac43a042958ee56f1de3f21385",
            "value": "config.json: 100%"
          }
        },
        "a7256eeeb67045d2922786c603d45c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba2ba1047bd44688a22667b81a014fc",
            "max": 601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d55369c1412f4fa1801b37527db501a5",
            "value": 601
          }
        },
        "2618025450084e509d50862e59e72a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23307169928a449784a016eaf431c415",
            "placeholder": "​",
            "style": "IPY_MODEL_4c7992507637491a8ab5d25ac07e4ca2",
            "value": " 601/601 [00:00&lt;00:00, 32.4kB/s]"
          }
        },
        "a18f832d261d41f1885c8a218f8ad770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6507c379c640ac98e75c250cda13c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e7c9ac43a042958ee56f1de3f21385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba2ba1047bd44688a22667b81a014fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d55369c1412f4fa1801b37527db501a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23307169928a449784a016eaf431c415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c7992507637491a8ab5d25ac07e4ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a8e5618eecd492c808172c6c1c640a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cd1eb3e1bc84b56a411d102aefa6ec6",
              "IPY_MODEL_883a8185c6f94d209437eac8fbbfe4ab",
              "IPY_MODEL_595d8eaa84934e8e9e3bad3052d033bf"
            ],
            "layout": "IPY_MODEL_d9a9f1481ede4f4bbe5030fc0cdf4211"
          }
        },
        "7cd1eb3e1bc84b56a411d102aefa6ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdc9e746098a4436adff87a5ea616b9e",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e6b5b191c94e96848bed3feebb5a99",
            "value": "model.safetensors.index.json: "
          }
        },
        "883a8185c6f94d209437eac8fbbfe4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_567209646e0d42b7951ce4c6dd836510",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaceec3c36014c6dbba82ba34ff08c9b",
            "value": 1
          }
        },
        "595d8eaa84934e8e9e3bad3052d033bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94970801e4b45448a15f4248d65d933",
            "placeholder": "​",
            "style": "IPY_MODEL_da33d451ce5c44f7a54a65d2d4144908",
            "value": " 23.9k/? [00:00&lt;00:00, 1.71MB/s]"
          }
        },
        "d9a9f1481ede4f4bbe5030fc0cdf4211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc9e746098a4436adff87a5ea616b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e6b5b191c94e96848bed3feebb5a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "567209646e0d42b7951ce4c6dd836510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "aaceec3c36014c6dbba82ba34ff08c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a94970801e4b45448a15f4248d65d933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da33d451ce5c44f7a54a65d2d4144908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2459babf83b4c84aa7d3c90d627e8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9e60e37fad548cbb9ec7eaa6e5d98f8",
              "IPY_MODEL_2a2ce18dc13145c2a5c6e64553685cab",
              "IPY_MODEL_f88735c3408248a0a6fe7c5e6dc5a25e"
            ],
            "layout": "IPY_MODEL_57974709fcea45e48601a6ddd0d1c4a2"
          }
        },
        "e9e60e37fad548cbb9ec7eaa6e5d98f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f18a52e5204e1b802f8ad1a51fcd16",
            "placeholder": "​",
            "style": "IPY_MODEL_1a55ea9e611543afbe9e98032bef97fc",
            "value": "Fetching 3 files: 100%"
          }
        },
        "2a2ce18dc13145c2a5c6e64553685cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_224176adcd834b9b9845876be6ef555a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0af84fcfd4545a19a875a01325a365b",
            "value": 3
          }
        },
        "f88735c3408248a0a6fe7c5e6dc5a25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1630dc722e1c4d0da1f4fd05877eaff4",
            "placeholder": "​",
            "style": "IPY_MODEL_b59dcc2bb3284301826f8c4ad998c55f",
            "value": " 3/3 [04:06&lt;00:00, 101.45s/it]"
          }
        },
        "57974709fcea45e48601a6ddd0d1c4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f18a52e5204e1b802f8ad1a51fcd16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a55ea9e611543afbe9e98032bef97fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "224176adcd834b9b9845876be6ef555a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0af84fcfd4545a19a875a01325a365b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1630dc722e1c4d0da1f4fd05877eaff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59dcc2bb3284301826f8c4ad998c55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "639e3877b74d42f59d0b4df122a979bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c80824fd0b346249169a006a1bb2cee",
              "IPY_MODEL_e9024ace610a4c7ab15fae19b2a1ab44",
              "IPY_MODEL_aa735e30b818407eaea6e2bf6230403b"
            ],
            "layout": "IPY_MODEL_6ba9e8e14fcc42f5b0e66edfb0ed6a84"
          }
        },
        "5c80824fd0b346249169a006a1bb2cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676cfc531f1947029bd6c92f2efbedc5",
            "placeholder": "​",
            "style": "IPY_MODEL_f4cb87f786964e9988db38b37db8d8ec",
            "value": "model-00003-of-00003.safetensors: 100%"
          }
        },
        "e9024ace610a4c7ab15fae19b2a1ab44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b210cfcd23941f8a3c709c6de6e6ca0",
            "max": 4546807800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9339162d01d48aca659ec5cdcb2ffa4",
            "value": 4546807800
          }
        },
        "aa735e30b818407eaea6e2bf6230403b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae8cac6d3c44fd2b23b12483c8edf74",
            "placeholder": "​",
            "style": "IPY_MODEL_78c35b36bbf84ad3bdd9cacb2c03d241",
            "value": " 4.55G/4.55G [04:04&lt;00:00, 31.7MB/s]"
          }
        },
        "6ba9e8e14fcc42f5b0e66edfb0ed6a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "676cfc531f1947029bd6c92f2efbedc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4cb87f786964e9988db38b37db8d8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b210cfcd23941f8a3c709c6de6e6ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9339162d01d48aca659ec5cdcb2ffa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ae8cac6d3c44fd2b23b12483c8edf74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c35b36bbf84ad3bdd9cacb2c03d241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5acf88c93a748219ac421085c13f913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e1937a443994d50833ee50ceb0d4358",
              "IPY_MODEL_ef0a4658c3634e52bc415ead83ce94e2",
              "IPY_MODEL_38ac0c8e3c3943709f2dbc5959338d6b"
            ],
            "layout": "IPY_MODEL_6a128243b98949b48612784b2d560a53"
          }
        },
        "7e1937a443994d50833ee50ceb0d4358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b16f1b6e88284adcb0913d425ef6ce18",
            "placeholder": "​",
            "style": "IPY_MODEL_15c05ae077c9410dbd9955713cf45df8",
            "value": "model-00002-of-00003.safetensors: 100%"
          }
        },
        "ef0a4658c3634e52bc415ead83ce94e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_850a804e1fc64213bf41abb78258115d",
            "max": 4999819336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60fdc94e59db480ebf339d7dce0a5a3f",
            "value": 4999819336
          }
        },
        "38ac0c8e3c3943709f2dbc5959338d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9da93537920449bbe1e49cf3b5af756",
            "placeholder": "​",
            "style": "IPY_MODEL_dcea51bf756d4b54bb0fac398d19141d",
            "value": " 5.00G/5.00G [04:05&lt;00:00, 94.9MB/s]"
          }
        },
        "6a128243b98949b48612784b2d560a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b16f1b6e88284adcb0913d425ef6ce18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c05ae077c9410dbd9955713cf45df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "850a804e1fc64213bf41abb78258115d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60fdc94e59db480ebf339d7dce0a5a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9da93537920449bbe1e49cf3b5af756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcea51bf756d4b54bb0fac398d19141d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c122a3b87d3431dad1c6f43efc30322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9657d916144d48aa9b17a507eea9b610",
              "IPY_MODEL_b19b133a21134bb5a9c1a6ec247119d6",
              "IPY_MODEL_18671052a3f4421eafac92a6b4319350"
            ],
            "layout": "IPY_MODEL_f7a357c5dffe4d8cb54e38729483f0db"
          }
        },
        "9657d916144d48aa9b17a507eea9b610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e683e6a06065445583630cfa6b4cf952",
            "placeholder": "​",
            "style": "IPY_MODEL_719183f142b146589ce96aac5033c3ee",
            "value": "model-00001-of-00003.safetensors: 100%"
          }
        },
        "b19b133a21134bb5a9c1a6ec247119d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311f9567f81e44119a429e9a4cfe25f1",
            "max": 4949453792,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16296035f7b0493b927ca6836ea6a82d",
            "value": 4949453792
          }
        },
        "18671052a3f4421eafac92a6b4319350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e5e5e4dc58a43919e46091c429918e8",
            "placeholder": "​",
            "style": "IPY_MODEL_a81454f0306e4b8f9876d10af5768edd",
            "value": " 4.95G/4.95G [04:05&lt;00:00, 56.7MB/s]"
          }
        },
        "f7a357c5dffe4d8cb54e38729483f0db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e683e6a06065445583630cfa6b4cf952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719183f142b146589ce96aac5033c3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "311f9567f81e44119a429e9a4cfe25f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16296035f7b0493b927ca6836ea6a82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e5e5e4dc58a43919e46091c429918e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81454f0306e4b8f9876d10af5768edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65340f070f784a128101dd47ad3ae9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_168ab778127948cdaf7cfe718db69f39",
              "IPY_MODEL_7a1592477df341e886aca61d86c99deb",
              "IPY_MODEL_0e244e864864430ea2447b812619019f"
            ],
            "layout": "IPY_MODEL_ba1e63005a664d78b71b22c1033c75d9"
          }
        },
        "168ab778127948cdaf7cfe718db69f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de36d6f95fa54f06a6cb39c5029de8dd",
            "placeholder": "​",
            "style": "IPY_MODEL_506d9085b76c4b5a8bf7da8c0fdeed65",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7a1592477df341e886aca61d86c99deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e84362f0bc440dcb51137a0148e89fe",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9f05872ad0642f2bd6998e453a93a99",
            "value": 3
          }
        },
        "0e244e864864430ea2447b812619019f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a59b613e2d04ad5a7a066002f8a9c64",
            "placeholder": "​",
            "style": "IPY_MODEL_1924b2578d3d47749360271052438df6",
            "value": " 3/3 [01:20&lt;00:00, 25.96s/it]"
          }
        },
        "ba1e63005a664d78b71b22c1033c75d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de36d6f95fa54f06a6cb39c5029de8dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506d9085b76c4b5a8bf7da8c0fdeed65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e84362f0bc440dcb51137a0148e89fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f05872ad0642f2bd6998e453a93a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a59b613e2d04ad5a7a066002f8a9c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1924b2578d3d47749360271052438df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f19c79637a443d8a6b790974958a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62ed5ea610bb43cb9fc88891f51373e1",
              "IPY_MODEL_9a3ae63f05084815a48e40692a3369f1",
              "IPY_MODEL_11421bb6a3384f2d961222829f30fc30"
            ],
            "layout": "IPY_MODEL_e7a910d531744c1fa5884e63b69e0de3"
          }
        },
        "62ed5ea610bb43cb9fc88891f51373e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37d219ff50544173a7d1e081909d4cef",
            "placeholder": "​",
            "style": "IPY_MODEL_807e36083aea4e9c883e46031ef83b82",
            "value": "generation_config.json: 100%"
          }
        },
        "9a3ae63f05084815a48e40692a3369f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_608e707293b94936bf3cddecf295c38c",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c1ba02856a542ec990d13c2e47498ea",
            "value": 116
          }
        },
        "11421bb6a3384f2d961222829f30fc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_132582e0dcab47b4ab3e73c9b098b833",
            "placeholder": "​",
            "style": "IPY_MODEL_355ed904ea21466e8c155723f5cdc311",
            "value": " 116/116 [00:00&lt;00:00, 11.7kB/s]"
          }
        },
        "e7a910d531744c1fa5884e63b69e0de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d219ff50544173a7d1e081909d4cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807e36083aea4e9c883e46031ef83b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "608e707293b94936bf3cddecf295c38c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1ba02856a542ec990d13c2e47498ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "132582e0dcab47b4ab3e73c9b098b833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355ed904ea21466e8c155723f5cdc311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76782c67287246cd9a562e503ddad9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e72423206da64e7c92a713bc07723a6e",
              "IPY_MODEL_01d9567baf934666ac4dffd4db977a93",
              "IPY_MODEL_45a05dc84fdc4e80adabe821b3df0481"
            ],
            "layout": "IPY_MODEL_fdbc66200d6648dca5d885ce81440b46"
          }
        },
        "e72423206da64e7c92a713bc07723a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f304a5b8224d788fdbdea6989432b6",
            "placeholder": "​",
            "style": "IPY_MODEL_0d94b83621cc4decbda7e4f8611d19bc",
            "value": "tokenizer_config.json: "
          }
        },
        "01d9567baf934666ac4dffd4db977a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f8ea1be3b6049c19210188f70e72a69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8703a3374f5744c181d3e53d4c9000f8",
            "value": 1
          }
        },
        "45a05dc84fdc4e80adabe821b3df0481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87c59cbb62d74d14bebac4110d468edd",
            "placeholder": "​",
            "style": "IPY_MODEL_94b626c9e40e441ca04c817ff29177fc",
            "value": " 141k/? [00:00&lt;00:00, 6.90MB/s]"
          }
        },
        "fdbc66200d6648dca5d885ce81440b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f304a5b8224d788fdbdea6989432b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d94b83621cc4decbda7e4f8611d19bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8ea1be3b6049c19210188f70e72a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8703a3374f5744c181d3e53d4c9000f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87c59cbb62d74d14bebac4110d468edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94b626c9e40e441ca04c817ff29177fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bca6fe1cf4b74881b7d8e544111c14ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd506f76c1e540d39da476a48b33c580",
              "IPY_MODEL_24e0ae4b5eec4cbda2b5d2fe4321fe1c",
              "IPY_MODEL_b3e28db757174eb684fd8e7efb08d800"
            ],
            "layout": "IPY_MODEL_0822ae4ed4004e2eb3b63e703eab355c"
          }
        },
        "bd506f76c1e540d39da476a48b33c580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df555693bb7543a79c782196e3436646",
            "placeholder": "​",
            "style": "IPY_MODEL_18f1eb2fe90b4d549a7559db0c306ceb",
            "value": "tokenizer.model: 100%"
          }
        },
        "24e0ae4b5eec4cbda2b5d2fe4321fe1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e66948afeb5490eb2cfa524dda37440",
            "max": 587404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c104aab5af4402bbe7c16540d75dcc2",
            "value": 587404
          }
        },
        "b3e28db757174eb684fd8e7efb08d800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d5cf23d437b45be86b9f62b04c62a67",
            "placeholder": "​",
            "style": "IPY_MODEL_0c6479cf0fb645f0b55ff5aace7f5b6d",
            "value": " 587k/587k [00:01&lt;00:00, 379kB/s]"
          }
        },
        "0822ae4ed4004e2eb3b63e703eab355c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df555693bb7543a79c782196e3436646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f1eb2fe90b4d549a7559db0c306ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e66948afeb5490eb2cfa524dda37440": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c104aab5af4402bbe7c16540d75dcc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d5cf23d437b45be86b9f62b04c62a67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6479cf0fb645f0b55ff5aace7f5b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ced1ebb520fa4c4d89b6bedad107079a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd692fcfec224a36a15235eb7855a610",
              "IPY_MODEL_67b45ed95f5b4e4da67e188c75744342",
              "IPY_MODEL_ad58218dd13944d6810c3c6ae9dd4155"
            ],
            "layout": "IPY_MODEL_bacbaa4e2e714e07af6d927f45f024b1"
          }
        },
        "cd692fcfec224a36a15235eb7855a610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e59b20e5734d438b887cbdbae93f06ec",
            "placeholder": "​",
            "style": "IPY_MODEL_c32b13f27cf74db393110ea6575dd94c",
            "value": "tokenizer.json: "
          }
        },
        "67b45ed95f5b4e4da67e188c75744342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_015bc83851a145adab8632a384ab5d8f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d33463937cff44859dbe569e177ed5d4",
            "value": 1
          }
        },
        "ad58218dd13944d6810c3c6ae9dd4155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67ac20160f9d4861af76fb7b7a712601",
            "placeholder": "​",
            "style": "IPY_MODEL_9e325ad79ca547f787c0e0af4b6538c0",
            "value": " 1.96M/? [00:00&lt;00:00, 72.0MB/s]"
          }
        },
        "bacbaa4e2e714e07af6d927f45f024b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59b20e5734d438b887cbdbae93f06ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32b13f27cf74db393110ea6575dd94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "015bc83851a145adab8632a384ab5d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d33463937cff44859dbe569e177ed5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67ac20160f9d4861af76fb7b7a712601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e325ad79ca547f787c0e0af4b6538c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f1d9b9c40bb409292955c6e708d3904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7447fb6f6d5b410892fb8c74d8513a12",
              "IPY_MODEL_0285370657fe4681942e71261375fb09",
              "IPY_MODEL_51c20d9ad9434ad2ab399cfb83053924"
            ],
            "layout": "IPY_MODEL_11c145705dc847b8b42d7811fe010f2a"
          }
        },
        "7447fb6f6d5b410892fb8c74d8513a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0f96627a49240559332ed17c8fa8f7b",
            "placeholder": "​",
            "style": "IPY_MODEL_64c9231e207946b88339de5709b4b655",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0285370657fe4681942e71261375fb09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b4517823cc494cb82bace769028153",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e56f204259524e42ac5f60bc7f4d5bab",
            "value": 414
          }
        },
        "51c20d9ad9434ad2ab399cfb83053924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b9b7bca847b453790a0020e049b6eb0",
            "placeholder": "​",
            "style": "IPY_MODEL_e4d089d56b7f42e7a163160296b8b3cf",
            "value": " 414/414 [00:00&lt;00:00, 48.7kB/s]"
          }
        },
        "11c145705dc847b8b42d7811fe010f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f96627a49240559332ed17c8fa8f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c9231e207946b88339de5709b4b655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16b4517823cc494cb82bace769028153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56f204259524e42ac5f60bc7f4d5bab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b9b7bca847b453790a0020e049b6eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d089d56b7f42e7a163160296b8b3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3973c01a57447ed9660d7d9bb2413d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3a50d230ae941c3885148d6ec2019b2",
              "IPY_MODEL_6353a302641a45e982e3d3848caf4106",
              "IPY_MODEL_9d7d2bcd1b8b4530b29102bb5532d47a"
            ],
            "layout": "IPY_MODEL_7e12f512d1f94ec2a336814970fa38fd"
          }
        },
        "a3a50d230ae941c3885148d6ec2019b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48f6246dff34bec83e3df5e24aa027e",
            "placeholder": "​",
            "style": "IPY_MODEL_5a653789570045e8a03c4ec35da718fa",
            "value": "modules.json: 100%"
          }
        },
        "6353a302641a45e982e3d3848caf4106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4484e0facf0e4c50868da6810d6cff84",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44f1c86d56514dfa97fe955769d949a5",
            "value": 349
          }
        },
        "9d7d2bcd1b8b4530b29102bb5532d47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb084473efa24e63882e38c0af878695",
            "placeholder": "​",
            "style": "IPY_MODEL_ed1fc82c50694edab347212c7635f39a",
            "value": " 349/349 [00:00&lt;00:00, 9.55kB/s]"
          }
        },
        "7e12f512d1f94ec2a336814970fa38fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48f6246dff34bec83e3df5e24aa027e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a653789570045e8a03c4ec35da718fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4484e0facf0e4c50868da6810d6cff84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f1c86d56514dfa97fe955769d949a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb084473efa24e63882e38c0af878695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1fc82c50694edab347212c7635f39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae26dced3a5548d39ba69e065af441e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9093c104c06148f5b8b6e4682807f0ea",
              "IPY_MODEL_5c093517caf54a84a02cf95748f4ceea",
              "IPY_MODEL_4f2173651ecd4029bc32d51d781c5348"
            ],
            "layout": "IPY_MODEL_a0dd545ab44441a28b4ec16c3301eb88"
          }
        },
        "9093c104c06148f5b8b6e4682807f0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b547134f5e084a38b2f8033586dc7e79",
            "placeholder": "​",
            "style": "IPY_MODEL_aa8f3bb523b74387b2d585195d919cad",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "5c093517caf54a84a02cf95748f4ceea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da740c823b0641d1be23bb32d345d419",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19e3b79802474bcd86a83e4ff754bca0",
            "value": 116
          }
        },
        "4f2173651ecd4029bc32d51d781c5348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_687fcafca1bf4ac5b5a0b557b60da8ec",
            "placeholder": "​",
            "style": "IPY_MODEL_10e9224ffc2e4565bbfa2de9de90c630",
            "value": " 116/116 [00:00&lt;00:00, 2.58kB/s]"
          }
        },
        "a0dd545ab44441a28b4ec16c3301eb88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b547134f5e084a38b2f8033586dc7e79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8f3bb523b74387b2d585195d919cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da740c823b0641d1be23bb32d345d419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e3b79802474bcd86a83e4ff754bca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "687fcafca1bf4ac5b5a0b557b60da8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e9224ffc2e4565bbfa2de9de90c630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3da587300f4940c4a66d7325a5f95b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf3b62bae959489aac9793380ba3614a",
              "IPY_MODEL_196904ae6ee840b29e0ebc85bd811659",
              "IPY_MODEL_6a13e8ff6e71499982ca1b9a8672a9bc"
            ],
            "layout": "IPY_MODEL_aa48eab05038435d806484c99da0d356"
          }
        },
        "bf3b62bae959489aac9793380ba3614a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fb3de034dc64f398facaadc3436d798",
            "placeholder": "​",
            "style": "IPY_MODEL_3338873f604643eb80499c7f9f79fe88",
            "value": "README.md: "
          }
        },
        "196904ae6ee840b29e0ebc85bd811659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18aa3fee76024a5b9b9869b1cc18b534",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a79b5c032c5f4c33b6967d000272170b",
            "value": 1
          }
        },
        "6a13e8ff6e71499982ca1b9a8672a9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c462fdf61994cd19417b73beb6838f5",
            "placeholder": "​",
            "style": "IPY_MODEL_d2faac9491514a67abc4743ad38c1f74",
            "value": " 11.6k/? [00:00&lt;00:00, 194kB/s]"
          }
        },
        "aa48eab05038435d806484c99da0d356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb3de034dc64f398facaadc3436d798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3338873f604643eb80499c7f9f79fe88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18aa3fee76024a5b9b9869b1cc18b534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a79b5c032c5f4c33b6967d000272170b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c462fdf61994cd19417b73beb6838f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2faac9491514a67abc4743ad38c1f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf2ed2cd25854448810707ec7f6d9a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0d53cf7229344fd8b792a54216bff2c",
              "IPY_MODEL_76daa0d016ea4ab0a47606b3836ecc63",
              "IPY_MODEL_2a475635a39449069951a013a97146fb"
            ],
            "layout": "IPY_MODEL_0f8dadcdf43346d4a21b8df8d17d2fcf"
          }
        },
        "b0d53cf7229344fd8b792a54216bff2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf26e12b02524e6daa7d00e2379c7ceb",
            "placeholder": "​",
            "style": "IPY_MODEL_cc87fbb9695949c8984b239098744eb7",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "76daa0d016ea4ab0a47606b3836ecc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e791aeb10404d609dd41225ad66a52d",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6bfdc9a19014270837825474a9976dd",
            "value": 53
          }
        },
        "2a475635a39449069951a013a97146fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d8a101980c4884871e6f6fd18a2c85",
            "placeholder": "​",
            "style": "IPY_MODEL_e46dee9e372142b18b82fd046e0111d6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.30kB/s]"
          }
        },
        "0f8dadcdf43346d4a21b8df8d17d2fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf26e12b02524e6daa7d00e2379c7ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc87fbb9695949c8984b239098744eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e791aeb10404d609dd41225ad66a52d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bfdc9a19014270837825474a9976dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68d8a101980c4884871e6f6fd18a2c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e46dee9e372142b18b82fd046e0111d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c59e0c5252d470abc52ec9a8e8e26d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc01d3b85b644983b0299b17dd70245a",
              "IPY_MODEL_e491114d304c416985cc69f82d6ed372",
              "IPY_MODEL_cd8f386f675b490a811433b1ed1f081b"
            ],
            "layout": "IPY_MODEL_c1d542cbeb55480c909074020c855d53"
          }
        },
        "bc01d3b85b644983b0299b17dd70245a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943dd0e12bf640508e24656ad4c09567",
            "placeholder": "​",
            "style": "IPY_MODEL_7545e5c277a64343a6887ae7e31755fe",
            "value": "config.json: 100%"
          }
        },
        "e491114d304c416985cc69f82d6ed372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23b9413d75f438fbe99f994fc5f38f8",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43306786899949568fc1f3fab1937a74",
            "value": 571
          }
        },
        "cd8f386f675b490a811433b1ed1f081b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_999b7dbb1e984bf599b9ca61d544c3b7",
            "placeholder": "​",
            "style": "IPY_MODEL_86805974a3014142beb8722ee5275f00",
            "value": " 571/571 [00:00&lt;00:00, 17.5kB/s]"
          }
        },
        "c1d542cbeb55480c909074020c855d53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943dd0e12bf640508e24656ad4c09567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7545e5c277a64343a6887ae7e31755fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f23b9413d75f438fbe99f994fc5f38f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43306786899949568fc1f3fab1937a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "999b7dbb1e984bf599b9ca61d544c3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86805974a3014142beb8722ee5275f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a614ccfa91341e9a84d4eb8eb729e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_042b01523bf04b0994496851da44fe5d",
              "IPY_MODEL_e5b1325017db46df8bcf7595a7903686",
              "IPY_MODEL_86d2a54faca44cf79704c23c7145ec62"
            ],
            "layout": "IPY_MODEL_eafd889c6f2345abb97fbbd3bde29fa1"
          }
        },
        "042b01523bf04b0994496851da44fe5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7bdeabb41b64f2ba8bd09970bc8ee8c",
            "placeholder": "​",
            "style": "IPY_MODEL_f58b659655864ddab0872529780541dc",
            "value": "model.safetensors: 100%"
          }
        },
        "e5b1325017db46df8bcf7595a7903686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce9588ea1304a1db8516db0090c63c1",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac1a2d57e5fa478495439b682c00013d",
            "value": 437971872
          }
        },
        "86d2a54faca44cf79704c23c7145ec62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdcce0b5e1024711b762286a2da83c04",
            "placeholder": "​",
            "style": "IPY_MODEL_aff19a689acc4c8db3ad7cca00f7f6eb",
            "value": " 438M/438M [00:03&lt;00:00, 205MB/s]"
          }
        },
        "eafd889c6f2345abb97fbbd3bde29fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bdeabb41b64f2ba8bd09970bc8ee8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f58b659655864ddab0872529780541dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ce9588ea1304a1db8516db0090c63c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1a2d57e5fa478495439b682c00013d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdcce0b5e1024711b762286a2da83c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aff19a689acc4c8db3ad7cca00f7f6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "573273300d724377a36e16212c9cd4cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82216a2363cb4504a55555787dc8dcb3",
              "IPY_MODEL_db8da59bf83c416c991f43671d781919",
              "IPY_MODEL_94b3d0a1353e49588d3be2c524323877"
            ],
            "layout": "IPY_MODEL_6f9dbc7474e548b59c901532b30c1e9f"
          }
        },
        "82216a2363cb4504a55555787dc8dcb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_718eb3c2b2a446d084c24164c6259e2f",
            "placeholder": "​",
            "style": "IPY_MODEL_d240d9d07a774a0d9bb7bbdb046eeb3b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "db8da59bf83c416c991f43671d781919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf17064a97c47b5b2123392caa0fd98",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99ab01ea46c44db4a73542e65b854dae",
            "value": 363
          }
        },
        "94b3d0a1353e49588d3be2c524323877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d79bb992b1c440e4b2c4c72f5b3c411b",
            "placeholder": "​",
            "style": "IPY_MODEL_dad130c526c4403398ab0a2e5d71f2c2",
            "value": " 363/363 [00:00&lt;00:00, 21.3kB/s]"
          }
        },
        "6f9dbc7474e548b59c901532b30c1e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718eb3c2b2a446d084c24164c6259e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d240d9d07a774a0d9bb7bbdb046eeb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf17064a97c47b5b2123392caa0fd98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ab01ea46c44db4a73542e65b854dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d79bb992b1c440e4b2c4c72f5b3c411b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad130c526c4403398ab0a2e5d71f2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f34910d602c479a8484bf1a5d9d0113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f63e6d06533484d8f76eb42852f2590",
              "IPY_MODEL_4d8d6dfa9bf24b3c9864d1468ad3dd03",
              "IPY_MODEL_da6a65010e2c476cad1456107c132c68"
            ],
            "layout": "IPY_MODEL_25b1fbda2e5a409ba5acec9489db01d4"
          }
        },
        "6f63e6d06533484d8f76eb42852f2590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50aed7a1ef924745890c7aef53af0981",
            "placeholder": "​",
            "style": "IPY_MODEL_48b583c01d694eb6b2b6ef3cbebad4d3",
            "value": "vocab.txt: "
          }
        },
        "4d8d6dfa9bf24b3c9864d1468ad3dd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb30dc65ba734064b6178a32e0e33a8b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19f2010b3bc8430cb85a452d9da3226b",
            "value": 1
          }
        },
        "da6a65010e2c476cad1456107c132c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df251bbe9cba418982a33242562de55d",
            "placeholder": "​",
            "style": "IPY_MODEL_1c527b17989f451eb4e1578dbd2f5839",
            "value": " 232k/? [00:00&lt;00:00, 15.6MB/s]"
          }
        },
        "25b1fbda2e5a409ba5acec9489db01d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50aed7a1ef924745890c7aef53af0981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b583c01d694eb6b2b6ef3cbebad4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb30dc65ba734064b6178a32e0e33a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "19f2010b3bc8430cb85a452d9da3226b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df251bbe9cba418982a33242562de55d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c527b17989f451eb4e1578dbd2f5839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae4e5e9a193e463482532bb1af8c00bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3388d6fad8549fc94dd61a2cfd9045e",
              "IPY_MODEL_183930ba19a64f42837696dddde01018",
              "IPY_MODEL_a2579bf967ee4c53ba11cf67a5893a4a"
            ],
            "layout": "IPY_MODEL_6c05b55ae017465896ed4bd15ef72add"
          }
        },
        "c3388d6fad8549fc94dd61a2cfd9045e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d918093beabf4f2e84c6e98f49a5c107",
            "placeholder": "​",
            "style": "IPY_MODEL_eb79ba32b1894c3da7aa07f92b5ab379",
            "value": "tokenizer.json: "
          }
        },
        "183930ba19a64f42837696dddde01018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_329e29298e894a489184b998366b111d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35e427376b6d432296c4f05f98c2ded1",
            "value": 1
          }
        },
        "a2579bf967ee4c53ba11cf67a5893a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2e3a40a0c44d28a4a4f25e6e545c20",
            "placeholder": "​",
            "style": "IPY_MODEL_34f868c064b34782a7bbbbf626b6797f",
            "value": " 466k/? [00:00&lt;00:00, 22.2MB/s]"
          }
        },
        "6c05b55ae017465896ed4bd15ef72add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d918093beabf4f2e84c6e98f49a5c107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb79ba32b1894c3da7aa07f92b5ab379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "329e29298e894a489184b998366b111d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "35e427376b6d432296c4f05f98c2ded1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d2e3a40a0c44d28a4a4f25e6e545c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f868c064b34782a7bbbbf626b6797f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11922234783749d7b2a36e925ee2f89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3989b32b1abe49be9fe4bbbf2e67c2cc",
              "IPY_MODEL_d4829b6ccd504335a49e324c660ca550",
              "IPY_MODEL_2d1e8d6fe526426abb406f94f3cd797a"
            ],
            "layout": "IPY_MODEL_130490a83e5c44518358ddd13e605516"
          }
        },
        "3989b32b1abe49be9fe4bbbf2e67c2cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0da2dc4edf24c7cb6f22ab5f521ce84",
            "placeholder": "​",
            "style": "IPY_MODEL_ce61cc1255114d48878004659869c866",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d4829b6ccd504335a49e324c660ca550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01b7cae0ab5b4b11ae01f92677e99f90",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b0369e857524a4c90966366a895d4b7",
            "value": 239
          }
        },
        "2d1e8d6fe526426abb406f94f3cd797a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce13e5d099114d47806cce68e287480d",
            "placeholder": "​",
            "style": "IPY_MODEL_007edcf55ea640cb870af9670f13b48e",
            "value": " 239/239 [00:00&lt;00:00, 27.7kB/s]"
          }
        },
        "130490a83e5c44518358ddd13e605516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0da2dc4edf24c7cb6f22ab5f521ce84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce61cc1255114d48878004659869c866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01b7cae0ab5b4b11ae01f92677e99f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b0369e857524a4c90966366a895d4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce13e5d099114d47806cce68e287480d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007edcf55ea640cb870af9670f13b48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ea65a4afc7a46c09efce8543a904bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3b9e3b110c04e33a78a1c7eab875db9",
              "IPY_MODEL_8f10f05b98e140958085970081f9119d",
              "IPY_MODEL_94dab4e7b0434d4cb5434848078d40c2"
            ],
            "layout": "IPY_MODEL_56107c13ef954b8998184c26c1154af2"
          }
        },
        "b3b9e3b110c04e33a78a1c7eab875db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ab46dedc77c4c2d979e47c0e6baf73b",
            "placeholder": "​",
            "style": "IPY_MODEL_17af80fb096a4b36aca8b214391bcb59",
            "value": "config.json: 100%"
          }
        },
        "8f10f05b98e140958085970081f9119d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ede9794af0248aaa799c478b51ecf40",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26d839ae9d28467a80b850068fd98e0c",
            "value": 190
          }
        },
        "94dab4e7b0434d4cb5434848078d40c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c962bd576d814609b98fb749ff2e74c0",
            "placeholder": "​",
            "style": "IPY_MODEL_1c31b50964dd4f4d8f90ce72758849f0",
            "value": " 190/190 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "56107c13ef954b8998184c26c1154af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab46dedc77c4c2d979e47c0e6baf73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17af80fb096a4b36aca8b214391bcb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ede9794af0248aaa799c478b51ecf40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d839ae9d28467a80b850068fd98e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c962bd576d814609b98fb749ff2e74c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c31b50964dd4f4d8f90ce72758849f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frn6WiM3t1fT",
        "outputId": "5858268a-766a-4b1a-8d7d-091bb2bc850d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install llama-index-llms-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERetcv-QFKbK",
        "outputId": "c0b03ae1-8a42-4b1c-ca92-a94bb650f0a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/2f/23/7c3ddad6bcca2dc3e9467d48156813d809b72b0ef995b895e707a2eceff3/llama_index-0.14.12-py3-none-any.whl\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM"
      ],
      "metadata": {
        "id": "g89stBKSFiqM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "making a directory and loading the pdf"
      ],
      "metadata": {
        "id": "uj5upvraJd2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "pbybBSrtGXKi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=SimpleDirectoryReader(\"/content/data\").load_data()"
      ],
      "metadata": {
        "id": "8Wb93Af2G55k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XMq43E7H315",
        "outputId": "a655c9e7-4994-4720-99f9-487ea86fa297"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id_='ec9b8a54-4648-4965-b72a-faf4546111d4', embedding=None, metadata={'page_label': 'i', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='INTRODUCTION TO MACHINE LEARNING', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='59911597-b045-426e-a489-2d55256bf7da', embedding=None, metadata={'page_label': 'ii', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0a14a387-2f49-4485-a485-497e5ceb557e', embedding=None, metadata={'page_label': 'iii', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Introduction to Machine Learning\\nAlex Smola and S.V.N. Vishwanathan\\nYahoo! Labs\\nSanta Clara\\n–and–\\nDepartments of Statistics and Computer Science\\nPurdue University\\n–and–\\nCollege of Engineering and Computer Science\\nAustralian National University\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1aaaf8d4-7e4c-4d30-98ab-99d422ce9c00', embedding=None, metadata={'page_label': 'iv', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='p u b l i s h e d b y t h e p r e s s s y n d i c a t e o f t h e u n i v e r s i t y o f c a m b r i d g e\\nThe Pitt Building, Trumpington Street, Cambridge, United Kingdom\\nc a m b r i d g e u n i v e r s i t y p r e s s\\nThe Edinburgh Building, Cambridge CB2 2RU, UK\\n40 West 20th Street, New York, NY 10011–4211, USA\\n477 Williamstown Road, Port Melbourne, VIC 3207, Australia\\nRuiz de Alarc´ on 13, 28014 Madrid, Spain\\nDock House, The Waterfront, Cape Town 8001, South Africa\\nhttp://www.cambridge.org\\nc⃝ Cambridge University Press 2008\\nThis book is in copyright. Subject to statutory exception\\nand to the provisions of relevant collective licensing agreements,\\nno reproduction of any part may take place without\\nthe written permission of Cambridge University Press.\\nFirst published 2008\\nPrinted in the United Kingdom at the University Press, Cambridge\\nTypeface Monotype Times 10/13pt System LATEX 2ε [Alexander J. Smola and S.V.N.\\nVishwanathan]\\nA catalogue record for this book is available from the British Library\\nLibrary of Congress Cataloguing in Publication data available\\nISBN 0 521 82583 0 hardback\\nAuthor: vishy\\nRevision: 252\\nTimestamp: October 1, 2010\\nURL: svn://smola@repos.stat.purdue.edu/thebook/trunk/Book/thebook.tex', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4019c719-8b3d-4da4-afc6-76a714e7cdd9', embedding=None, metadata={'page_label': 'v', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents\\nPreface page 1\\n1 Introduction 3\\n1.1 A Taste of Machine Learning 3\\n1.1.1 Applications 3\\n1.1.2 Data 7\\n1.1.3 Problems 9\\n1.2 Probability Theory 12\\n1.2.1 Random Variables 12\\n1.2.2 Distributions 13\\n1.2.3 Mean and Variance 15\\n1.2.4 Marginalization, Independence, Conditioning, and\\nBayes Rule 16\\n1.3 Basic Algorithms 20\\n1.3.1 Naive Bayes 22\\n1.3.2 Nearest Neighbor Estimators 24\\n1.3.3 A Simple Classiﬁer 27\\n1.3.4 Perceptron 29\\n1.3.5 K-Means 32\\n2 Density Estimation 37\\n2.1 Limit Theorems 37\\n2.1.1 Fundamental Laws 38\\n2.1.2 The Characteristic Function 42\\n2.1.3 Tail Bounds 45\\n2.1.4 An Example 48\\n2.2 Parzen Windows 51\\n2.2.1 Discrete Density Estimation 51\\n2.2.2 Smoothing Kernel 52\\n2.2.3 Parameter Estimation 54\\n2.2.4 Silverman’s Rule 57\\n2.2.5 Watson-Nadaraya Estimator 59\\n2.3 Exponential Families 60\\n2.3.1 Basics 60\\nv', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b451f9e6-8753-4c64-81b3-a6a03647179d', embedding=None, metadata={'page_label': 'vi', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='vi 0 Contents\\n2.3.2 Examples 62\\n2.4 Estimation 66\\n2.4.1 Maximum Likelihood Estimation 66\\n2.4.2 Bias, Variance and Consistency 68\\n2.4.3 A Bayesian Approach 71\\n2.4.4 An Example 75\\n2.5 Sampling 77\\n2.5.1 Inverse Transformation 78\\n2.5.2 Rejection Sampler 82\\n3 Optimization 91\\n3.1 Preliminaries 91\\n3.1.1 Convex Sets 92\\n3.1.2 Convex Functions 92\\n3.1.3 Subgradients 96\\n3.1.4 Strongly Convex Functions 97\\n3.1.5 Convex Functions with Lipschitz Continous Gradient 98\\n3.1.6 Fenchel Duality 98\\n3.1.7 Bregman Divergence 100\\n3.2 Unconstrained Smooth Convex Minimization 102\\n3.2.1 Minimizing a One-Dimensional Convex Function 102\\n3.2.2 Coordinate Descent 104\\n3.2.3 Gradient Descent 104\\n3.2.4 Mirror Descent 108\\n3.2.5 Conjugate Gradient 111\\n3.2.6 Higher Order Methods 115\\n3.2.7 Bundle Methods 121\\n3.3 Constrained Optimization 125\\n3.3.1 Projection Based Methods 125\\n3.3.2 Lagrange Duality 127\\n3.3.3 Linear and Quadratic Programs 131\\n3.4 Stochastic Optimization 135\\n3.4.1 Stochastic Gradient Descent 136\\n3.5 Nonconvex Optimization 137\\n3.5.1 Concave-Convex Procedure 137\\n3.6 Some Practical Advice 139\\n4 Online Learning and Boosting 143\\n4.1 Halving Algorithm 143\\n4.2 Weighted Majority 144', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b1d4deb4-ff31-4715-a3e6-0eba931225e0', embedding=None, metadata={'page_label': 'vii', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Contents vii\\n5 Conditional Densities 149\\n5.1 Logistic Regression 150\\n5.2 Regression 151\\n5.2.1 Conditionally Normal Models 151\\n5.2.2 Posterior Distribution 151\\n5.2.3 Heteroscedastic Estimation 151\\n5.3 Multiclass Classiﬁcation 151\\n5.3.1 Conditionally Multinomial Models 151\\n5.4 What is a CRF? 152\\n5.4.1 Linear Chain CRFs 152\\n5.4.2 Higher Order CRFs 152\\n5.4.3 Kernelized CRFs 152\\n5.5 Optimization Strategies 152\\n5.5.1 Getting Started 152\\n5.5.2 Optimization Algorithms 152\\n5.5.3 Handling Higher order CRFs 152\\n5.6 Hidden Markov Models 153\\n5.7 Further Reading 153\\n5.7.1 Optimization 153\\n6 Kernels and Function Spaces 155\\n6.1 The Basics 155\\n6.1.1 Examples 156\\n6.2 Kernels 161\\n6.2.1 Feature Maps 161\\n6.2.2 The Kernel Trick 161\\n6.2.3 Examples of Kernels 161\\n6.3 Algorithms 161\\n6.3.1 Kernel Perceptron 161\\n6.3.2 Trivial Classiﬁer 161\\n6.3.3 Kernel Principal Component Analysis 161\\n6.4 Reproducing Kernel Hilbert Spaces 161\\n6.4.1 Hilbert Spaces 163\\n6.4.2 Theoretical Properties 163\\n6.4.3 Regularization 163\\n6.5 Banach Spaces 164\\n6.5.1 Properties 164\\n6.5.2 Norms and Convex Sets 164\\n7 Linear Models 165\\n7.1 Support Vector Classiﬁcation 165', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3af30475-021d-4463-90c9-5c00efd2dced', embedding=None, metadata={'page_label': 'viii', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='viii 0 Contents\\n7.1.1 A Regularized Risk Minimization Viewpoint 170\\n7.1.2 An Exponential Family Interpretation 170\\n7.1.3 Specialized Algorithms for Training SVMs 172\\n7.2 Extensions 177\\n7.2.1 The ν trick 177\\n7.2.2 Squared Hinge Loss 179\\n7.2.3 Ramp Loss 180\\n7.3 Support Vector Regression 181\\n7.3.1 Incorporating General Loss Functions 184\\n7.3.2 Incorporating the ν Trick 186\\n7.4 Novelty Detection 186\\n7.5 Margins and Probability 189\\n7.6 Beyond Binary Classiﬁcation 189\\n7.6.1 Multiclass Classiﬁcation 190\\n7.6.2 Multilabel Classiﬁcation 191\\n7.6.3 Ordinal Regression and Ranking 192\\n7.7 Large Margin Classiﬁers with Structure 193\\n7.7.1 Margin 193\\n7.7.2 Penalized Margin 193\\n7.7.3 Nonconvex Losses 193\\n7.8 Applications 193\\n7.8.1 Sequence Annotation 193\\n7.8.2 Matching 193\\n7.8.3 Ranking 193\\n7.8.4 Shortest Path Planning 193\\n7.8.5 Image Annotation 193\\n7.8.6 Contingency Table Loss 193\\n7.9 Optimization 193\\n7.9.1 Column Generation 193\\n7.9.2 Bundle Methods 193\\n7.9.3 Overrelaxation in the Dual 193\\n7.10 CRFs vs Structured Large Margin Models 194\\n7.10.1 Loss Function 194\\n7.10.2 Dual Connections 194\\n7.10.3 Optimization 194\\nAppendix 1 Linear Algebra and Functional Analysis 197\\nAppendix 2 Conjugate Distributions 201\\nAppendix 3 Loss Functions 203\\nBibliography 221', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8652349a-682a-42df-98b8-ef4c7f892885', embedding=None, metadata={'page_label': '1', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preface\\nSince this is a textbook we biased our selection of references towards easily\\naccessible work rather than the original references. While this may not be\\nin the interest of the inventors of these concepts, it greatly simpliﬁes access\\nto those topics. Hence we encourage the reader to follow the references in\\nthe cited works should they be interested in ﬁnding out who may claim\\nintellectual ownership of certain key ideas.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b6ddaf44-8010-4d4d-bea9-22cdc390c9ee', embedding=None, metadata={'page_label': '2', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 0 Preface\\nStructure of the Book\\nIntroduction\\nDensity \\nEstimation\\nGraphical \\nModels\\nKernels Optimization\\nConditional \\nDensities\\nConditional \\nRandom Fields\\nLinear Models\\nStructured \\nEstimation\\nDuality and \\nEstimation\\nMoment\\nMethods\\nReinforcement \\nLearning\\nIntroduction\\nDensity \\nEstimation\\nGraphical \\nModels\\nKernels Optimization\\nConditional \\nDensities\\nConditional \\nRandom Fields\\nLinear Models\\nStructured \\nEstimation\\nDuality and \\nEstimation\\nMoment\\nMethods\\nReinforcement \\nLearning\\nIntroduction\\nDensity \\nEstimation\\nGraphical \\nModels\\nKernels Optimization\\nConditional \\nDensities\\nConditional \\nRandom Fields\\nLinear Models\\nStructured \\nEstimation\\nDuality and \\nEstimation\\nMoment\\nMethods\\nReinforcement \\nLearning\\nCanberra, August 2008', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0ffa10a2-4d1d-4da9-81cc-7685d7ebfd5f', embedding=None, metadata={'page_label': '3', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1\\nIntroduction\\nOver the past two decades Machine Learning has become one of the main-\\nstays of information technology and with that, a rather central, albeit usually\\nhidden, part of our life. With the ever increasing amounts of data becoming\\navailable there is good reason to believe that smart data analysis will become\\neven more pervasive as a necessary ingredient for technological progress.\\nThe purpose of this chapter is to provide the reader with an overview over\\nthe vast range of applications which have at their heart a machine learning\\nproblem and to bring some degree of order to the zoo of problems. After\\nthat, we will discuss some basic tools from statistics and probability theory,\\nsince they form the language in which many machine learning problems must\\nbe phrased to become amenable to solving. Finally, we will outline a set of\\nfairly basic yet eﬀective algorithms to solve an important problem, namely\\nthat of classiﬁcation. More sophisticated tools, a discussion of more general\\nproblems and a detailed analysis will follow in later parts of the book.\\n1.1 A Taste of Machine Learning\\nMachine learning can appear in many guises. We now discuss a number of\\napplications, the types of data they deal with, and ﬁnally, we formalize the\\nproblems in a somewhat more stylized fashion. The latter is key if we want to\\navoid reinventing the wheel for every new application. Instead, much of the\\nart of machine learning is to reduce a range of fairly disparate problems to\\na set of fairly narrow prototypes. Much of the science of machine learning is\\nthen to solve those problems and provide good guarantees for the solutions.\\n1.1.1 Applications\\nMost readers will be familiar with the concept of web page ranking. That\\nis, the process of submitting a query to a search engine, which then ﬁnds\\nwebpages relevant to the query and which returns them in their order of\\nrelevance. See e.g. Figure 1.1 for an example of the query results for “ma-\\nchine learning”. That is, the search engine returns a sorted list of webpages\\ngiven a query. To achieve this goal, a search engine needs to ‘know’ which\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cb05c527-4e97-46e3-91e4-51dd3b110850', embedding=None, metadata={'page_label': '4', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"4 1 Introduction\\nWeb Images Maps News Shopping Gmail more !\\n \\n    \\nSponsored Links\\nMachine Learning\\nGoogle Sydney needs machine\\nlearning experts. Apply today!\\nwww.google.com.au/jobs\\nSign in\\n \\nSearch  Advanced Search\\n  Preferences\\n Web    Scholar   Results 1 - 10 of about 10,500,000 for machine learning. (0.06 seconds) \\nMachine learning - Wikipedia, the free encyclopedia\\nAs a broad subfield of artificial intelligence, machine learning is concerned with the design\\nand development of algorithms and techniques that allow ...\\nen.wikipedia.org/wiki/Machine_learning - 43k - Cached - Similar pages\\nMachine Learning textbook\\nMachine Learning is the study of computer algorithms that improve automatically through\\nexperience. Applications range from datamining programs that ...\\nwww.cs.cmu.edu/~tom/mlbook.html - 4k - Cached - Similar pages\\nmachine learning\\nwww.aaai.org/AITopics/html/machine.html - Similar pages\\nMachine Learning\\nA list of links to papers and other resources on machine learning.\\nwww.machinelearning.net/ - 14k - Cached - Similar pages\\nIntroduction to Machine Learning\\nThis page has pointers to my draft book on Machine Learning and to its individual\\nchapters. They can be downloaded in Adobe Acrobat format. ...\\nai.stanford.edu/~nilsson/mlbook.html - 15k - Cached - Similar pages\\nMachine Learning - Artificial Intelligence (incl. Robotics ...\\nMachine Learning - Artificial Intelligence. Machine Learning is an international forum for\\nresearch on computational approaches to learning.\\nwww.springer.com/computer/artificial/journal/10994 - 39k - Cached - Similar pages\\nMachine Learning (Theory)\\nGraduating students in Statistics appear to be at a substantial handicap compared to\\ngraduating students in Machine Learning, despite being in substantially ...\\nhunch.net/ - 94k - Cached - Similar pages\\nAmazon.com: Machine Learning: Tom M. Mitchell: Books\\nAmazon.com: Machine Learning: Tom M. Mitchell: Books.\\nwww.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077 - 210k -\\nCached - Similar pages\\nMachine Learning Journal\\nMachine Learning publishes articles on the mechanisms through which intelligent systems\\nimprove their performance over time. We invite authors to submit ...\\npages.stern.nyu.edu/~fprovost/MLJ/ - 3k - Cached - Similar pages\\nCS 229: Machine Learning\\nSTANFORD. CS229 Machine Learning Autumn 2007. Announcements. Final reports from\\nthis year's class projects have been posted here. ...\\ncs229.stanford.edu/ - 10k - Cached - Similar pages\\n12345678910 Next\\n \\n \\nSearch\\nSearch within results | Language Tools | Search Tips | Dissatisfied? Help us improve | Try Google Experimental\\n©2008 Google - Google Home - Advertising Programs - Business Solutions - About Google\\nmachine learning\\nmachine learning\\nGoogle\\nFig. 1.1. The 5 top scoring webpages for the query “machine learning”\\npages are relevant and which pages match the query. Such knowledge can be\\ngained from several sources: the link structure of webpages, their content,\\nthe frequency with which users will follow the suggested links in a query, or\\nfrom examples of queries in combination with manually ranked webpages.\\nIncreasingly machine learning rather than guesswork and clever engineering\\nis used to automate the process of designing a good search engine [RPB06].\\nA rather related application is collaborative ﬁltering . Internet book-\\nstores such as Amazon, or video rental sites such as Netﬂix use this informa-\\ntion extensively to entice users to purchase additional goods (or rent more\\nmovies). The problem is quite similar to the one of web page ranking. As\\nbefore, we want to obtain a sorted list (in this case of articles). The key dif-\\nference is that an explicit query is missing and instead we can only use past\\npurchase and viewing decisions of the user to predict future viewing and\\npurchase habits. The key side information here are the decisions made by\\nsimilar users, hence the collaborative nature of the process. See Figure 1.2\\nfor an example. It is clearly desirable to have an automatic system to solve\\nthis problem, thereby avoiding guesswork and time [BK07].\\nAn equally ill-deﬁned problem is that of automatic translation of doc-\\numents. At one extreme, we could aim at fully understanding a text before\\ntranslating it using a curated set of rules crafted by a computational linguist\\nwell versed in the two languages we would like to translate. This is a rather\\narduous task, in particular given that text is not always grammatically cor-\\nrect, nor is the document understanding part itself a trivial one. Instead, we\\ncould simply use examples of translated documents, such as the proceedings\\nof the Canadian parliament or other multilingual entities (United Nations,\\nEuropean Union, Switzerland) to learn how to translate between the two\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e4eb4f30-b801-4ad3-a2f9-711fcc43730f', embedding=None, metadata={'page_label': '5', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1 A Taste of Machine Learning 5\\nlanguages. In other words, we could use examples of translations to learn\\nhow to translate. This machine learning approach proved quite successful\\n[?].\\nMany security applications, e.g. for access control, use face recognition as\\none of its components. That is, given the photo (or video recording) of a\\nperson, recognize who this person is. In other words, the system needs to\\nclassify the faces into one of many categories (Alice, Bob, Charlie, . . . ) or\\ndecide that it is an unknown face. A similar, yet conceptually quite diﬀerent\\nproblem is that of veriﬁcation. Here the goal is to verify whether the person\\nin question is who he claims to be. Note that diﬀerently to before, this\\nis now a yes/no question. To deal with diﬀerent lighting conditions, facial\\nexpressions, whether a person is wearing glasses, hairstyle, etc., it is desirable\\nto have a system which learns which features are relevant for identifying a\\nperson.\\nAnother application where learning helps is the problem ofnamed entity\\nrecognition (see Figure 1.4). That is, the problem of identifying entities,\\nsuch as places, titles, names, actions, etc. from documents. Such steps are\\ncrucial in the automatic digestion and understanding of documents. Some\\nmodern e-mail clients, such as Apple’s Mail.app nowadays ship with the\\nability to identify addresses in mails and ﬁling them automatically in an\\naddress book. While systems using hand-crafted rules can lead to satisfac-\\ntory results, it is far more eﬃcient to use examples of marked-up documents\\nto learn such dependencies automatically, in particular if we want to de-\\nploy our system in many languages. For instance, while ’bush’ and ’rice’\\nYour Amazon.com \\n Today\\'s DealsGifts & Wish Lists Gift Cards Your Account  |  Help\\nAdvertise on Amazon\\n5 star:  (23)\\n4 star:  (2)\\n3 star:  (3)\\n2 star:  (2)\\n1 star:  (0)\\n   \\nQuantity: \\n1\\n \\nor\\nSign in to turn on 1-Click ordering.\\n \\n  \\nMore Buying Choices\\n16 used & new from\\n$52.00\\nHave one to sell? \\n \\n  \\n \\n \\nShare your own customer images\\nSearch inside another edition of this book\\nAre You an Author or\\nPublisher? \\nFind out how to publish\\nyour own Kindle Books\\n \\n  \\nHello. Sign in to get personalized recommendations. New customer? Start here.   \\n \\nBooks    \\nBooks Advanced SearchBrowse SubjectsHot New ReleasesBestsellersThe New York Times® Best Sellers Libros En EspañolBargain BooksTextbooks\\nJoin Amazon Prime and ship Two-Day for free and Overnight for $3.99. Already a member? Sign in.\\nMachine Learning (Mcgraw-Hill International Edit)\\n(Paperback)\\nby Thomas Mitchell (Author) \"Ever since computers were invented, we have wondered whether\\nthey might be made to learn...\" (more)\\n  \\n  \\n (30 customer reviews)  \\nList Price:$87.47\\nPrice:$87.47 & this item ships for FREE with Super Saver Shipping.\\nDetails\\nAvailability: Usually ships within 4 to 7 weeks. Ships from and sold by Amazon.com. Gift-\\nwrap available.\\n16 used & new available from $52.00\\nAlso Available in:List Price:Our Price:Other Offers:\\nHardcover (1) $153.44$153.4434 used & new from $67.00\\n \\n  \\nBetter Together\\nBuy this book with Introduction to Machine Learning (Adaptive Computation and Machine Learning) by Ethem Alpaydin today!\\nBuy Together Today: $130.87\\nCustomers Who Bought This Item Also Bought\\nPattern Recognition and\\nMachine Learning\\n(Information Science and\\nStatistics) by Christopher\\nM. Bishop\\n (30)  $60.50\\nArtificial Intelligence: A\\nModern Approach (2nd\\nEdition) (Prentice Hall\\nSeries in Artificial\\nIntelligence) by Stuart\\nRussell\\n (76)  $115.00\\nThe Elements of Statistical\\nLearning by T. Hastie\\n (25)  $72.20\\nPattern Classification (2nd\\nEdition) by Richard O.\\nDuda\\n (25)  $115.00\\nData Mining: Practical\\nMachine Learning Tools\\nand Techniques, Second\\nEdition (Morgan Kaufmann\\nSeries in Data\\nManagement Systems) by\\nIan H. Witten\\n (21)  $39.66\\n› Explore similar items : Books (50)\\nEditorial Reviews\\nBook Description\\nThis exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly\\nchanging field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical\\nand coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text\\nfor college students. --This text refers to the Hardcover edition. \\nBook Info\\nPresents the key algorithms and theory that form the core of machine learning. Discusses such theoretical issues as How does learning\\nperformance vary with the number of training examples presented? and Which learning algorithms are most appropriate for various types of\\nlearning tasks? DLC: Computer algorithms. --This text refers to the Hardcover edition.\\nProduct Details\\nPaperback: 352 pages\\nPublisher: McGraw-Hill Education (ISE Editions); 1st edition (October 1, 1997)\\nLanguage: English\\nISBN-10: 0071154671\\nISBN-13: 978-0071154673\\nProduct Dimensions: 9 x 5.9 x 1.1 inches\\nShipping Weight: 1.2 pounds (View shipping rates and policies)\\nAverage Customer Review: \\n  \\n (30 customer reviews)\\nAmazon.com Sales Rank: #104,460 in Books (See Bestsellers in Books)\\nPopular in this category: (What\\'s this?)\\n#11 in Books > Computers & Internet > Computer Science > Artificial Intelligence > Machine Learning\\n(Publishers and authors: Improve Your Sales)\\nIn-Print Editions: Hardcover (1) |  All Editions\\n Would you like to update product info or give feedback on images? (We\\'ll ask you to sign in so we can get back to you)\\nInside This Book (learn more) \\nBrowse and search another edition of this book.\\nFirst Sentence:\\nEver since computers were invented, we have wondered whether they might be made to learn. Read the first page\\nBrowse Sample Pages:\\nFront Cover | Copyright | Table of Contents | Excerpt | Index | Back Cover | Surprise Me!\\nSearch Inside This Book:\\n \\nCustomers viewing this page may be interested in these Sponsored Links (What\\'s this?)\\nOnline Law Degree\\nhttp://www.edu-onlinedegree.org Juris Doctor JD & LLM Masters Low tuition, Free Textbooks \\nLearning CDs\\nwww.mindperk.com Save on powerful mind-boosting CDs & DVDs. Huge Selection \\nVideo Edit Magic\\nwww.deskshare.com/download Video Editing Software trim, modify color, and merge video \\nTags Customers Associate with This Product (What\\'s this?)\\nClick on a tag to find related items, discussions, and people.\\nmachine learning (6)\\nartificial intelligence (2)\\ncomputer science (1)\\npattern recognition (1)\\nYour tags: Add your first tag\\nHelp others find this product - tag it for Amazon search\\nNo one has tagged this product for Amazon search yet. Why not be the first to\\nsuggest a search for which it should appear?\\nSearch Products Tagged with\\n \\nAre you the publisher or author? Learn how Amazon can help you make this book an eBook. \\nIf you are a publisher or author and hold the digital rights to a book, you can make it available as an eBook on Amazon.com. Learn more\\nRate This Item to Improve Your Recommendations\\nI own itNot rated Your rating\\nDon\\'t like it < > I love it!\\nSave your\\nrating\\n  \\n?\\n 1\\n2\\n3\\n4\\n5\\n \\nCustomer Reviews\\n30 Reviews\\n \\n \\nAverage Customer Review\\n(30 customer reviews)\\n \\n \\n  \\nShare your thoughts with other customers:\\nMost Helpful Customer Reviews\\n 44 of 44 people found the following review helpful:\\n An excellent overview for the adv. undergrad or beg. grad,\\nSeptember 30, 2002\\nBy Todd Ebert (Long Beach California) - See all my reviews\\nThis review is from: Machine Learning (Hardcover)\\nI agree with some of the previous reviews which criticize the book for its lack of\\ndepth, but I believe this to be an asset rather than a liability given its target\\naudience (seniors and beginning grad. students). The average college senior typically\\nknows very little about subjects like neural networks, genetic algorithms, or Baysian\\nnetworks, and this book goes a long way in demystifying these subjects in a very\\nclear, concise, and understandable way. Moreover, the first-year grad. student who is\\ninterested in possibly doing research in this field needs more of an overview than to\\ndive deeply into \\none of the many branches which themselves have had entire books written about\\nthem. This is one of the few if only books where one will find diverse areas of\\nlearning (e.g. analytical, reinforcment, Bayesian, neural-network, genetic-algorithmic)\\nall within the same cover.\\nBut more than just an encyclopedic introduction, the author makes a number of\\nconnections between the different paradigms. For example, he explains that\\nassociated with each paradigm is the notion of an inductive-learning bias, i.e. the\\nunderlying assumptions that lend validity to a given learning approach. These end-of-\\nchapter discussions on bias seem very interesting and unique to this book.\\nFinally, I used this book for part of the reading material for an intro. AI class, and\\nreceived much positive feedback from the students, although some did find the\\npresentation a bit too abstract for their undergraduate tastes\\n Comment | Permalink | Was this review helpful to you?  \\n  (Report this) \\n 22 of 27 people found the following review helpful:\\n Great compilation, May 18, 2001\\nBy Steven Burns (-) - See all my reviews\\nThis review is from: Machine Learning (Hardcover)\\nThis book is completely worth the price, and worth the hardcover to take care of it.\\nThe main chapters of the book are independent, so you can read them in any order.\\nThe way it explains the different learning approaches is beautiful because: 1)it\\nexplains them nicely 2)it gives examples and 3)it presents pseudocode summaries of\\nthe algorithms. As a software developer, what else could I possibly ask for?\\n Comment | Permalink | Was this review helpful to you?  \\n  (Report this) \\n 23 of 23 people found the following review helpful:\\n Venerable, in both senses, April 4, 2004\\nBy eldil (Albuquerque NM) - See all my reviews\\nThis review is from: Machine Learning (Hardcover)\\nIt\\'s pretty well done, it covers theory and core areas but - maybe it was more the\\nstate of the field when it was written - I found it unsatisfyingly un-synthesized,\\nunconnected, and short of detail (but this is subjective). I found the 2nd edition of\\nRussell and Norvig to be a better introduction where it covers the same topic, which\\nit does for everything I can think of, except VC dimension.\\nThe book sorely needs an update, it was written in 1997 and the field has moved\\nfast. A comparison with Mitchell\\'s current course (materials generously available\\nonline) shows that about 1/4 of the topics taught have arisen since the book was\\npublished; Boosting, Support Vector Machines and Hidden Markov Models to name\\nthe best-known. The book also does not cover statistical or data mining methods.\\nDespite the subjective complaint about lack of depth it does give the theoretical\\nroots and many fundamental techniques decently and readably. For many purposes\\nthough it may have been superceded by R&N 2nd ed.\\n Comment | Permalink | Was this review helpful to you?  \\n  (Report this) \\nShare your thoughts with other customers: \\n› See all 30 customer reviews...\\n \\n \\nMost Recent Customer Reviews\\n Outstanding\\nI read this book about 7 years ago while in\\nthe PhD program at Stanford University. I\\nconsider this book not only the best\\nMachine Learning book, but one of the best\\nbooks in all... Read more\\nPublished 6 months ago by Husam Abu-Haimed\\n Great Start to Machine Learning\\nI have used this book during my masters\\nand found it to be an extremely helpful and\\na gentle introduction to the thick and things\\nof machine learning applications.\\nRead more\\nPublished 6 months ago by Subrat Nanda\\n Best book I\\'ve seen on topic\\nI have this book listed as one of the best\\nand most interesting I\\'ve ever read. I loved\\nthe book just as much as I loved the course\\nwe used it in. Read more\\nPublished 13 months ago by Lars Kristensson\\n too expensive I would say\\ngreat book if you wanna start sth anywhere\\nin machine learning, but it is toooooo\\nexpensive.\\nPublished 17 months ago by X. Wu\\n Excellent book, concise and\\nreadable\\nThis is a great book if you\\'re starting out\\nwith machine learning. It\\'s rare to come\\nacross a book like this that is very well\\nwritten and has technical depth. Read more\\nPublished 20 months ago by Part Time Reader\\n great book\\nThis is a great book because it focuses on\\nmachine learning techniques. It has been\\nused as textbook in my class.\\nPublished on November 11, 2005 by Jay\\n Great introduction book for\\nstudents in data mining and machine\\nlearning class\\nAlthough this text book is not required in\\nmy data mining class, but I found it is very\\nhelpful for my study. Read more\\nPublished on October 24, 2005 by Thanh Doan\\n Excellently written\\nI am using this textbook for a Machine\\nLearning class. While my professor is\\nexcellent, I must say that this book is a\\nwelcome addition to class. Read more\\nPublished on October 12, 2005 by Gregor Kronenberger\\n Just a brief introduction to ML\\n...\\nFirst of all, the statistical part of machine\\nlearning is JUST a real subset of\\nmathematical statisitcs, whatever Bayesian\\nor frequentist. Read more\\nPublished on September 12, 2005 by supercutepig\\n Excellent reference book\\nI liked the book. But I think author must\\nprovide more figures in the book like Duda\\nand Hart\\'s Pattern Classification book.\\nRead more\\nPublished on December 25, 2004 by Fatih Nar\\nSearch Customer Reviews\\n Only search this product\\'s reviews\\n› See all 30 customer reviews...\\n \\nCustomer Discussions Beta (What\\'s this?)\\nNew! See recommended Discussions for You\\nThis product\\'s forum (0 discussions)\\nDiscussion RepliesLatest Post\\nNo discussions yet\\nAsk questions, Share opinions, Gain insight\\nStart a new discussion\\nTopic:\\n   Related forums\\nmachine learning (start the discussion)\\nartificial intelligence  (1 discussion)\\nProduct Information from the Amapedia Community Beta (What\\'s this?)\\nBe the first person to add an article about this item at Amapedia.com. \\n› See featured Amapedia.com articles \\nListmania!\\n Machine Learning and Graphs: A list by J. Chan \"PhD Student (Computer\\nScience)\"\\n Bayesian Network Books: A list by Tincture Of Iodine \"TOI\"\\n Books on Algorithms on a variety of topics: A list by calvinnme \"Texan refugee\"\\nCreate a Listmania! list\\nSearch Listmania!\\nSo You\\'d Like to...\\n Learn Advanced Mathematics on Your Own: A guide by Gal Gross \"Wir müssen\\nwissen, wir werden wissen. - David Hilbert\"\\n Learn more about Artificial Intelligence (AI) and Games: A guide by John Funge\\n study curriculum of B.S. computer science (honors mode): A guide by\\n\"josie_roberts\"\\nCreate a guide\\nSearch Guides\\nLook for Similar Items by Category\\nComputers & Internet > Computer Science > Artificial Intelligence > Machine Learning\\nLook for Similar Items by Subject\\n Machine learning\\n Computer Books: General\\nFind books matching ALL checked subjects \\ni.e., each book must be in subject 1 AND subject 2 AND ... \\nHarry Potter Store\\nOur Harry\\nPotter\\nStore\\nfeatures\\nall things\\nHarry,\\nincluding\\nbooks, audio CDs and\\ncassettes, DVDs,\\nsoundtracks, and more.\\n \\nGot Your Neti Pot?\\nGive your\\nsinuses a\\nbath with\\none of the many neti\\npots in our Health &\\nPersonal Care Store.\\n›See more\\n \\nDrop It Like It\\'s\\nWaterproof\\nAnd\\nshockproof,\\ncrushproof,\\nand\\nfreezeproof. All that, in\\naddition to 7-megapixel\\nresolution and Bright\\nCapture technology,\\nmakes the Olympus\\nStylus 770SW the\\nperfect vacation\\ncompanion. Plus, it\\'s now\\navailable for only\\n$289.94 from\\nAmazon.com.\\n \\nEditors\\' Faves in\\nBooks\\nSave\\n40%\\non The\\nSignificant 7, our favorite\\npicks for the month.\\n \\n \\n  \\nFeedback \\n If you need help or have a question for Customer Service, contact us.\\n Would you like to update product info or give feedback on images? (We\\'ll ask you to sign in so we can get back to you)\\n Is there any other feedback you would like to provide? Click here\\nWhere\\'s My Stuff?\\nTrack your recent orders.\\nView or change your orders in Your Account.\\nShipping & Returns\\nSee our shipping rates & policies.\\nReturn an item (here\\'s our Returns Policy).\\nNeed Help?\\nForgot your password? Click here.\\nRedeem or buy a gift certificate/card.\\nVisit our Help department.\\nSearch \\nAmazon.com     \\nYour Recent History (What\\'s this?)\\n \\nRecently Viewed Products\\nAfter viewing product detail pages or search results, look here to find an easy way to navigate back to pages you are interested in.\\nLook to the right column to find helpful suggestions for your shopping session.\\n› View & edit Your Browsing History\\n   \\n Amazon.com Home  |   Directory of All Stores\\nInternational Sites:  Canada  |  United Kingdom  |  Germany  |  Japan  |  France  |  China\\nHelp  |  View Cart  |  Your Account  |  Sell Items  |  1-Click Settings\\n   \\n   \\n   \\nFig. 1.2. Books recommended by Amazon.com when viewing Tom Mitchell’s Ma-\\nchine Learning Book [Mit97]. It is desirable for the vendor to recommend relevant\\nbooks which a user might purchase.\\nFig. 1.3. 11 Pictures of the same person taken from the Yale face recognition\\ndatabase. The challenge is to recognize that we are dealing with the same per-\\nson in all 11 cases.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1d7e194d-831c-44f3-855f-9840f6546c6b', embedding=None, metadata={'page_label': '6', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6 1 Introduction\\nHAVANA (Reuters) - The European Union’s top development aid official\\nleft Cuba on Sunday convinced that EU diplomatic sanctions against\\nthe communist island should be dropped after Fidel Castro’s\\nretirement, his main aide said.\\n<TYPE=\"ORGANIZATION\">HAVANA</> (<TYPE=\"ORGANIZATION\">Reuters</>) - The\\n<TYPE=\"ORGANIZATION\">European Union </>’s top development aid official left\\n<TYPE=\"ORGANIZATION\">Cuba</> on Sunday convinced that EU diplomatic sanctions\\nagainst the communist <TYPE=\"LOCATION\">island</> should be dropped after\\n<TYPE=\"PERSON\">Fidel Castro </>’s retirement, his main aide said.\\nFig. 1.4. Named entity tagging of a news article (using LingPipe). The relevant\\nlocations, organizations and persons are tagged for further information extraction.\\nare clearly terms from agriculture, it is equally clear that in the context of\\ncontemporary politics they refer to members of the Republican Party.\\nOther applications which take advantage of learning are speech recog-\\nnition (annotate an audio sequence with text, such as the system shipping\\nwith Microsoft Vista), the recognition of handwriting (annotate a sequence\\nof strokes with text, a feature common to many PDAs), trackpads of com-\\nputers (e.g. Synaptics, a major manufacturer of such pads derives its name\\nfrom the synapses of a neural network), the detection of failure in jet en-\\ngines, avatar behavior in computer games (e.g. Black and White), direct\\nmarketing (companies use past purchase behavior to guesstimate whether\\nyou might be willing to purchase even more) and ﬂoor cleaning robots (such\\nas iRobot’s Roomba). The overarching theme of learning problems is that\\nthere exists a nontrivial dependence between some observations, which we\\nwill commonly refer to as x and a desired response, which we refer to as y,\\nfor which a simple set of deterministic rules is not known. By using learning\\nwe can infer such a dependency between x and y in a systematic fashion.\\nWe conclude this section by discussing the problem of classiﬁcation,\\nsince it will serve as a prototypical problem for a signiﬁcant part of this\\nbook. It occurs frequently in practice: for instance, when performing spam\\nﬁltering, we are interested in a yes/no answer as to whether an e-mail con-\\ntains relevant information or not. Note that this issue is quite user depen-\\ndent: for a frequent traveller e-mails from an airline informing him about\\nrecent discounts might prove valuable information, whereas for many other\\nrecipients this might prove more of an nuisance (e.g. when the e-mail relates\\nto products available only overseas). Moreover, the nature of annoying e-\\nmails might change over time, e.g. through the availability of new products\\n(Viagra, Cialis, Levitra, . . . ), diﬀerent opportunities for fraud (the Nigerian\\n419 scam which took a new twist after the Iraq war), or diﬀerent data types\\n(e.g. spam which consists mainly of images). To combat these problems we', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7b45ec71-cc35-4421-a698-e2f86e8b4ee6', embedding=None, metadata={'page_label': '7', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1 A Taste of Machine Learning 7\\nFig. 1.5. Binary classiﬁcation; separate stars from diamonds. In this example we\\nare able to do so by drawing a straight line which separates both sets. We will see\\nlater that this is an important example of what is called a linear classiﬁer.\\nwant to build a system which is able to learn how to classify new e-mails.\\nA seemingly unrelated problem, that of cancer diagnosis shares a common\\nstructure: given histological data (e.g. from a microarray analysis of a pa-\\ntient’s tissue) infer whether a patient is healthy or not. Again, we are asked\\nto generate a yes/no answer given a set of observations. See Figure 1.5 for\\nan example.\\n1.1.2 Data\\nIt is useful to characterize learning problems according to the type of data\\nthey use. This is a great help when encountering new challenges, since quite\\noften problems on similar data types can be solved with very similar tech-\\nniques. For instance natural language processing and bioinformatics use very\\nsimilar tools for strings of natural language text and for DNA sequences.\\nVectors constitute the most basic entity we might encounter in our work.\\nFor instance, a life insurance company might be interesting in obtaining the\\nvector of variables (blood pressure, heart rate, height, weight, cholesterol\\nlevel, smoker, gender) to infer the life expectancy of a potential customer.\\nA farmer might be interested in determining the ripeness of fruit based on\\n(size, weight, spectral data). An engineer might want to ﬁnd dependencies\\nin (voltage, current) pairs. Likewise one might want to represent documents\\nby a vector of counts which describe the occurrence of words. The latter is\\ncommonly referred to as bag of words features.\\nOne of the challenges in dealing with vectors is that the scales and units\\nof diﬀerent coordinates may vary widely. For instance, we could measure the\\nheight in kilograms, pounds, grams, tons, stones, all of which would amount\\nto multiplicative changes. Likewise, when representing temperatures, we\\nhave a full class of aﬃne transformations, depending on whether we rep-\\nresent them in terms of Celsius, Kelvin or Farenheit. One way of dealing', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='43fc1c99-2420-4fc5-a3d1-fff89ec5e4e7', embedding=None, metadata={'page_label': '8', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='8 1 Introduction\\nwith those issues in an automatic fashion is to normalize the data. We will\\ndiscuss means of doing so in an automatic fashion.\\nLists: In some cases the vectors we obtain may contain a variable number\\nof features. For instance, a physician might not necessarily decide to perform\\na full battery of diagnostic tests if the patient appears to be healthy.\\nSets may appear in learning problems whenever there is a large number of\\npotential causes of an eﬀect, which are not well determined. For instance, it is\\nrelatively easy to obtain data concerning the toxicity of mushrooms. It would\\nbe desirable to use such data to infer the toxicity of a new mushroom given\\ninformation about its chemical compounds. However, mushrooms contain a\\ncocktail of compounds out of which one or more may be toxic. Consequently\\nwe need to infer the properties of an object given a set of features, whose\\ncomposition and number may vary considerably.\\nMatrices are a convenient means of representing pairwise relationships.\\nFor instance, in collaborative ﬁltering applications the rows of the matrix\\nmay represent users whereas the columns correspond to products. Only in\\nsome cases we will have knowledge about a given (user, product) combina-\\ntion, such as the rating of the product by a user.\\nA related situation occurs whenever we only have similarity information\\nbetween observations, as implemented by a semi-empirical distance mea-\\nsure. Some homology searches in bioinformatics, e.g. variants of BLAST\\n[AGML90], only return a similarity score which does not necessarily satisfy\\nthe requirements of a metric.\\nImages could be thought of as two dimensional arrays of numbers, that is,\\nmatrices. This representation is very crude, though, since they exhibit spa-\\ntial coherence (lines, shapes) and (natural images exhibit) a multiresolution\\nstructure. That is, downsampling an image leads to an object which has very\\nsimilar statistics to the original image. Computer vision and psychooptics\\nhave created a raft of tools for describing these phenomena.\\nVideo adds a temporal dimension to images. Again, we could represent\\nthem as a three dimensional array. Good algorithms, however, take the tem-\\nporal coherence of the image sequence into account.\\nTrees and Graphs are often used to describe relations between collec-\\ntions of objects. For instance the ontology of webpages of the DMOZ project\\n(www.dmoz.org) has the form of a tree with topics becoming increasingly\\nreﬁned as we traverse from the root to one of the leaves (Arts → Animation\\n→ Anime → General Fan Pages → Oﬃcial Sites). In the case of gene ontol-\\nogy the relationships form a directed acyclic graph, also referred to as the\\nGO-DAG [ABB+00].\\nBoth examples above describe estimation problems where our observations', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f2da1a5e-9463-477c-952f-8b83766b82db', embedding=None, metadata={'page_label': '9', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1 A Taste of Machine Learning 9\\nare vertices of a tree or graph. However, graphs themselves may be the\\nobservations. For instance, the DOM-tree of a webpage, the call-graph of\\na computer program, or the protein-protein interaction networks may form\\nthe basis upon which we may want to perform inference.\\nStrings occur frequently, mainly in the area of bioinformatics and natural\\nlanguage processing. They may be the input to our estimation problems, e.g.\\nwhen classifying an e-mail as spam, when attempting to locate all names of\\npersons and organizations in a text, or when modeling the topic structure\\nof a document. Equally well they may constitute the output of a system.\\nFor instance, we may want to perform document summarization, automatic\\ntranslation, or attempt to answer natural language queries.\\nCompound structures are the most commonly occurring object. That\\nis, in most situations we will have a structured mix of diﬀerent data types.\\nFor instance, a webpage might contain images, text, tables, which in turn\\ncontain numbers, and lists, all of which might constitute nodes on a graph of\\nwebpages linked among each other. Good statistical modelling takes such de-\\npendencies and structures into account in order to tailor suﬃciently ﬂexible\\nmodels.\\n1.1.3 Problems\\nThe range of learning problems is clearly large, as we saw when discussing\\napplications. That said, researchers have identiﬁed an ever growing number\\nof templates which can be used to address a large set of situations. It is those\\ntemplates which make deployment of machine learning in practice easy and\\nour discussion will largely focus on a choice set of such problems. We now\\ngive a by no means complete list of templates.\\nBinary Classiﬁcation is probably the most frequently studied problem\\nin machine learning and it has led to a large number of important algorithmic\\nand theoretic developments over the past century. In its simplest form it\\nreduces to the question: given a pattern x drawn from a domain X, estimate\\nwhich value an associated binary random variable y ∈ {± 1} will assume.\\nFor instance, given pictures of apples and oranges, we might want to state\\nwhether the object in question is an apple or an orange. Equally well, we\\nmight want to predict whether a home owner might default on his loan,\\ngiven income data, his credit history, or whether a given e-mail is spam or\\nham. The ability to solve this basic problem already allows us to address a\\nlarge variety of practical settings.\\nThere are many variants exist with regard to the protocol in which we are\\nrequired to make our estimation:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='43d7b863-5f50-49cc-ad20-d8d621e64d43', embedding=None, metadata={'page_label': '10', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='10 1 Introduction\\nFig. 1.6. Left: binary classiﬁcation. Right: 3-class classiﬁcation. Note that in the\\nlatter case we have much more degree for ambiguity. For instance, being able to\\ndistinguish stars from diamonds may not suﬃce to identify either of them correctly,\\nsince we also need to distinguish both of them from triangles.\\n• We might see a sequence of (xi, yi) pairs for which yi needs to be estimated\\nin an instantaneous online fashion. This is commonly referred to as online\\nlearning.\\n• We might observe a collection X := {x1, . . . xm} and Y := {y1, . . . ym} of\\npairs (xi, yi) which are then used to estimate y for a (set of) so-far unseen\\nX′ =\\n{\\nx′\\n1, . . . , x′\\nm′\\n}\\n. This is commonly referred to as batch learning.\\n• We might be allowed to know X′ already at the time of constructing the\\nmodel. This is commonly referred to as transduction.\\n• We might be allowed to choose X for the purpose of model building. This\\nis known as active learning.\\n• We might not have full information about X, e.g. some of the coordinates\\nof the xi might be missing, leading to the problem of estimation with\\nmissing variables.\\n• The sets X and X′ might come from diﬀerent data sources, leading to the\\nproblem of covariate shift correction.\\n• We might be given observations stemming from two problems at the same\\ntime with the side information that both problems are somehow related.\\nThis is known as co-training.\\n• Mistakes of estimation might be penalized diﬀerently depending on the\\ntype of error, e.g. when trying to distinguish diamonds from rocks a very\\nasymmetric loss applies.\\nMulticlass Classiﬁcation is the logical extension of binary classiﬁca-\\ntion. The main diﬀerence is that now y ∈ { 1, . . . , n} may assume a range\\nof diﬀerent values. For instance, we might want to classify a document ac-\\ncording to the language it was written in (English, French, German, Spanish,\\nHindi, Japanese, Chinese, . . . ). See Figure 1.6 for an example. The main dif-\\nference to before is that the cost of error may heavily depend on the type of', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c9ace372-b91f-49b2-9bb7-3e638ceed200', embedding=None, metadata={'page_label': '11', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.1 A Taste of Machine Learning 11\\nFig. 1.7. Regression estimation. We are given a number of instances (indicated by\\nblack dots) and would like to ﬁnd some function f mapping the observations X to\\nR such that f(x) is close to the observed values.\\nerror we make. For instance, in the problem of assessing the risk of cancer, it\\nmakes a signiﬁcant diﬀerence whether we mis-classify an early stage of can-\\ncer as healthy (in which case the patient is likely to die) or as an advanced\\nstage of cancer (in which case the patient is likely to be inconvenienced from\\noverly aggressive treatment).\\nStructured Estimation goes beyond simple multiclass estimation by\\nassuming that the labels y have some additional structure which can be used\\nin the estimation process. For instance, y might be a path in an ontology,\\nwhen attempting to classify webpages, y might be a permutation, when\\nattempting to match objects, to perform collaborative ﬁltering, or to rank\\ndocuments in a retrieval setting. Equally well, y might be an annotation of\\na text, when performing named entity recognition. Each of those problems\\nhas its own properties in terms of the set of y which we might consider\\nadmissible, or how to search this space. We will discuss a number of those\\nproblems in Chapter ??.\\nRegression is another prototypical application. Here the goal is to esti-\\nmate a real-valued variable y ∈ R given a pattern x (see e.g. Figure 1.7). For\\ninstance, we might want to estimate the value of a stock the next day, the\\nyield of a semiconductor fab given the current process, the iron content of\\nore given mass spectroscopy measurements, or the heart rate of an athlete,\\ngiven accelerometer data. One of the key issues in which regression problems\\ndiﬀer from each other is the choice of a loss. For instance, when estimating\\nstock values our loss for a put option will be decidedly one-sided. On the\\nother hand, a hobby athlete might only care that our estimate of the heart\\nrate matches the actual on average.\\nNovelty Detection is a rather ill-deﬁned problem. It describes the issue\\nof determining “unusual” observations given a set of past measurements.\\nClearly, the choice of what is to be considered unusual is very subjective.\\nA commonly accepted notion is that unusual events occur rarely. Hence a\\npossible goal is to design a system which assigns to each observation a rating', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9cbf70f9-c379-456e-b777-3401c6206c69', embedding=None, metadata={'page_label': '12', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='12 1 Introduction\\nFig. 1.8. Left: typical digits contained in the database of the US Postal Service.\\nRight: unusual digits found by a novelty detection algorithm [SPST +01] (for a\\ndescription of the algorithm see Section 7.4). The score below the digits indicates\\nthe degree of novelty. The numbers on the lower right indicate the class associated\\nwith the digit.\\nas to how novel it is. Readers familiar with density estimation might contend\\nthat the latter would be a reasonable solution. However, we neither need a\\nscore which sums up to 1 on the entire domain, nor do we care particularly\\nmuch about novelty scores fortypical observations. We will later see how this\\nsomewhat easier goal can be achieved directly. Figure 1.8 has an example of\\nnovelty detection when applied to an optical character recognition database.\\n1.2 Probability Theory\\nIn order to deal with the instances of where machine learning can be used, we\\nneed to develop an adequate language which is able to describe the problems\\nconcisely. Below we begin with a fairly informal overview over probability\\ntheory. For more details and a very gentle and detailed discussion see the\\nexcellent book of [BT03].\\n1.2.1 Random Variables\\nAssume that we cast a dice and we would like to know our chances whether\\nwe would see 1 rather than another digit. If the dice is fair all six outcomes\\nX = {1, . . . ,6} are equally likely to occur, hence we would see a 1 in roughly\\n1 out of 6 cases. Probability theory allows us to model uncertainty in the out-\\ncome of such experiments. Formally we state that 1 occurs with probability\\n1\\n6.\\nIn many experiments, such as the roll of a dice, the outcomes are of a\\nnumerical nature and we can handle them easily. In other cases, the outcomes\\nmay not be numerical, e.g., if we toss a coin and observe heads or tails. In\\nthese cases, it is useful to associate numerical values to the outcomes. This\\nis done via a random variable. For instance, we can let a random variable', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c4ad4543-c6a0-4d00-ad17-bba494775931', embedding=None, metadata={'page_label': '13', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Probability Theory 13\\nX take on a value +1 whenever the coin lands heads and a value of −1\\notherwise. Our notational convention will be to use uppercase letters, e.g.,\\nX, Y etc to denote random variables and lower case letters, e.g., x, y etc to\\ndenote the values they take.\\nX\\nweight\\nheight\\nξ(x)\\nx\\nFig. 1.9. The random variable ξ maps from the set of outcomes of an experiment\\n(denoted here by X) to real numbers. As an illustration here X consists of the\\npatients a physician might encounter, and they are mapped via ξ to their weight\\nand height.\\n1.2.2 Distributions\\nPerhaps the most important way to characterize a random variable is to\\nassociate probabilities with the values it can take. If the random variable is\\ndiscrete, i.e., it takes on a ﬁnite number of values, then this assignment of\\nprobabilities is called a probability mass function or PMF for short. A PMF\\nmust be, by deﬁnition, non-negative and must sum to one. For instance,\\nif the coin is fair, i.e., heads and tails are equally likely, then the random\\nvariable X described above takes on values of +1 and −1 with probability\\n0.5. This can be written as\\nP r(X = +1) = 0.5 and P r(X = −1) = 0.5. (1.1)\\nWhen there is no danger of confusion we will use the slightly informal no-\\ntation p(x) := P r(X = x).\\nIn case of a continuous random variable the assignment of probabilities\\nresults in a probability density function or PDF for short. With some abuse\\nof terminology, but keeping in line with convention, we will often use density\\nor distribution instead of probability density function. As in the case of the\\nPMF, a PDF must also be non-negative and integrate to one. Figure 1.10\\nshows two distributions: the uniform distribution\\np(x) =\\n{\\n1\\nb−a if x ∈ [a, b]\\n0 otherwise ,\\n(1.2)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b87a360d-8a30-4fee-b79a-dbeec4dff13e', embedding=None, metadata={'page_label': '14', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='14 1 Introduction\\n-4\\n -2\\n 0\\n 2\\n 4\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n-4\\n -2\\n 0\\n 2\\n 4\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nFig. 1.10. Two common densities. Left: uniform distribution over the interval\\n[−1, 1]. Right: Normal distribution with zero mean and unit variance.\\nand the Gaussian distribution (also called normal distribution)\\np(x) = 1√\\n2πσ2 exp\\n(\\n−(x − µ)2\\n2σ2\\n)\\n. (1.3)\\nClosely associated with a PDF is the indeﬁnite integral over p. It is com-\\nmonly referred to as the cumulative distribution function (CDF).\\nDeﬁnition 1.1 (Cumulative Distribution Function) For a real valued\\nrandom variable X with PDF p the associated Cumulative Distribution Func-\\ntion F is given by\\nF (x′) := Pr\\n{\\nX ≤ x′}\\n=\\n∫ x′\\n−∞\\ndp(x). (1.4)\\nThe CDF F (x′) allows us to perform range queries on p eﬃciently. For\\ninstance, by integral calculus we obtain\\nPr(a ≤ X ≤ b) =\\n∫ b\\na\\ndp(x) = F (b) − F (a). (1.5)\\nThe values of x′ for which F (x′) assumes a speciﬁc value, such as 0 .1 or 0.5\\nhave a special name. They are called the quantiles of the distribution p.\\nDeﬁnition 1.2 (Quantiles) Let q ∈ (0, 1). Then the value of x′ for which\\nPr(X < x′) ≤ q and Pr(X > x′) ≤ 1 − q is the q-quantile of the distribution\\np. Moreover, the value x′ associated with q = 0.5 is called the median.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='478e4ea1-1b6e-4e88-8f3c-4352db972949', embedding=None, metadata={'page_label': '15', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Probability Theory 15\\np(x)\\nFig. 1.11. Quantiles of a distribution correspond to the area under the integral of\\nthe density p(x) for which the integral takes on a pre-speciﬁed value. Illustrated\\nare the 0.1, 0.5 and 0.9 quantiles respectively.\\n1.2.3 Mean and Variance\\nA common question to ask about a random variable is what its expected\\nvalue might be. For instance, when measuring the voltage of a device, we\\nmight ask what its typical values might be. When deciding whether to ad-\\nminister a growth hormone to a child a doctor might ask what a sensible\\nrange of height should be. For those purposes we need to deﬁne expectations\\nand related quantities of distributions.\\nDeﬁnition 1.3 (Mean) We deﬁne the mean of a random variable X as\\nE[X] :=\\n∫\\nxdp(x) (1.6)\\nMore generally, if f : R → R is a function, then f(X) is also a random\\nvariable. Its mean is mean given by\\nE[f(X)] :=\\n∫\\nf(x)dp(x). (1.7)\\nWhenever X is a discrete random variable the integral in (1.6) can be re-\\nplaced by a summation:\\nE[X] =\\n∑\\nx\\nxp(x). (1.8)\\nFor instance, in the case of a dice we have equal probabilities of 1 /6 for all\\n6 possible outcomes. It is easy to see that this translates into a mean of\\n(1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5.\\nThe mean of a random variable is useful in assessing expected losses and\\nbeneﬁts. For instance, as a stock broker we might be interested in the ex-\\npected value of our investment in a year’s time. In addition to that, however,\\nwe also might want to investigate the risk of our investment. That is, how\\nlikely it is that the value of the investment might deviate from its expecta-\\ntion since this might be more relevant for our decisions. This means that we', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a38573f1-2e62-46cb-90c7-c53e21b2659e', embedding=None, metadata={'page_label': '16', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='16 1 Introduction\\nneed a variable to quantify the risk inherent in a random variable. One such\\nmeasure is the variance of a random variable.\\nDeﬁnition 1.4 (Variance) We deﬁne the variance of a random variable\\nX as\\nVar[X] := E\\n[\\n(X − E[X])2\\n]\\n. (1.9)\\nAs before, if f : R → R is a function, then the variance of f(X) is given by\\nVar[f(X)] := E\\n[\\n(f(X) − E[f(X)])2\\n]\\n. (1.10)\\nThe variance measures by how much on average f(X) deviates from its ex-\\npected value. As we shall see in Section 2.1, an upper bound on the variance\\ncan be used to give guarantees on the probability that f(X) will be within\\nϵ of its expected value. This is one of the reasons why the variance is often\\nassociated with the risk of a random variable. Note that often one discusses\\nproperties of a random variable in terms of its standard deviation, which is\\ndeﬁned as the square root of the variance.\\n1.2.4 Marginalization, Independence, Conditioning, and Bayes\\nRule\\nGiven two random variables X and Y , one can write their joint density\\np(x, y). Given the joint density, one can recover p(x) by integrating out y.\\nThis operation is called marginalization:\\np(x) =\\n∫\\ny\\ndp(x, y). (1.11)\\nIf Y is a discrete random variable, then we can replace the integration with\\na summation:\\np(x) =\\n∑\\ny\\np(x, y). (1.12)\\nWe say that X and Y are independent, i.e., the values that X takes does\\nnot depend on the values that Y takes whenever\\np(x, y) = p(x)p(y). (1.13)\\nIndependence is useful when it comes to dealing with large numbers of ran-\\ndom variables whose behavior we want to estimate jointly. For instance,\\nwhenever we perform repeated measurements of a quantity, such as when', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='91582c90-61eb-4469-bc63-3385a49f6fc2', embedding=None, metadata={'page_label': '17', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Probability Theory 17\\n-0.5\\n 0.0\\n 0.5\\n 1.0\\n 1.5 2.0-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n-0.5\\n 0.0\\n 0.5\\n 1.0\\n 1.5 2.0-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nFig. 1.12. Left: a sample from two dependent random variables. Knowing about\\nﬁrst coordinate allows us to improve our guess about the second coordinate. Right:\\na sample drawn from two independent random variables, obtained by randomly\\npermuting the dependent sample.\\nmeasuring the voltage of a device, we will typically assume that the individ-\\nual measurements are drawn from the same distribution and that they are\\nindependent of each other. That is, having measured the voltage a number\\nof times will not aﬀect the value of the next measurement. We will call such\\nrandom variables to be independently and identically distributed, or in short,\\niid random variables. See Figure 1.12 for an example of a pair of random\\nvariables drawn from dependent and independent distributions respectively.\\nConversely, dependence can be vital in classiﬁcation and regression prob-\\nlems. For instance, the traﬃc lights at an intersection are dependent of each\\nother. This allows a driver to perform the inference that when the lights are\\ngreen in his direction there will be no traﬃc crossing his path, i.e. the other\\nlights will indeed be red. Likewise, whenever we are given a picture x of a\\ndigit, we hope that there will be dependence between x and its label y.\\nEspecially in the case of dependent random variables, we are interested\\nin conditional probabilities, i.e., probability that X takes on a particular\\nvalue given the value of Y . Clearly P r(X = rain|Y = cloudy) is higher than\\nP r(X = rain|Y = sunny). In other words, knowledge about the value of Y\\nsigniﬁcantly inﬂuences the distribution ofX. This is captured via conditional\\nprobabilities:\\np(x|y) := p(x, y)\\np(y) . (1.14)\\nEquation 1.14 leads to one of the key tools in statistical inference.\\nTheorem 1.5 (Bayes Rule) Denote by X and Y random variables then', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='03c4e5b3-1a81-443d-9b36-325adea7446b', embedding=None, metadata={'page_label': '18', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='18 1 Introduction\\nthe following holds\\np(y|x) = p(x|y)p(y)\\np(x) . (1.15)\\nThis follows from the fact that p(x, y) = p(x|y)p(y) = p(y|x)p(x). The key\\nconsequence of (1.15) is that we may reverse the conditioning between a\\npair of random variables.\\n1.2.4.1 An Example\\nWe illustrate our reasoning by means of a simple example — inference using\\nan AIDS test. Assume that a patient would like to have such a test carried\\nout on him. The physician recommends a test which is guaranteed to detect\\nHIV-positive whenever a patient is infected. On the other hand, for healthy\\npatients it has a 1% error rate. That is, with probability 0.01 it diagnoses\\na patient as HIV-positive even when he is, in fact, HIV-negative. Moreover,\\nassume that 0.15% of the population is infected.\\nNow assume that the patient has the test carried out and the test re-\\nturns ’HIV-negative’. In this case, logic implies that he is healthy, since the\\ntest has 100% detection rate. In the converse case things are not quite as\\nstraightforward. Denote by X and T the random variables associated with\\nthe health status of the patient and the outcome of the test respectively. We\\nare interested in p(X = HIV+|T = HIV+). By Bayes rule we may write\\np(X = HIV+|T = HIV+) = p(T = HIV+|X = HIV+)p(X = HIV+)\\np(T = HIV+)\\nWhile we know all terms in the numerator, p(T = HIV+) itself is unknown.\\nThat said, it can be computed via\\np(T = HIV+) =\\n∑\\nx∈{HIV+,HIV-}\\np(T = HIV+, x)\\n=\\n∑\\nx∈{HIV+,HIV-}\\np(T = HIV+|x)p(x)\\n= 1.0 · 0.0015 + 0.01 · 0.9985.\\nSubstituting back into the conditional expression yields\\np(X = HIV+|T = HIV+) = 1.0 · 0.0015\\n1.0 · 0.0015 + 0.01 · 0.9985 = 0.1306.\\nIn other words, even though our test is quite reliable, there is such a low\\nprior probability of having been infected with AIDS that there is not much\\nevidence to accept the hypothesis even after this test.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='31f66ba0-fdbd-450e-ace0-0590c80e54fd', embedding=None, metadata={'page_label': '19', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.2 Probability Theory 19\\nage x\\ntest 1\\ntest 2\\nFig. 1.13. A graphical description of our HIV testing scenario. Knowing the age of\\nthe patient inﬂuences our prior on whether the patient is HIV positive (the random\\nvariable X). The outcomes of the tests 1 and 2 are independent of each other given\\nthe status X. We observe the shaded random variables (age, test 1, test 2) and\\nwould like to infer the un-shaded random variable X. This is a special case of a\\ngraphical model which we will discuss in Chapter ??.\\nLet us now think how we could improve the diagnosis. One way is to ob-\\ntain further information about the patient and to use this in the diagnosis.\\nFor instance, information about his age is quite useful. Suppose the patient\\nis 35 years old. In this case we would want to compute p(X = HIV+|T =\\nHIV+, A = 35) where the random variable A denotes the age. The corre-\\nsponding expression yields:\\np(T = HIV+|X = HIV+, A)p(X = HIV+|A)\\np(T = HIV+|A)\\nHere we simply conditioned all random variables on A in order to take addi-\\ntional information into account. We may assume that the test isindependent\\nof the age of the patient, i.e.\\np(t|x, a) = p(t|x).\\nWhat remains therefore is p(X = HIV+|A). Recent US census data pegs this\\nnumber at approximately 0.9%. Plugging all data back into the conditional\\nexpression yields 1·0.009\\n1·0.009+0.01·0.991 = 0 .48. What has happened here is that\\nby including additional observed random variables our estimate has become\\nmore reliable. Combination of evidence is a powerful tool. In our case it\\nhelped us make the classiﬁcation problem of whether the patient is HIV-\\npositive or not more reliable.\\nA second tool in our arsenal is the use of multiple measurements. After\\nthe ﬁrst test the physician is likely to carry out a second test to conﬁrm the\\ndiagnosis. We denote by T1 and T2 (and t1, t2 respectively) the two tests.\\nObviously, what we want is that T2 will give us an “independent” second\\nopinion of the situation. In other words, we want to ensure that T2 does\\nnot make the same mistakes as T1. For instance, it is probably a bad idea\\nto repeat T1 without changes, since it might perform the same diagnostic', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='be21252c-4cf7-488e-9487-28a6da867dda', embedding=None, metadata={'page_label': '20', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='20 1 Introduction\\nmistake as before. What we want is that the diagnosis of T2 is independent\\nof that of T2 given the health status X of the patient. This is expressed as\\np(t1, t2|x) = p(t1|x)p(t2|x). (1.16)\\nSee Figure 1.13 for a graphical illustration of the setting. Random variables\\nsatisfying the condition (1.16) are commonly referred to as conditionally\\nindependent. In shorthand we writeT1, T2 ⊥ ⊥X. For the sake of the argument\\nwe assume that the statistics for T2 are given by\\np(t2|x) x = HIV- x = HIV+\\nt2 = HIV- 0.95 0.01\\nt2 = HIV+ 0.05 0.99\\nClearly this test is less reliable than the ﬁrst one. However, we may now\\ncombine both estimates to obtain a very reliable estimate based on the\\ncombination of both events. For instance, for t1 = t2 = HIV+ we have\\np(X = HIV+|T1 = HIV+, T2 = HIV+) = 1.0 · 0.99 · 0.009\\n1.0 · 0.99 · 0.009 + 0.01 · 0.05 · 0.991 = 0.95.\\nIn other words, by combining two tests we can now conﬁrm with very high\\nconﬁdence that the patient is indeed diseased. What we have carried out is a\\ncombination of evidence. Strong experimental evidence of two positive tests\\neﬀectively overcame an initially very strong prior which suggested that the\\npatient might be healthy.\\nTests such as in the example we just discussed are fairly common. For\\ninstance, we might need to decide which manufacturing procedure is prefer-\\nable, which choice of parameters will give better results in a regression es-\\ntimator, or whether to administer a certain drug. Note that often our tests\\nmay not be conditionally independent and we would need to take this into\\naccount.\\n1.3 Basic Algorithms\\nWe conclude our introduction to machine learning by discussing four simple\\nalgorithms, namely Naive Bayes, Nearest Neighbors, the Mean Classiﬁer,\\nand the Perceptron, which can be used to solve a binary classiﬁcation prob-\\nlem such as that described in Figure 1.5. We will also introduce the K-means\\nalgorithm which can be employed when labeled data is not available. All\\nthese algorithms are readily usable and easily implemented from scratch in\\ntheir most basic form.\\nFor the sake of concreteness assume that we are interested in spam ﬁlter-\\ning. That is, we are given a set ofm e-mails xi, denoted byX := {x1, . . . , xm}', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b3b129a7-6c86-4f83-a1ac-65a2f71e7dfc', embedding=None, metadata={'page_label': '21', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 21\\nFrom: \"LucindaParkison497072\" <LucindaParkison497072@hotmail.com>\\nTo: <kargr@earthlink.net>\\nSubject: we think ACGU is our next winner\\nDate: Mon, 25 Feb 2008 00:01:01 -0500\\nMIME-Version: 1.0\\nX-OriginalArrivalTime: 25 Feb 2008 05:01:01.0329 (UTC) FILETIME=[6A931810:01C8776B]\\nReturn-Path: lucindaparkison497072@hotmail.com\\n(ACGU) .045 UP 104.5%\\nI do think that (ACGU) at it’s current levels looks extremely attractive.\\nAsset Capital Group, Inc., (ACGU) announced that it is expanding the marketing of bio-remediation fluids and cleaning equipment. After\\nits recent acquisition of interest in American Bio-Clean Corporation and an 80\\nNews is expected to be released next week on this growing company and could drive the price even higher. Buy (ACGU) Monday at open. I\\nbelieve those involved at this stage could enjoy a nice ride up.\\nFig. 1.14. Example of a spam e-mail\\nx1: The quick brown fox jumped over the lazy dog.\\nx2: The dog hunts a fox.\\nthe quick brown fox jumped over lazy dog hunts a\\nx1 2 1 1 1 1 1 1 1 0 0\\nx2 1 0 0 1 0 0 0 1 1 1\\nFig. 1.15. Vector space representation of strings.\\nand associated labels yi, denoted by Y := {y1, . . . , ym}. Here the labels sat-\\nisfy yi ∈ {spam, ham}. The key assumption we make here is that the pairs\\n(xi, yi) are drawn jointly from some distribution p(x, y) which represents\\nthe e-mail generating process for a user. Moreover, we assume that there\\nis suﬃciently strong dependence between x and y that we will be able to\\nestimate y given x and a set of labeled instances X, Y.\\nBefore we do so we need to address the fact that e-mails such as Figure 1.14\\nare text, whereas the three algorithms we present will require data to be\\nrepresented in a vectorial fashion. One way of converting text into a vector\\nis by using the so-called bag of words representation [Mar61, Lew98]. In its\\nsimplest version it works as follows: Assume we have a list of all possible\\nwords occurring inX, that is a dictionary, then we are able to assign a unique\\nnumber with each of those words (e.g. the position in the dictionary). Now\\nwe may simply count for each document xi the number of times a given\\nword j is occurring. This is then used as the value of the j-th coordinate\\nof xi. Figure 1.15 gives an example of such a representation. Once we have\\nthe latter it is easy to compute distances, similarities, and other statistics\\ndirectly from the vectorial representation.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7c5c2fea-c36a-4379-97e4-6f57c034194a', embedding=None, metadata={'page_label': '22', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='22 1 Introduction\\n1.3.1 Naive Bayes\\nIn the example of the AIDS test we used the outcomes of the test to infer\\nwhether the patient is diseased. In the context of spam ﬁltering the actual\\ntext of the e-mail x corresponds to the test and the label y is equivalent to\\nthe diagnosis. Recall Bayes Rule (1.15). We could use the latter to infer\\np(y|x) = p(x|y)p(y)\\np(x) .\\nWe may have a good estimate of p(y), that is, the probability of receiving\\na spam or ham mail. Denote by mham and mspam the number of ham and\\nspam e-mails in X. In this case we can estimate\\np(ham) ≈ mham\\nm and p(spam) ≈ mspam\\nm .\\nThe key problem, however, is that we do not know p(x|y) or p(x). We may\\ndispose of the requirement of knowing p(x) by settling for a likelihood ratio\\nL(x) := p(spam|x)\\np(ham|x) = p(x|spam)p(spam)\\np(x|ham)p(ham) . (1.17)\\nWhenever L(x) exceeds a given threshold c we decide that x is spam and\\nconsequently reject the e-mail. If c is large then our algorithm is conservative\\nand classiﬁes an email as spam only if p(spam|x) ≫ p(ham|x). On the other\\nhand, if c is small then the algorithm aggressively classiﬁes emails as spam.\\nThe key obstacle is that we have no access top(x|y). This is where we make\\nour key approximation. Recall Figure 1.13. In order to model the distribution\\nof the test outcomes T1 and T2 we made the assumption that they are\\nconditionally independent of each other given the diagnosis. Analogously,\\nwe may now treat the occurrence of each word in a document as a separate\\ntest and combine the outcomes in a naive fashion by assuming that\\np(x|y) =\\n# of words in x∏\\nj=1\\np(wj|y), (1.18)\\nwhere wj denotes the j-th word in document x. This amounts to the as-\\nsumption that the probability of occurrence of a word in a document is\\nindependent of all other words given the category of the document. Even\\nthough this assumption does not hold in general – for instance, the word\\n“York” is much more likely to after the word “New” – it suﬃces for our\\npurposes (see Figure 1.16).\\nThis assumption reduces the diﬃculty of knowing p(x|y) to that of esti-\\nmating the probabilities of occurrence of individual words w. Estimates for', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7efd0649-a65f-447b-9073-f80687c2ef38', embedding=None, metadata={'page_label': '23', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 23\\ny\\nword 1 word 2 ...word nword 3\\nFig. 1.16. Naive Bayes model. The occurrence of individual words is independent\\nof each other, given the category of the text. For instance, the word Viagra is fairly\\nfrequent if y = spam but it is considerably less frequent if y = ham, except when\\nconsidering the mailbox of a Pﬁzer sales representative.\\np(w|y) can be obtained, for instance, by simply counting the frequency oc-\\ncurrence of the word within documents of a given class. That is, we estimate\\np(w|spam) ≈\\n∑m\\ni=1\\n∑# of words in xi\\nj=1\\n{\\nyi = spam and wj\\ni = w\\n}\\n∑m\\ni=1\\n∑# of words in xi\\nj=1 {yi = spam}\\nHere\\n{\\nyi = spam and wj\\ni = w\\n}\\nequals 1 if and only if xi is labeled as spam\\nand w occurs as the j-th word in xi. The denominator is simply the total\\nnumber of words in spam documents. Similarly one can compute p(w|ham).\\nIn principle we could perform the above summation whenever we see a new\\ndocument x. This would be terribly ineﬃcient, since each such computation\\nrequires a full pass through X and Y. Instead, we can perform a single pass\\nthrough X and Y and store the resulting statistics as a good estimate of the\\nconditional probabilities. Algorithm 1.1 has details of an implementation.\\nNote that we performed a number of optimizations: Firstly, the normaliza-\\ntion by m−1\\nspam and m−1\\nham respectively is independent of x, hence we incor-\\nporate it as a ﬁxed oﬀset. Secondly, since we are computing a product over\\na large number of factors the numbers might lead to numerical overﬂow or\\nunderﬂow. This can be addressed by summing over the logarithm of terms\\nrather than computing products. Thirdly, we need to address the issue of\\nestimating p(w|y) for words w which we might not have seen before. One\\nway of dealing with this is to increment all counts by 1. This method is\\ncommonly referred to as Laplace smoothing. We will encounter a theoretical\\njustiﬁcation for this heuristic in Section 2.3.\\nThis simple algorithm is known to perform surprisingly well, and variants\\nof it can be found in most modern spam ﬁlters. It amounts to what is\\ncommonly known as “Bayesian spam ﬁltering”. Obviously, we may apply it\\nto problems other than document categorization, too.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cc6cffdf-c2ea-4bed-8b73-4350b54b5736', embedding=None, metadata={'page_label': '24', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='24 1 Introduction\\nAlgorithm 1.1 Naive Bayes\\nTrain(X, Y) {reads documents X and labels Y}\\nCompute dictionary D of X with n words.\\nCompute m, mham and mspam.\\nInitialize b := log c+log mham −log mspam to oﬀset the rejection threshold\\nInitialize p ∈ R2×n with pij = 1, wspam = n, wham = n.\\n{Count occurrence of each word }\\n{Here xj\\ni denotes the number of times word j occurs in document xi}\\nfor i = 1 to m do\\nif yi = spam then\\nfor j = 1 to n do\\np0,j ← p0,j + xj\\ni\\nwspam ← wspam + xj\\ni\\nend for\\nelse\\nfor j = 1 to n do\\np1,j ← p1,j + xj\\ni\\nwham ← wham + xj\\ni\\nend for\\nend if\\nend for\\n{Normalize counts to yield word probabilities }\\nfor j = 1 to n do\\np0,j ← p0,j/wspam\\np1,j ← p1,j/wham\\nend for\\nClassify(x) {classiﬁes document x}\\nInitialize score threshold t = −b\\nfor j = 1 to n do\\nt ← t + xj(log p0,j − log p1,j)\\nend for\\nif t > 0 return spam else return ham\\n1.3.2 Nearest Neighbor Estimators\\nAn even simpler estimator than Naive Bayes is nearest neighbors. In its most\\nbasic form it assigns the label of its nearest neighbor to an observation x\\n(see Figure 1.17). Hence, all we need to implement it is a distance measure\\nd(x, x′) between pairs of observations. Note that this distance need not even\\nbe symmetric. This means that nearest neighbor classiﬁers can be extremely', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='98496d03-280a-47cd-a4cb-f42e9211f437', embedding=None, metadata={'page_label': '25', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 25\\nFig. 1.17. 1 nearest neighbor classiﬁer. Depending on whether the query point x is\\nclosest to the star, diamond or triangles, it uses one of the three labels for it.\\nFig. 1.18. k-Nearest neighbor classiﬁers using Euclidean distances. Left: decision\\nboundaries obtained from a 1-nearest neighbor classiﬁer. Middle: color-coded sets\\nof where the number of red / blue points ranges between 7 and 0. Right: decision\\nboundary determining where the blue or red dots are in the majority.\\nﬂexible. For instance, we could use string edit distances to compare two\\ndocuments or information theory based measures.\\nHowever, the problem with nearest neighbor classiﬁcation is that the esti-\\nmates can be very noisy whenever the data itself is very noisy. For instance,\\nif a spam email is erroneously labeled as nonspam then all emails which\\nare similar to this email will share the same fate. See Figure 1.18 for an\\nexample. In this case it is beneﬁcial to pool together a number of neighbors,\\nsay the k-nearest neighbors of x and use a majority vote to decide the class\\nmembership of x. Algorithm 1.2 has a description of the algorithm. Note\\nthat nearest neighbor algorithms can yield excellent performance when used\\nwith a good distance measure. For instance, the technology underlying the\\nNetﬂix progress prize [BK07] was essentially nearest neighbours based.\\nNote that it is trivial to extend the algorithm to regression. All we need\\nto change in Algorithm 1.2 is to return the average of the values yi instead\\nof their majority vote. Figure 1.19 has an example.\\nNote that the distance computation d(xi, x) for all observations can be-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d372be11-6f2d-4ef3-aa76-b5ff6b971206', embedding=None, metadata={'page_label': '26', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='26 1 Introduction\\nAlgorithm 1.2 k-Nearest Neighbor Classiﬁcation\\nClassify(X, Y, x) {reads documents X, labels Y and query x}\\nfor i = 1 to m do\\nCompute distance d(xi, x)\\nend for\\nCompute set I containing indices for the k smallest distances d(xi, x).\\nreturn majority label of {yi where i ∈ I}.\\nFig. 1.19. k-Nearest neighbor regression estimator using Euclidean distances. Left:\\nsome points ( x, y) drawn from a joint distribution. Middle: 1-nearest neighbour\\nclassiﬁer. Right: 7-nearest neighbour classiﬁer. Note that the regression estimate is\\nmuch more smooth.\\ncome extremely costly, in particular whenever the number of observations is\\nlarge or whenever the observations xi live in a very high dimensional space.\\nRandom projections are a technique that can alleviate the high computa-\\ntional cost of Nearest Neighbor classiﬁers. A celebrated lemma by Johnson\\nand Lindenstrauss [DG03] asserts that a set of m points in high dimensional\\nEuclidean space can be projected into a O(log m/ϵ2) dimensional Euclidean\\nspace such that the distance between any two points changes only by a fac-\\ntor of (1 ± ϵ). Since Euclidean distances are preserved, running the Nearest\\nNeighbor classiﬁer on this mapped data yields the same results but at a\\nlower computational cost [GIM99].\\nThe surprising fact is that the projection relies on a simple randomized\\nalgorithm: to obtain a d-dimensional representation of n-dimensional ran-\\ndom observations we pick a matrix R ∈ Rd×n where each element is drawn\\nindependently from a normal distribution with n− 1\\n2 variance and zero mean.\\nMultiplying x with this projection matrix can be shown to achieve this prop-\\nerty with high probability. For details see [DG03].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3fa97942-6a34-4c2e-baad-977280c1bf19', embedding=None, metadata={'page_label': '27', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 27\\nw\\nμ-\\nμ+\\nx\\nFig. 1.20. A trivial classiﬁer. Classiﬁcation is carried out in accordance to which of\\nthe two means µ− or µ+ is closer to the test point x. Note that the sets of positive\\nand negative labels respectively form a half space.\\n1.3.3 A Simple Classiﬁer\\nWe can use geometry to design another simple classiﬁcation algorithm [SS02]\\nfor our problem. For simplicity we assume that the observationsx ∈ Rd, such\\nas the bag-of-words representation of e-mails. We deﬁne the means µ+ and\\nµ− to correspond to the classes y ∈ {±1} via\\nµ− := 1\\nm−\\n∑\\nyi=−1\\nxi and µ+ := 1\\nm+\\n∑\\nyi=1\\nxi.\\nHere we used m− and m+ to denote the number of observations with label\\nyi = −1 and yi = +1 respectively. An even simpler approach than using the\\nnearest neighbor classiﬁer would be to use the class label which corresponds\\nto the mean closest to a new query x, as described in Figure 1.20.\\nFor Euclidean distances we have\\n∥µ− − x∥2 = ∥µ−∥2 + ∥x∥2 − 2 ⟨µ−, x⟩ and (1.19)\\n∥µ+ − x∥2 = ∥µ+∥2 + ∥x∥2 − 2 ⟨µ+, x⟩ . (1.20)\\nHere ⟨·, ·⟩ denotes the standard dot product between vectors. Taking diﬀer-\\nences between the two distances yields\\nf(x) := ∥µ+ − x∥2 − ∥µ− − x∥2 = 2 ⟨µ− − µ+, x⟩ + ∥µ−∥2 − ∥µ+∥2 .\\n(1.21)\\nThis is a linear function in x and its sign corresponds to the labels we esti-\\nmate for x. Our algorithm sports an important property: The classiﬁcation\\nrule can be expressed via dot products. This follows from\\n∥µ+∥2 = ⟨µ+, µ+⟩ = m−2\\n+\\n∑\\nyi=yj=1\\n⟨xi, xj⟩ and ⟨µ+, x⟩ = m−1\\n+\\n∑\\nyi=1\\n⟨xi, x⟩ .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b20c623c-3911-4c4d-9c5e-52fad101e1b0', embedding=None, metadata={'page_label': '28', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='28 1 Introduction\\nX\\nφ(x)x\\nH\\nFig. 1.21. The feature map φ maps observations x from X into a feature space H.\\nThe map φ is a convenient way of encoding pre-processing steps systematically.\\nAnalogous expressions can be computed for µ−. Consequently we may ex-\\npress the classiﬁcation rule (1.21) as\\nf(x) =\\nm∑\\ni=1\\nαi ⟨xi, x⟩ + b (1.22)\\nwhere b = m−2\\n−\\n∑\\nyi=yj=−1 ⟨xi, xj⟩ − m−2\\n+\\n∑\\nyi=yj=1 ⟨xi, xj⟩ and αi = yi/myi.\\nThis oﬀers a number of interesting extensions. Recall that when dealing\\nwith documents we needed to perform pre-processing to map e-mails into a\\nvector space. In general, we may pick arbitrary maps φ : X → H mapping\\nthe space of observations into a feature space H, as long as the latter is\\nendowed with a dot product (see Figure 1.21). This means that instead of\\ndealing with ⟨x, x′⟩ we will be dealing with ⟨φ(x), φ(x′)⟩.\\nAs we will see in Chapter 6, wheneverH is a so-called Reproducing Kernel\\nHilbert Space, the inner product can be abbreviated in the form of a kernel\\nfunction k(x, x′) which satisﬁes\\nk(x, x′) :=\\n⟨\\nφ(x), φ(x′)\\n⟩\\n. (1.23)\\nThis small modiﬁcation leads to a number of very powerful algorithm and\\nit is at the foundation of an area of research called kernel methods. We\\nwill encounter a number of such algorithms for regression, classiﬁcation,\\nsegmentation, and density estimation over the course of the book. Examples\\nof suitable k are the polynomial kernel k(x, x′) = ⟨x, x′⟩d for d ∈ N and the\\nGaussian RBF kernel k(x, x′) = e−γ∥x−x′∥2\\nfor γ > 0.\\nThe upshot of (1.23) is that our basic algorithm can be kernelized. That\\nis, we may rewrite (1.21) as\\nf(x) =\\nm∑\\ni=1\\nαik(xi, x) + b (1.24)\\nwhere as before αi = yi/myi and the oﬀset b is computed analogously. As', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1c482273-2871-4efd-bde1-8b8096b60368', embedding=None, metadata={'page_label': '29', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 29\\nAlgorithm 1.3 The Perceptron\\nPerceptron(X, Y) {reads stream of observations ( xi, yi)}\\nInitialize w = 0 and b = 0\\nwhile There exists some ( xi, yi) with yi(⟨w, xi⟩ + b) ≤ 0 do\\nw ← w + yixi and b ← b + yi\\nend while\\nAlgorithm 1.4 The Kernel Perceptron\\nKernelPerceptron(X, Y) {reads stream of observations ( xi, yi)}\\nInitialize f = 0\\nwhile There exists some ( xi, yi) with yif(xi) ≤ 0 do\\nf ← f + yik(xi, ·) + yi\\nend while\\na consequence we have now moved from a fairly simple and pedestrian lin-\\near classiﬁer to one which yields a nonlinear function f(x) with a rather\\nnontrivial decision boundary.\\n1.3.4 Perceptron\\nIn the previous sections we assumed that our classiﬁer had access to a train-\\ning set of spam and non-spam emails. In real life, such a set might be diﬃcult\\nto obtain all at once. Instead, a user might want to haveinstant results when-\\never a new e-mail arrives and he would like the system to learn immediately\\nfrom any corrections to mistakes the system makes.\\nTo overcome both these diﬃculties one could envisage working with the\\nfollowing protocol: As emails arrive our algorithm classiﬁes them as spam or\\nnon-spam, and the user provides feedback as to whether the classiﬁcation is\\ncorrect or incorrect. This feedback is then used to improve the performance\\nof the classiﬁer over a period of time.\\nThis intuition can be formalized as follows: Our classiﬁer maintains a\\nparameter vector. At the t-th time instance it receives a data point xt, to\\nwhich it assigns a label ˆyt using its current parameter vector. The true label\\nyt is then revealed, and used to update the parameter vector of the classiﬁer.\\nSuch algorithms are said to be online. We will now describe perhaps the\\nsimplest classiﬁer of this kind namely the Perceptron [Heb49, Ros58].\\nLet us assume that the data points xt ∈ Rd, and labels yt ∈ {± 1}. As\\nbefore we represent an email as a bag-of-words vector and we assign +1 to\\nspam emails and −1 to non-spam emails. The Perceptron maintains a weight', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='083adbe7-c9dd-4da4-87eb-f40f4835cf08', embedding=None, metadata={'page_label': '30', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='30 1 Introduction\\nw*\\nwt\\nw*wt+1\\nxt xt\\nFig. 1.22. The Perceptron without bias. Left: at time t we have a weight vector wt\\ndenoted by the dashed arrow with corresponding separating plane (also dashed).\\nFor reference we include the linear separator w∗ and its separating plane (both\\ndenoted by a solid line). As a new observation xt arrives which happens to be\\nmis-classiﬁed by the current weight vector wt we perform an update. Also note the\\nmargin between the point xt and the separating hyperplane deﬁned by w∗. Right:\\nThis leads to the weight vector wt+1 which is more aligned with w∗.\\nvector w ∈ Rd and classiﬁes xt according to the rule\\nˆyt := sign{⟨w, xt⟩ + b}, (1.25)\\nwhere ⟨w, xt⟩ denotes the usual Euclidean dot product andb is an oﬀset. Note\\nthe similarity of (1.25) to (1.21) of the simple classiﬁer. Just as the latter,\\nthe Perceptron is a linear classiﬁer which separates its domain Rd into two\\nhalfspaces, namely {x| ⟨w, x⟩ + b > 0} and its complement. If ˆyt = yt then\\nno updates are made. On the other hand, if ˆ yt ̸= yt the weight vector is\\nupdated as\\nw ← w + ytxt and b ← b + yt. (1.26)\\nFigure 1.22 shows an update step of the Perceptron algorithm. For simplicity\\nwe illustrate the case without bias, that is, where b = 0 and where it remains\\nunchanged. A detailed description of the algorithm is given in Algorithm 1.3.\\nAn important property of the algorithm is that it performs updates on w\\nby multiples of the observations xi on which it makes a mistake. Hence we\\nmay express w as w = ∑\\ni∈Error yixi. Just as before, we can replace xi and x\\nby φ(xi) and φ(x) to obtain a kernelized version of the Perceptron algorithm\\n[FS99] (Algorithm 1.4).\\nIf the dataset (X, Y) is linearly separable, then the Perceptron algorithm', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6a3048c0-a484-46fc-81a5-02bb778f273e', embedding=None, metadata={'page_label': '31', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 31\\neventually converges and correctly classiﬁes all the points in X. The rate of\\nconvergence however depends on the margin. Roughly speaking, the margin\\nquantiﬁes how linearly separable a dataset is, and hence how easy it is to\\nsolve a given classiﬁcation problem.\\nDeﬁnition 1.6 (Margin) Let w ∈ Rd be a weight vector and let b ∈ R be\\nan oﬀset. The margin of an observation x ∈ Rd with associated label y is\\nγ(x, y) := y (⟨w, x⟩ + b) . (1.27)\\nMoreover, the margin of an entire set of observations X with labels Y is\\nγ(X, Y) := min\\ni\\nγ(xi, yi). (1.28)\\nGeometrically speaking (see Figure 1.22) the margin measures the distance\\nof x from the hyperplane deﬁned by {x| ⟨w, x⟩ + b = 0}. Larger the margin,\\nthe more well separated the data and hence easier it is to ﬁnd a hyperplane\\nwith correctly classiﬁes the dataset. The following theorem asserts that if\\nthere exists a linear classiﬁer which can classify a dataset with a large mar-\\ngin, then the Perceptron will also correctly classify the same dataset after\\nmaking a small number of mistakes.\\nTheorem 1.7 (Novikoﬀ’s theorem) Let (X, Y) be a dataset with at least\\none example labeled +1 and one example labeled −1. Let R := maxt ∥xt∥, and\\nassume that there exists (w∗, b∗) such that ∥w∗∥ = 1 and γt := yt(⟨w∗, xt⟩ +\\nb∗) ≥ γ for all t. Then, the Perceptron will make at most (1+R2)(1+(b∗)2)\\nγ2\\nmistakes.\\nThis result is remarkable since it does not depend on the dimensionality\\nof the problem. Instead, it only depends on the geometry of the setting,\\nas quantiﬁed via the margin γ and the radius R of a ball enclosing the\\nobservations. Interestingly, a similar bound can be shown for Support Vector\\nMachines [Vap95] which we will be discussing in Chapter 7.\\nProof We can safely ignore the iterations where no mistakes were made\\nand hence no updates were carried out. Therefore, without loss of generality\\nassume that the t-th update was made after seeing the t-th observation and\\nlet wt denote the weight vector after the update. Furthermore, for simplicity\\nassume that the algorithm started with w0 = 0 and b0 = 0. By the update\\nequation (1.26) we have\\n⟨wt, w∗⟩ + btb∗ = ⟨wt−1, w∗⟩ + bt−1b∗ + yt(⟨xt, w∗⟩ + b∗)\\n≥ ⟨wt−1, w∗⟩ + bt−1b∗ + γ.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='09090b24-fd35-4e23-a74d-aa8740ad59fb', embedding=None, metadata={'page_label': '32', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='32 1 Introduction\\nBy induction it follows that ⟨wt, w∗⟩+btb∗ ≥ tγ. On the other hand we made\\nan update because yt(⟨xt, wt−1⟩ + bt−1) < 0. By using ytyt = 1,\\n∥wt∥2 + b2\\nt = ∥wt−1∥2 + b2\\nt−1 + y2\\nt ∥xt∥2 + 1 + 2yt(⟨wt−1, xt⟩ + bt−1)\\n≤ ∥wt−1∥2 + b2\\nt−1 + ∥xt∥2 + 1\\nSince ∥xt∥2 = R2 we can again apply induction to conclude that∥wt∥2+b2\\nt ≤\\nt\\n[\\nR2 + 1\\n]\\n. Combining the upper and the lower bounds, using the Cauchy-\\nSchwartz inequality, and ∥w∗∥ = 1 yields\\ntγ ≤ ⟨wt, w∗⟩ + btb∗ =\\n⟨[ wt\\nbt\\n]\\n,\\n[ w∗\\nb∗\\n]⟩\\n≤\\n\\ued79\\ued79\\ued79\\ued79\\n[ wt\\nbt\\n]\\ued79\\ued79\\ued79\\ued79\\n\\ued79\\ued79\\ued79\\ued79\\n[ w∗\\nb∗\\n]\\ued79\\ued79\\ued79\\ued79 =\\n√\\n∥wt∥2 + b2\\nt\\n√\\n1 + (b∗)2\\n≤\\n√\\nt(R2 + 1)\\n√\\n1 + (b∗)2.\\nSquaring both sides of the inequality and rearranging the terms yields an\\nupper bound on the number of updates and hence the number of mistakes.\\nThe Perceptron was the building block of research on Neural Networks\\n[Hay98, Bis95]. The key insight was to combine large numbers of such net-\\nworks, often in a cascading fashion, to larger objects and to fashion opti-\\nmization algorithms which would lead to classiﬁers with desirable properties.\\nIn this book we will take a complementary route. Instead of increasing the\\nnumber of nodes we will investigate what happens when increasing the com-\\nplexity of the feature map φ and its associated kernel k. The advantage of\\ndoing so is that we will reap the beneﬁts from convex analysis and linear\\nmodels, possibly at the expense of a slightly more costly function evaluation.\\n1.3.5 K-Means\\nAll the algorithms we discussed so far are supervised, that is, they assume\\nthat labeled training data is available. In many applications this is too much\\nto hope for; labeling may be expensive, error prone, or sometimes impossi-\\nble. For instance, it is very easy to crawl and collect every page within the\\nwww.purdue.edu domain, but rather time consuming to assign a topic to\\neach page based on its contents. In such cases, one has to resort to unsuper-\\nvised learning. A prototypical unsupervised learning algorithm is K-means,\\nwhich is clustering algorithm. Given X = {x1, . . . , xm} the goal of K-means\\nis to partition it into k clusters such that each point in a cluster is similar\\nto points from its own cluster than with points from some other cluster.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='48e4c45e-3ff7-4995-913c-ebc99230e5be', embedding=None, metadata={'page_label': '33', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 33\\nTowards this end, deﬁne prototype vectors µ1, . . . , µk and an indicator\\nvector rij which is 1 if, and only if, xi is assigned to cluster j. To cluster our\\ndataset we will minimize the following distortion measure, which minimizes\\nthe distance of each point from the prototype vector:\\nJ(r, µ) := 1\\n2\\nm∑\\ni=1\\nk∑\\nj=1\\nrij∥xi − µj∥2, (1.29)\\nwhere r = {rij}, µ = {µj}, and ∥ · ∥ 2 denotes the usual Euclidean square\\nnorm.\\nOur goal is to ﬁnd r and µ, but since it is not easy to jointly minimize J\\nwith respect to both r and µ, we will adapt a two stage strategy:\\nStage 1 Keep the µ ﬁxed and determine r. In this case, it is easy to see\\nthat the minimization decomposes into m independent problems.\\nThe solution for the i-th data point xi can be found by setting:\\nrij = 1 if j = argmin\\nj′\\n∥xi − µj′∥2, (1.30)\\nand 0 otherwise.\\nStage 2 Keep the r ﬁxed and determine µ. Since the r’s are ﬁxed, J is an\\nquadratic function of µ. It can be minimized by setting the derivative\\nwith respect to µj to be 0:\\nm∑\\ni=1\\nrij(xi − µj) = 0 for all j. (1.31)\\nRearranging obtains\\nµj =\\n∑\\ni rijxi∑\\ni rij\\n. (1.32)\\nSince ∑\\ni rij counts the number of points assigned to clusterj, we are\\nessentially setting µj to be the sample mean of the points assigned\\nto cluster j.\\nThe algorithm stops when the cluster assignments do not change signiﬁ-\\ncantly. Detailed pseudo-code can be found in Algorithm 1.5.\\nTwo issues with K-Means are worth noting. First, it is sensitive to the\\nchoice of the initial cluster centers µ. A number of practical heuristics have\\nbeen developed. For instance, one could randomly choose k points from the\\ngiven dataset as cluster centers. Other methods try to pick k points from X\\nwhich are farthest away from each other. Second, it makes ahard assignment\\nof every point to a cluster center. Variants which we will encounter later in', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b9e547c9-186f-43f9-b70c-35311d1ff978', embedding=None, metadata={'page_label': '34', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='34 1 Introduction\\nAlgorithm 1.5 K-Means\\nCluster(X) {Cluster dataset X}\\nInitialize cluster centers µj for j = 1, . . . , k randomly\\nrepeat\\nfor i = 1 to m do\\nCompute j′ = argminj=1,...,k d(xi, µj)\\nSet rij′ = 1 and rij = 0 for all j′ ̸= j\\nend for\\nfor j = 1 to k do\\nCompute µj =\\n∑\\ni rij xi∑\\ni rij\\nend for\\nuntil Cluster assignments rij are unchanged\\nreturn {µ1, . . . , µk} and rij\\nthe book will relax this. Instead of letting rij ∈ { 0, 1} these soft variants\\nwill replace it with the probability that a given xi belongs to cluster j.\\nThe K-Means algorithm concludes our discussion of a set of basic machine\\nlearning methods for classiﬁcation and regression. They provide a useful\\nstarting point for an aspiring machine learning researcher. In this book we\\nwill see many more such algorithms as well as connections between these\\nbasic algorithms and their more advanced counterparts.\\nProblems\\nProblem 1.1 (Eyewitness) Assume that an eyewitness is 90% certain\\nthat a given person committed a crime in a bar. Moreover, assume that\\nthere were 50 people in the restaurant at the time of the crime. What is the\\nposterior probability of the person actually having committed the crime.\\nProblem 1.2 (DNA Test) Assume the police have a DNA library of 10\\nmillion records. Moreover, assume that the false recognition probability is\\nbelow 0.00001% per record. Suppose a match is found after a database search\\nfor an individual. What are the chances that the identiﬁcation is correct? You\\ncan assume that the total population is 100 million people. Hint: compute\\nthe probability of no match occurring ﬁrst.\\nProblem 1.3 (Bomb Threat) Suppose that the probability that one of a\\nthousand passengers on a plane has a bomb is 1 : 1, 000, 000. Assuming that\\nthe probability to have a bomb is evenly distributed among the passengers,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='698aa321-d189-4e5e-87ce-5b27d6ff0a49', embedding=None, metadata={'page_label': '35', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1.3 Basic Algorithms 35\\nthe probability that two passengers have a bomb is roughly equal to 10−12.\\nTherefore, one might decide to take a bomb on a plane to decrease chances\\nthat somebody else has a bomb. What is wrong with this argument?\\nProblem 1.4 (Monty-Hall Problem) Assume that in a TV show the\\ncandidate is given the choice between three doors. Behind two of the doors\\nthere is a pencil and behind one there is the grand prize, a car. The candi-\\ndate chooses one door. After that, the showmaster opens another door behind\\nwhich there is a pencil. Should the candidate switch doors after that? What\\nis the probability of winning the car?\\nProblem 1.5 (Mean and Variance for Random Variables) Denote by\\nXi random variables. Prove that in this case\\nEX1,...XN\\n[ ∑\\ni\\nxi\\n]\\n=\\n∑\\ni\\nEXi[xi] and VarX1,...XN\\n[ ∑\\ni\\nxi\\n]\\n=\\n∑\\ni\\nVarXi[xi]\\nTo show the second equality assume independence of the Xi.\\nProblem 1.6 (Two Dices) Assume you have a game which uses the max-\\nimum of two dices. Compute the probability of seeing any of the events\\n{1, . . . ,6}. Hint: prove ﬁrst that the cumulative distribution function of the\\nmaximum of a pair of random variables is the square of the original cumu-\\nlative distribution function.\\nProblem 1.7 (Matching Coins) Consider the following game: two play-\\ners bring a coin each. the ﬁrst player bets that when tossing the coins both\\nwill match and the second one bets that they will not match. Show that even\\nif one of the players were to bring a tainted coin, the game still would be\\nfair. Show that it is in the interest of each player to bring a fair coin to the\\ngame. Hint: assume that the second player knows that the ﬁrst coin favors\\nheads over tails.\\nProblem 1.8 (Randomized Maximization) How many observations do\\nyou need to draw from a distribution to ensure that the maximum over them\\nis larger than 95% of all observations with at least 95% probability? Hint:\\ngeneralize the result from Problem 1.6 to the maximum over n random vari-\\nables.\\nApplication: Assume we have 1000 computers performing MapReduce [DG08]\\nand the Reducers have to wait until all 1000 Mappers are ﬁnished with their\\njob. Compute the quantile of the typical time to completion.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0565af42-1cc3-4cec-a0ce-f9d991a479fe', embedding=None, metadata={'page_label': '36', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='36 1 Introduction\\nProblem 1.9 Prove that the Normal distribution (1.3) has mean µ and\\nvariance σ2. Hint: exploit the fact that p is symmetric around µ.\\nProblem 1.10 (Cauchy Distribution) Prove that for the density\\np(x) = 1\\nπ(1 + x2) (1.33)\\nmean and variance are undeﬁned. Hint: show that the integral diverges.\\nProblem 1.11 (Quantiles) Find a distribution for which the mean ex-\\nceeds the median. Hint: the mean depends on the value of the high-quantile\\nterms, whereas the median does not.\\nProblem 1.12 (Multicategory Naive Bayes) Prove that for multicate-\\ngory Naive Bayes the optimal decision is given by\\ny∗(x) := argmax\\ny\\np(y)\\nn∏\\ni=1\\np([x]i|y) (1.34)\\nwhere y ∈ Y is the class label of the observation x.\\nProblem 1.13 (Bayes Optimal Decisions) Denote by y∗(x) = argmaxy p(y|x)\\nthe label associated with the largest conditional class probability. Prove that\\nfor y∗(x) the probability of choosing the wrong label y is given by\\nl(x) := 1 − p(y∗(x)|x).\\nMoreover, show that y∗(x) is the label incurring the smallest misclassiﬁcation\\nerror.\\nProblem 1.14 (Nearest Neighbor Loss) Show that the expected loss in-\\ncurred by the nearest neighbor classiﬁer does not exceed twice the loss of the\\nBayes optimal decision.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='158aca49-9bd7-45bf-a521-ff9856421768', embedding=None, metadata={'page_label': '37', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2\\nDensity Estimation\\n2.1 Limit Theorems\\nAssume you are a gambler and go to a casino to play a game of dice. As\\nit happens, it is your unlucky day and among the 100 times you toss the\\ndice, you only see ’6’ eleven times. For a fair dice we know that each face\\nshould occur with equal probability 1\\n6. Hence the expected value over 100\\ndraws is 100\\n6 ≈ 17, which is considerably more than the eleven times that we\\nobserved. Before crying foul you decide that some mathematical analysis is\\nin order.\\nThe probability of seeing a particular sequence of m trials out of which n\\nare a ’6’ is given by 1\\n6\\nn 5\\n6\\nm−n. Moreover, there are\\n(m\\nn\\n)\\n= m!\\nn!(m−n)! diﬀerent\\nsequences of ’6’ and ’not 6’ with proportions n and m−n respectively. Hence\\nwe may compute the probability of seeing a ’6’ only 11 or less via\\nPr(X ≤ 11) =\\n11∑\\ni=0\\np(i) =\\n11∑\\ni=0\\n(100\\ni\\n) [1\\n6\\n]i [ 5\\n6\\n]100−i\\n≈ 7.0% (2.1)\\nAfter looking at this ﬁgure you decide that things are probably reasonable.\\nAnd, in fact, they are consistent with the convergence behavior of a sim-\\nulated dice in Figure 2.1. In computing (2.1) we have learned something\\nuseful: the expansion is a special case of a binomial series. The ﬁrst term\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=10\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=20\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=50\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=100\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=200\\n123456\\n0.0\\n0.1\\n0.2\\n0.3\\nm=500\\nFig. 2.1. Convergence of empirical means to expectations. From left to right: em-\\npirical frequencies of occurrence obtained by casting a dice 10, 20, 50, 100, 200, and\\n500 times respectively. Note that after 20 throws we still have not observed a single\\n’6’, an event which occurs with only\\n[5\\n6\\n]20\\n≈ 2.6% probability.\\n37', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f89771dc-42d3-416b-a200-d25d204bd56e', embedding=None, metadata={'page_label': '38', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='38 2 Density Estimation\\ncounts the number of conﬁgurations in which we could observei times ’6’ in a\\nsequence of 100 dice throws. The second and third term are the probabilities\\nof seeing one particular instance of such a sequence.\\nNote that in general we may not be as lucky, since we may have con-\\nsiderably less information about the setting we are studying. For instance,\\nwe might not know the actual probabilities for each face of the dice, which\\nwould be a likely assumption when gambling at a casino of questionable\\nreputation. Often the outcomes of the system we are dealing with may be\\ncontinuous valued random variables rather than binary ones, possibly even\\nwith unknown range. For instance, when trying to determine the average\\nwage through a questionnaire we need to determine how many people we\\nneed to ask in order to obtain a certain level of conﬁdence.\\nTo answer such questions we need to discuss limit theorems. They tell\\nus by how much averages over a set of observations may deviate from the\\ncorresponding expectations and how many observations we need to draw to\\nestimate a number of probabilities reliably. For completeness we will present\\nproofs for some of the more fundamental theorems in Section 2.1.2. They\\nare useful albeit non-essential for the understanding of the remainder of the\\nbook and may be omitted.\\n2.1.1 Fundamental Laws\\nThe Law of Large Numbers developed by Bernoulli in 1713 is one of the\\nfundamental building blocks of statistical analysis. It states that averages\\nover a number of observations converge to their expectations given a suﬃ-\\nciently large number of observations and given certain assumptions on the\\nindependence of these observations. It comes in two ﬂavors: the weak and\\nthe strong law.\\nTheorem 2.1 (Weak Law of Large Numbers) Denote by X1, . . . , Xm\\nrandom variables drawn from p(x) with mean µ = EXi[xi] for all i. Moreover\\nlet\\n¯Xm := 1\\nm\\nm∑\\ni=1\\nXi (2.2)\\nbe the empirical average over the random variables Xi. Then for any ϵ > 0\\nthe following holds\\nlim\\nm→∞\\nPr\\n(⏐⏐ ¯Xm − µ\\n⏐⏐ ≤ ϵ\\n)\\n= 1. (2.3)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='066de20a-4126-4c8c-8ff4-da7eb36c906a', embedding=None, metadata={'page_label': '39', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 39\\n101 102 103\\n1\\n2\\n3\\n4\\n5\\n6\\nFig. 2.2. The mean of a number of casts of a dice. The horizontal straight line\\ndenotes the mean 3.5. The uneven solid line denotes the actual mean ¯Xn as a\\nfunction of the number of draws, given as a semilogarithmic plot. The crosses denote\\nthe outcomes of the dice. Note how ¯Xn ever more closely approaches the mean 3 .5\\nare we obtain an increasing number of observations.\\nThis establishes that, indeed, for large enough sample sizes, the average will\\nconverge to the expectation. The strong law strengthens this as follows:\\nTheorem 2.2 (Strong Law of Large Numbers) Under the conditions\\nof Theorem 2.1 we have Pr\\n(\\nlimm→∞ ¯Xm = µ\\n)\\n= 1.\\nThe strong law implies that almost surely (in a measure theoretic sense) ¯Xm\\nconverges to µ, whereas the weak law only states that for everyϵ the random\\nvariable ¯Xm will be within the interval [µ−ϵ, µ+ϵ]. Clearly the strong implies\\nthe weak law since the measure of the events ¯Xm = µ converges to 1, hence\\nany ϵ-ball around µ would capture this.\\nBoth laws justify that we may take sample averages, e.g. over a number\\nof events such as the outcomes of a dice and use the latter to estimate their\\nmeans, their probabilities (here we treat the indicator variable of the event\\nas a {0; 1}-valued random variable), their variances or related quantities. We\\npostpone a proof until Section 2.1.2, since an eﬀective way of proving Theo-\\nrem 2.1 relies on the theory of characteristic functions which we will discuss\\nin the next section. For the moment, we only give a pictorial illustration in\\nFigure 2.2.\\nOnce we established that the random variable ¯Xm = m−1 ∑m\\ni=1 Xi con-\\nverges to its mean µ, a natural second question is to establish how quickly it\\nconverges and what the properties of the limiting distribution of ¯Xm −µ are.\\nNote in Figure 2.2 that the initial deviation from the mean is large whereas\\nas we observe more data the empirical mean approaches the true one.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bc29ab7f-d894-4c53-8d78-239e0d7cc6c0', embedding=None, metadata={'page_label': '40', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='40 2 Density Estimation\\n101 102 103\\n1\\n2\\n3\\n4\\n5\\n6\\nFig. 2.3. Five instantiations of a running average over outcomes of a toss of a dice.\\nNote that all of them converge to the mean 3 .5. Moreover note that they all are\\nwell contained within the upper and lower envelopes given by µ ±\\n√\\nVarX[x]/m.\\nThe central limit theorem answers this question exactly by addressing a\\nslightly more general question, namely whether the sum over a number of\\nindependent random variables where each of them arises from a diﬀerent\\ndistribution might also have a well behaved limiting distribution. This is\\nthe case as long as the variance of each of the random variables is bounded.\\nThe limiting distribution of such a sum is Gaussian. This aﬃrms the pivotal\\nrole of the Gaussian distribution.\\nTheorem 2.3 (Central Limit Theorem) Denote by Xi independent ran-\\ndom variables with means µi and standard deviation σi. Then\\nZm :=\\n[ m∑\\ni=1\\nσ2\\ni\\n]− 1\\n2\\n[ m∑\\ni=1\\nXi − µi\\n]\\n(2.4)\\nconverges to a Normal Distribution with zero mean and unit variance.\\nNote that just like the law of large numbers the central limit theorem (CLT)\\nis an asymptotic result. That is, only in the limit of an inﬁnite number of\\nobservations will it become exact. That said, it often provides an excellent\\napproximation even for ﬁnite numbers of observations, as illustrated in Fig-\\nure 2.4. In fact, the central limit theorem and related limit theorems build\\nthe foundation of what is known as asymptotic statistics.\\nExample 2.1 (Dice) If we are interested in computing the mean of the\\nvalues returned by a dice we may apply the CLT to the sum over m variables', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c526dba2-26bc-4a02-b291-5207835efae5', embedding=None, metadata={'page_label': '41', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 41\\nwhich have all mean µ = 3.5 and variance (see Problem 2.1)\\nVarX[x] = EX[x2] − EX[x]2 = (1 + 4 + 9 + 16 + 25 + 36)/6 − 3.52 ≈ 2.92.\\nWe now study the random variable Wm := m−1 ∑m\\ni=1[Xi − 3.5]. Since each\\nof the terms in the sum has zero mean, also Wm’s mean vanishes. Moreover,\\nWm is a multiple of Zm of (2.4). Hence we have that Wm converges to a\\nnormal distribution with zero mean and standard deviation 2.92m− 1\\n2 .\\nConsequently the average of m tosses of the dice yields a random vari-\\nable with mean 3.5 and it will approach a normal distribution with variance\\nm− 1\\n2 2.92. In other words, the empirical mean converges to its average at\\nrate O(m− 1\\n2 ). Figure 2.3 gives an illustration of the quality of the bounds\\nimplied by the CLT.\\nOne remarkable property of functions of random variables is that in many\\nconditions convergence properties of the random variables are bestowed upon\\nthe functions, too. This is manifest in the following two results: a variant\\nof Slutsky’s theorem and the so-called delta method. The former deals with\\nlimit behavior whereas the latter deals with an extension of the central limit\\ntheorem.\\nTheorem 2.4 (Slutsky’s Theorem) Denote by Xi, Yi sequences of ran-\\ndom variables with Xi → X and Yi → c for c ∈ R in probability. Moreover,\\ndenote by g(x, y) a function which is continuous for all (x, c). In this case\\nthe random variable g(Xi, Yi) converges in probability to g(X, c).\\nFor a proof see e.g. [Bil68]. Theorem 2.4 is often referred to as the continuous\\nmapping theorem (Slutsky only proved the result for aﬃne functions). It\\nmeans that for functions of random variables it is possible to pull the limiting\\nprocedure into the function. Such a device is useful when trying to prove\\nasymptotic normality and in order to obtain characterizations of the limiting\\ndistribution.\\nTheorem 2.5 (Delta Method) Assume that Xn ∈ Rd is asymptotically\\nnormal with a−2\\nn (Xn − b) → N(0, Σ) for a2\\nn → 0. Moreover, assume that\\ng : Rd → Rl is a mapping which is continuously diﬀerentiable at b. In this\\ncase the random variable g(Xn) converges\\na−2\\nn (g(Xn) − g(b)) → N(0, [∇xg(b)]Σ[∇xg(b)]⊤). (2.5)\\nProof Via a Taylor expansion we see that\\na−2\\nn [g(Xn) − g(b)] = [∇xg(ξn)]⊤a−2\\nn (Xn − b) (2.6)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='db94b7c1-4e3b-486d-837e-994031908db6', embedding=None, metadata={'page_label': '42', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='42 2 Density Estimation\\nHere ξn lies on the line segment [ b, Xn]. Since Xn → b we have that ξn → b,\\ntoo. Since g is continuously diﬀerentiable at b we may apply Slutsky’s the-\\norem to see that a−2\\nn [g(Xn) − g(b)] → [∇xg(b)]⊤a−2\\nn (Xn − b). As a con-\\nsequence, the transformed random variable is asymptotically normal with\\ncovariance [∇xg(b)]Σ[∇xg(b)]⊤.\\nWe will use the delta method when it comes to investigating properties of\\nmaximum likelihood estimators in exponential families. There g will play the\\nrole of a mapping between expectations and the natural parametrization of\\na distribution.\\n2.1.2 The Characteristic Function\\nThe Fourier transform plays a crucial role in many areas of mathematical\\nanalysis and engineering. This is equally true in statistics. For historic rea-\\nsons its applications to distributions is called the characteristic function,\\nwhich we will discuss in this section. At its foundations lie standard tools\\nfrom functional analysis and signal processing [Rud73, Pap62]. We begin by\\nrecalling the basic properties:\\nDeﬁnition 2.6 (Fourier Transform) Denote by f : Rn → C a function\\ndeﬁned on a d-dimensional Euclidean space. Moreover, let x, ω ∈ Rn. Then\\nthe Fourier transform F and its inverse F−1 are given by\\nF [f](ω) := (2π)−d\\n2\\n∫\\nRn\\nf(x) exp(−i ⟨ω, x⟩)dx (2.7)\\nF−1[g](x) := (2π)−d\\n2\\n∫\\nRn\\ng(ω) exp(i ⟨ω, x⟩)dω. (2.8)\\nThe key insight is that F−1 ◦ F = F ◦ F−1 = Id. In other words, F and\\nF−1 are inverses to each other for all functions which are L2 integrable on\\nRd, which includes probability distributions. One of the key advantages of\\nFourier transforms is that derivatives and convolutions on f translate into\\nmultiplications. That is F [f ◦ g] = (2π)\\nd\\n2 F [f] · F [g]. The same rule applies\\nto the inverse transform, i.e. F−1[f ◦ g] = (2π)\\nd\\n2 F−1[f]F−1[g].\\nThe beneﬁt for statistical analysis is that often problems are more easily\\nexpressed in the Fourier domain and it is easier to prove convergence results\\nthere. These results then carry over to the original domain. We will be\\nexploiting this fact in the proof of the law of large numbers and the central\\nlimit theorem. Note that the deﬁnition of Fourier transforms can be extended\\nto more general domains such as groups. See e.g. [BCR84] for further details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='21a5e70f-f505-4f3f-9bb9-c76016f6e53d', embedding=None, metadata={'page_label': '43', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 43\\nWe next introduce the notion of a characteristic function of a distribution.1\\nDeﬁnition 2.7 (Characteristic Function) Denote by p(x) a distribution\\nof a random variable X ∈ Rd. Then the characteristic function φX(ω) with\\nω ∈ Rd is given by\\nφX(ω) := (2π)\\nd\\n2 F−1[p(x)] =\\n∫\\nexp(i ⟨ω, x⟩)dp(x). (2.9)\\nIn other words, φX(ω) is the inverse Fourier transform applied to the prob-\\nability measure p(x). Consequently φX(ω) uniquely characterizes p(x) and\\nmoreover, p(x) can be recovered from φX(ω) via the forward Fourier trans-\\nform. One of the key utilities of characteristic functions is that they allow\\nus to deal in easy ways with sums of random variables.\\nTheorem 2.8 (Sums of random variables and convolutions) Denote\\nby X, Y ∈ R two independent random variables. Moreover, denote by Z :=\\nX + Y the sum of both random variables. Then the distribution over Z sat-\\nisﬁes p(z) = p(x) ◦ p(y). Moreover, the characteristic function yields:\\nφZ(ω) = φX(ω)φY (ω). (2.10)\\nProof Z is given by Z = X + Y . Hence, for a given Z = z we have\\nthe freedom to choose X = x freely provided that Y = z − x. In terms of\\ndistributions this means that the joint distribution p(z, x) is given by\\np(z, x) = p(Y = z − x)p(x)\\nand hence p(z) =\\n∫\\np(Y = z − x)dp(x) = [p(x) ◦ p(y)](z).\\nThe result for characteristic functions follows form the property of the\\nFourier transform.\\nFor sums of several random variables the characteristic function is the prod-\\nuct of the individual characteristic functions. This allows us to prove both\\nthe weak law of large numbers and the central limit theorem (see Figure 2.4\\nfor an illustration) by proving convergence in the Fourier domain.\\nProof [Weak Law of Large Numbers] At the heart of our analysis lies\\na Taylor expansion of the exponential into\\nexp(iwx) = 1 + i ⟨w, x⟩ + o(|w|)\\nand hence φX(ω) = 1 + iwEX[x] + o(|w|).\\n1 In Chapter ?? we will discuss more general descriptions of distributions of whichφX is a special\\ncase. In particular, we will replace the exponential exp( i⟨ω, x⟩) by a kernel function k(x, x′).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0db41091-1c98-476b-a1fe-6663feee3dbd', embedding=None, metadata={'page_label': '44', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='44 2 Density Estimation\\n-50 5\\n0.0\\n0.5\\n1.0\\n-50 5\\n0.0\\n0.5\\n1.0\\n-50 5\\n0.0\\n0.5\\n1.0\\n-50 5\\n0.0\\n0.5\\n1.0\\n-50 5\\n0.0\\n0.5\\n1.0\\n-101\\n0.0\\n0.5\\n1.0\\n1.5\\n-101\\n0.0\\n0.5\\n1.0\\n1.5\\n-101\\n0.0\\n0.5\\n1.0\\n1.5\\n-101\\n0.0\\n0.5\\n1.0\\n1.5\\n-101\\n0.0\\n0.5\\n1.0\\n1.5\\nFig. 2.4. A working example of the central limit theorem. The top row contains\\ndistributions of sums of uniformly distributed random variables on the interval\\n[0.5, 0.5]. From left to right we have sums of 1, 2, 4, 8 and 16 random variables. The\\nbottom row contains the same distribution with the means rescaled by √m, where\\nm is the number of observations. Note how the distribution converges increasingly\\nto the normal distribution.\\nGiven m random variables Xi with mean EX[x] = µ this means that their\\naverage ¯Xm := 1\\nm\\n∑m\\ni=1 Xi has the characteristic function\\nφ ¯Xm(ω) =\\n(\\n1 + i\\nm wµ + o(m−1 |w|)\\n)m\\n(2.11)\\nIn the limit of m → ∞ this converges to exp( iwµ), the characteristic func-\\ntion of the constant distribution with mean µ. This proves the claim that in\\nthe large sample limit ¯Xm is essentially constant with mean µ.\\nProof [Central Limit Theorem] We use the same idea as above to prove\\nthe CLT. The main diﬀerence, though, is that we need to assume that the\\nsecond moments of the random variables Xi exist. To avoid clutter we only\\nprove the case of constant mean EXi[xi] = µ and variance VarXi[xi] = σ2.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='306d0c99-9018-4ff0-931f-d99e9bea9113', embedding=None, metadata={'page_label': '45', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 45\\nLet Zm := 1√\\nmσ2\\n∑m\\ni=1(Xi − µ). Our proof relies on showing convergence\\nof the characteristic function of Zm, i.e. φZm to that of a normally dis-\\ntributed random variable W with zero mean and unit variance. Expanding\\nthe exponential to second order yields:\\nexp(iwx) = 1 + iwx − 1\\n2 w2x2 + o(|w|2)\\nand hence φX(ω) = 1 + iwEX[x] − 1\\n2 w2VarX[x] + o(|w|2)\\nSince the mean of Zm vanishes by centering ( Xi − µ) and the variance per\\nvariable is m−1 we may write the characteristic function of Zm via\\nφZm(ω) =\\n(\\n1 − 1\\n2m w2 + o(m−1 |w|2)\\n)m\\nAs before, taking limits m → ∞ yields the exponential function. We have\\nthat lim m→∞ φZm(ω) = exp( − 1\\n2 ω2) which is the characteristic function of\\nthe normal distribution with zero mean and variance 1. Since the character-\\nistic function transform is injective this proves our claim.\\nNote that the characteristic function has a number of useful properties. For\\ninstance, it can also be used as moment generating function via the identity:\\n∇n\\nωφX(0) = i−nEX[xn]. (2.12)\\nIts proof is left as an exercise. See Problem 2.2 for details. This connection\\nalso implies (subject to regularity conditions) that if we know the moments\\nof a distribution we are able to reconstruct it directly since it allows us\\nto reconstruct its characteristic function. This idea has been exploited in\\ndensity estimation [Cra46] in the form of Edgeworth and Gram-Charlier\\nexpansions [Hal92].\\n2.1.3 Tail Bounds\\nIn practice we never have access to aninﬁnite number of observations. Hence\\nthe central limit theorem does not apply but is just an approximation to the\\nreal situation. For instance, in the case of the dice, we might want to state\\nworst case bounds for ﬁnite sums of random variables to determine by how\\nmuch the empirical mean may deviate from its expectation. Those bounds\\nwill not only be useful for simple averages but to quantify the behavior of\\nmore sophisticated estimators based on a set of observations.\\nThe bounds we discuss below diﬀer in the amount of knowledge they\\nassume about the random variables in question. For instance, we might only', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c7a789eb-8af5-41db-8f7a-a0ab2e67219b', embedding=None, metadata={'page_label': '46', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='46 2 Density Estimation\\nknow their mean. This leads to the Gauss-Markov inequality. If we know\\ntheir mean and their variance we are able to state a stronger bound, the\\nChebyshev inequality. For an even stronger setting, when we know that\\neach variable has bounded range, we will be able to state a Chernoﬀ bound.\\nThose bounds are progressively more tight and also more diﬃcult to prove.\\nWe state them in order of technical sophistication.\\nTheorem 2.9 (Gauss-Markov) Denote by X ≥ 0 a random variable and\\nlet µ be its mean. Then for any ϵ > 0 we have\\nPr(X ≥ ϵ) ≤ µ\\nϵ . (2.13)\\nProof We use the fact that for nonnegative random variables\\nPr(X ≥ ϵ) =\\n∫ ∞\\nϵ\\ndp(x) ≤\\n∫ ∞\\nϵ\\nx\\nϵ dp(x) ≤ ϵ−1\\n∫ ∞\\n0\\nxdp(x) = µ\\nϵ .\\nThis means that for random variables with a small mean, the proportion of\\nsamples with large value has to be small.\\nConsequently deviations from the mean are O(ϵ−1). However, note that this\\nbound does not depend on the number of observations. A useful application\\nof the Gauss-Markov inequality is Chebyshev’s inequality. It is a statement\\non the range of random variables using its variance.\\nTheorem 2.10 (Chebyshev) Denote by X a random variable with mean\\nµ and variance σ2. Then the following holds for ϵ > 0:\\nPr(|x − µ| ≥ ϵ) ≤ σ2\\nϵ2 . (2.14)\\nProof Denote by Y := |X − µ|2 the random variable quantifying the\\ndeviation of X from its mean µ. By construction we know that EY [y] = σ2.\\nNext let γ := ϵ2. Applying Theorem 2.9 to Y and γ yields Pr(Y > γ ) ≤ σ2/γ\\nwhich proves the claim.\\nNote the improvement to the Gauss-Markov inequality. Where before we had\\nbounds whose conﬁdence improved with O(ϵ−1) we can now state O(ϵ−2)\\nbounds for deviations from the mean.\\nExample 2.2 (Chebyshev bound) Assume that ¯Xm := m−1 ∑m\\ni=1 Xi is\\nthe average over m random variables with mean µ and variance σ2. Hence\\n¯Xm also has mean µ. Its variance is given by\\nVar ¯Xm[¯xm] =\\nm∑\\ni=1\\nm−2VarXi[xi] = m−1σ2.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3328e032-253c-42b1-9e81-421e7e0c3552', embedding=None, metadata={'page_label': '47', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 47\\nApplying Chebyshev’s inequality yields that the probability of a deviation\\nof ϵ from the mean µ is bounded by σ2\\nmϵ2 . For ﬁxed failure probability δ =\\nPr(| ¯Xm − µ| > ϵ) we have\\nδ ≤ σ2m−1ϵ−2 and equivalently ϵ ≤ σ/\\n√\\nmδ.\\nThis bound is quite reasonable for large δ but it means that for high levels\\nof conﬁdence we need a huge number of observations.\\nMuch stronger results can be obtained if we are able to bound the range\\nof the random variables. Using the latter, we reap an exponential improve-\\nment in the quality of the bounds in the form of the McDiarmid [McD89]\\ninequality. We state the latter without proof:\\nTheorem 2.11 (McDiarmid) Denote by f : Xm → R a function on X\\nand let Xi be independent random variables. In this case the following holds:\\nPr (|f(x1, . . . , xm) − EX1,...,Xm[f(x1, . . . , xm)]| > ϵ) ≤ 2 exp\\n(\\n−2ϵ2C−2)\\n.\\nHere the constant C2 is given by C2 = ∑m\\ni=1 c2\\ni where\\n⏐⏐f(x1, . . . , xi, . . . , xm) − f(x1, . . . , x′\\ni, . . . , xm)\\n⏐⏐ ≤ ci\\nfor all x1, . . . , xm, x′\\ni and for all i.\\nThis bound can be used for averages of a number of observations when\\nthey are computed according to some algorithm as long as the latter can be\\nencoded in f. In particular, we have the following bound [Hoe63]:\\nTheorem 2.12 (Hoeﬀding) Denote by Xi iid random variables with bounded\\nrange Xi ∈ [a, b] and mean µ. Let ¯Xm := m−1 ∑m\\ni=1 Xi be their average.\\nThen the following bound holds:\\nPr\\n(⏐⏐ ¯Xm − µ\\n⏐⏐ > ϵ\\n)\\n≤ 2 exp\\n(\\n− 2mϵ2\\n(b − a)2\\n)\\n. (2.15)\\nProof This is a corollary of Theorem 2.11. In ¯Xm each individual random\\nvariable has range [ a/m, b/m] and we set f(X1, . . . , Xm) := ¯Xm. Straight-\\nforward algebra shows that C2 = m−2(b − a)2. Plugging this back into\\nMcDiarmid’s theorem proves the claim.\\nNote that (2.15) is exponentially better than the previous bounds. With\\nincreasing sample size the conﬁdence level also increases exponentially.\\nExample 2.3 (Hoeﬀding bound) As in example 2.2 assume that Xi are\\niid random variables and let ¯Xm be their average. Moreover, assume that', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='46f46f74-6757-4360-9ed0-60eb97323715', embedding=None, metadata={'page_label': '48', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='48 2 Density Estimation\\nXi ∈ [a, b] for all i. As before we want to obtain guarantees on the probability\\nthat | ¯Xm − µ| > ϵ. For a given level of conﬁdence 1 − δ we need to solve\\nδ ≤ 2 exp\\n(\\n− 2mϵ2\\n(b−a)2\\n)\\n(2.16)\\nfor ϵ. Straightforward algebra shows that in this case ϵ needs to satisfy\\nϵ ≥ |b − a|\\n√\\n[log 2 − log δ] /2m (2.17)\\nIn other words, while the conﬁdence level only enters logarithmically into the\\ninequality, the sample size m improves our conﬁdence only with ϵ = O(m− 1\\n2 ).\\nThat is, in order to improve our conﬁdence interval from ϵ = 0.1 to ϵ = 0.01\\nwe need 100 times as many observations.\\nWhile this bound is tight (see Problem 2.5 for details), it is possible to ob-\\ntain better bounds if we know additional information. In particular knowing\\na bound on the variance of a random variable in addition to knowing that it\\nhas bounded range would allow us to strengthen the statement considerably.\\nThe Bernstein inequality captures this connection. For details see [BBL05]\\nor works on empirical process theory [vdVW96, SW86, Vap82].\\n2.1.4 An Example\\nIt is probably easiest to illustrate the various bounds using a concrete exam-\\nple. In a semiconductor fab processors are produced on a wafer. A typical\\n300mm wafer holds about 400 chips. A large number of processing steps\\nare required to produce a ﬁnished microprocessor and often it is impossible\\nto assess the eﬀect of a design decision until the ﬁnished product has been\\nproduced.\\nAssume that the production manager wants to change some step from\\nprocess ’A’ to some other process ’B’. The goal is to increase the yield of\\nthe process, that is, the number of chips of the 400 potential chips on the\\nwafer which can be sold. Unfortunately this number is a random variable,\\ni.e. the number of working chips per wafer can vary widely between diﬀerent\\nwafers. Since process ’A’ has been running in the factory for a very long\\ntime we may assume that the yield is well known, say it is µA = 350 out\\nof 400 processors on average. It is our goal to determine whether process\\n’B’ is better and what its yield may be. Obviously, since production runs\\nare expensive we want to be able to determine this number as quickly as\\npossible, i.e. using as few wafers as possible. The production manager is risk\\naverse and wants to ensure that the new process is really better. Hence he\\nrequires a conﬁdence level of 95% before he will change the production.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='50818bca-dd06-43d4-8ab5-0e06b00aeeef', embedding=None, metadata={'page_label': '49', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.1 Limit Theorems 49\\nA ﬁrst step is to formalize the problem. Since we know process ’A’ exactly\\nwe only need to concern ourselves with ’B’. We associate the random variable\\nXi with wafer i. A reasonable (and somewhat simplifying) assumption is to\\nposit that all Xi are independent and identically distributed where all Xi\\nhave the mean µB. Obviously we do not know µB — otherwise there would\\nbe no reason for testing! We denote by ¯Xm the average of the yields of m\\nwafers using process ’B’. What we are interested in is the accuracy ϵ for\\nwhich the probability\\nδ = Pr(| ¯Xm − µB| > ϵ) satisﬁes δ ≤ 0.05.\\nLet us now discuss how the various bounds behave. For the sake of the\\nargument assume that µB − µA = 20, i.e. the new process produces on\\naverage 20 additional usable chips.\\nChebyshev In order to apply the Chebyshev inequality we need to bound\\nthe variance of the random variables Xi. The worst possible variance would\\noccur if Xi ∈ { 0; 400} where both events occur with equal probability. In\\nother words, with equal probability the wafer if fully usable or it is entirely\\nbroken. This amounts to σ2 = 0 .5(200 − 0)2 + 0.5(200 − 400)2 = 40 , 000.\\nSince for Chebyshev bounds we have\\nδ ≤ σ2m−1ϵ−2 (2.18)\\nwe can solve for m = σ2/δϵ2 = 40, 000/(0.05 ·400) = 20, 000. In other words,\\nwe would typically need 20,000 wafers to assess with reasonable conﬁdence\\nwhether process ’B’ is better than process ’A’. This is completely unrealistic.\\nSlightly better bounds can be obtained if we are able to make better\\nassumptions on the variance. For instance, if we can be sure that the yield\\nof process ’B’ is at least 300, then the largest possible variance is 0 .25(300 −\\n0)2 + 0.75(300 − 400)2 = 30 , 000, leading to a minimum of 15,000 wafers\\nwhich is not much better.\\nHoeﬀding Since the yields are in the interval {0, . . . ,400} we have an ex-\\nplicit bound on the range of observations. Recall the inequality (2.16) which\\nbounds the failure probably δ = 0.05 by an exponential term. Solving this\\nfor m yields\\nm ≥ 0.5|b − a|2ϵ−2 log(2/δ) ≈ 737.8 (2.19)\\nIn other words, we need at lest 738 wafers to determine whether process ’B’\\nis better. While this is a signiﬁcant improvement of almost two orders of\\nmagnitude, it still seems wasteful and we would like to do better.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7e0c882e-f6dd-4d9e-9e0d-0bd28bfed8e0', embedding=None, metadata={'page_label': '50', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='50 2 Density Estimation\\nCentral Limit Theorem The central limit theorem is an approximation.\\nThis means that our reasoning is not accurate any more. That said, for\\nlarge enough sample sizes, the approximation is good enough to use it for\\npractical predictions. Assume for the moment that we knew the variance σ2\\nexactly. In this case we know that ¯Xm is approximately normal with mean\\nµB and variance m−1σ2. We are interested in the interval [µ − ϵ, µ+ ϵ] which\\ncontains 95% of the probability mass of a normal distribution. That is, we\\nneed to solve the integral\\n1\\n2πσ2\\n∫ µ+ϵ\\nµ−ϵ\\nexp\\n(\\n−(x − µ)2\\n2σ2\\n)\\ndx = 0.95 (2.20)\\nThis can be solved eﬃciently using the cumulative distribution function of\\na normal distribution (see Problem 2.3 for more details). One can check\\nthat (2.20) is solved for ϵ = 2 .96σ. In other words, an interval of ±2.96σ\\ncontains 95% of the probability mass of a normal distribution. The number\\nof observations is therefore determined by\\nϵ = 2.96σ/√m and hence m = 8.76 σ2\\nϵ2 (2.21)\\nAgain, our problem is that we do not know the variance of the distribution.\\nUsing the worst-case bound on the variance, i.e. σ2 = 40, 000 would lead to\\na requirement of at least m = 876 wafers for testing. However, while we do\\nnot know the variance, we may estimate it along with the mean and use the\\nempirical estimate, possibly plus some small constant to ensure we do not\\nunderestimate the variance, instead of the upper bound.\\nAssuming that ﬂuctuations turn out to be in the order of 50 processors,\\ni.e. σ2 = 2500, we are able to reduce our requirement to approximately 55\\nwafers. This is probably an acceptable number for a practical test.\\nRates and Constants The astute reader will have noticed that all three\\nconﬁdence bounds had scaling behavior m = O(ϵ−2). That is, in all cases\\nthe number of observations was a fairly ill behaved function of the amount\\nof conﬁdence required. If we were just interested in convergence per se, a\\nstatement like that of the Chebyshev inequality would have been entirely\\nsuﬃcient. The various laws and bounds can often be used to obtain con-\\nsiderably better constants for statistical conﬁdence guarantees. For more\\ncomplex estimators, such as methods to classify, rank, or annotate data,\\na reasoning such as the one above can become highly nontrivial. See e.g.\\n[MYA94, Vap98] for further details.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b6cd66fd-6fcc-49a2-bcad-e76d256c1fd4', embedding=None, metadata={'page_label': '51', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Parzen Windows 51\\n2.2 Parzen Windows\\n2.2.1 Discrete Density Estimation\\nThe convergence theorems discussed so far mean that we can use empir-\\nical observations for the purpose of density estimation. Recall the case of\\nthe Naive Bayes classiﬁer of Section 1.3.1. One of the key ingredients was\\nthe ability to use information about word counts for diﬀerent document\\nclasses to estimate the probability p(wj|y), where wj denoted the number\\nof occurrences of word j in document x, given that it was labeled y. In the\\nfollowing we discuss an extremely simple and crude method for estimating\\nprobabilities. It relies on the fact that for random variables Xi drawn from\\ndistribution p(x) with discrete values Xi ∈ X we have\\nlim\\nm→∞\\nˆpX(x) = p(x) (2.22)\\nwhere ˆpX(x) := m−1\\nm∑\\ni=1\\n{xi = x} for all x ∈ X. (2.23)\\nLet us discuss a concrete case. We assume that we have 12 documents and\\nwould like to estimate the probability of occurrence of the word ’dog’ from\\nit. As raw data we have:\\nDocument ID 1 2 3 4 5 6 7 8 9 10 11 12\\nOccurrences of ‘dog’ 1 0 2 0 4 6 3 0 6 2 0 1\\nThis means that the word ‘dog’ occurs the following number of times:\\nOccurrences of ‘dog’ 0 1 2 3 4 5 6\\nNumber of documents 4 2 2 1 1 0 2\\nSomething unusual is happening here: for some reason we never observed\\n5 instances of the word dog in our documents, only 4 and less, or alter-\\nnatively 6 times. So what about 5 times? It is reasonable to assume that\\nthe corresponding value should not be 0 either. Maybe we did not sample\\nenough. One possible strategy is to add pseudo-counts to the observations.\\nThis amounts to the following estimate:\\nˆpX(x) := (m + |X|)−1\\n[\\n1 +\\nm∑\\ni=1\\n{xi = x} = p(x)\\n]\\n(2.24)\\nClearly the limit for m → ∞ is still p(x). Hence, asymptotically we do not\\nlose anything. This prescription is what we used in Algorithm 1.1 used a\\nmethod called Laplace smoothing. Below we contrast the two methods:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e2a0118b-6696-4bf6-a737-15a4cc3de191', embedding=None, metadata={'page_label': '52', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='52 2 Density Estimation\\nOccurrences of ‘dog’ 0 1 2 3 4 5 6\\nNumber of documents 4 2 2 1 1 0 2\\nFrequency of occurrence 0.33 0.17 0.17 0.083 0.083 0 0.17\\nLaplace smoothing 0.26 0.16 0.16 0.11 0.11 0.05 0.16\\nThe problem with this method is that as |X| increases we need increasingly\\nmore observations to obtain even a modicum of precision. On average, we\\nwill need at least one observation for every x ∈ X. This can be infeasible for\\nlarge domains as the following example shows.\\nExample 2.4 (Curse of Dimensionality) Assume that X = {0, 1}d, i.e.\\nx consists of binary bit vectors of dimensionality d. As d increases the size of\\nX increases exponentially, requiring an exponential number of observations\\nto perform density estimation. For instance, if we work with images, a 100 ×\\n100 black and white picture would require in the order of 103010 observations\\nto model such fairly low-resolution images accurately. This is clearly utterly\\ninfeasible — the number of particles in the known universe is in the order\\nof 1080. Bellman [Bel61] was one of the ﬁrst to formalize this dilemma by\\ncoining the term ’curse of dimensionality’.\\nThis example clearly shows that we need better tools to deal with high-\\ndimensional data. We will present one of such tools in the next section.\\n2.2.2 Smoothing Kernel\\nWe now proceed to proper density estimation. Assume that we want to\\nestimate the distribution of weights of a population. Sample data from a\\npopulation might look as follows: X = {57, 88, 54, 84, 83, 59, 56, 43, 70, 63,\\n90, 98, 102, 97, 106, 99, 103, 112 }. We could use this to perform a density\\nestimate by placing discrete components at the locations xi ∈ X with weight\\n1/|X| as what is done in Figure 2.5. There is no reason to believe that weights\\nare quantized in kilograms, or grams, or miligrams (or pounds and stones).\\nAnd even if it were, we would expect that similar weights would have similar\\ndensities associated with it. Indeed, as the right diagram of Figure 2.5 shows,\\nthe corresponding density is continuous.\\nThe key question arising is how we may transform X into a realistic\\nestimate of the density p(x). Starting with a ’density estimate’ with only\\ndiscrete terms\\nˆp(x) = 1\\nm\\nm∑\\ni=1\\nδ(x − xi) (2.25)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0195bdfd-35bd-44fd-8506-a4896e13ef46', embedding=None, metadata={'page_label': '53', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Parzen Windows 53\\nwe may choose to smooth it out by a smoothing kernel h(x) such that the\\nprobability mass becomes somewhat more spread out. For a density estimate\\non X ⊆ Rd this is achieved by\\nˆp(x) = 1\\nm\\nm∑\\ni=1\\nr−dh\\n(x−xi\\nr\\n)\\n. (2.26)\\nThis expansion is commonly known as the Parzen windows estimate. Note\\nthat obviously h must be chosen such that h(x) ≥ 0 for all x ∈ X and\\nmoreover that\\n∫\\nh(x)dx = 1 in order to ensure that (2.26) is a proper prob-\\nability distribution. We now formally justify this smoothing. Let R be a\\nsmall region such that\\nq =\\n∫\\nR\\np(x) dx.\\nOut of the m samples drawn from p(x), the probability that k of them fall\\nin region R is given by the binomial distribution\\n(m\\nk\\n)\\nqk(1 − q)m−k.\\nThe expected fraction of points falling inside the region can easily be com-\\nputed from the expected value of the Binomial distribution: E[k/m] = q.\\nSimilarly, the variance can be computed as Var[ k/m] = q(1 − q)/m. As\\nm → ∞ the variance goes to 0 and hence the estimate peaks around the\\nexpectation. We can therefore set\\nk ≈ mq.\\nIf we assume that R is so small that p(x) is constant over R, then\\nq ≈ p(x) · V,\\nwhere V is the volume of R. Rearranging we obtain\\np(x) ≈ k\\nmV . (2.27)\\nLet us now set R to be a cube with side length r, and deﬁne a function\\nh(u) =\\n{\\n1 if |ui| ≤ 1\\n2\\n0 otherwise.\\nObserve that h\\n(x−xi\\nr\\n)\\nis 1 if and only if xi lies inside a cube of size r centered', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0bea10df-5d5f-49aa-9919-5a822ce29a55', embedding=None, metadata={'page_label': '54', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='54 2 Density Estimation\\naround x. If we let\\nk =\\nm∑\\ni=1\\nh\\n(x − xi\\nr\\n)\\n,\\nthen one can use (2.27) to estimate p via\\nˆp(x) = 1\\nm\\nm∑\\ni=1\\nr−dh\\n(x − xi\\nr\\n)\\n,\\nwhere rd is the volume of the hypercube of sizer in d dimensions. By symme-\\ntry, we can interpret this equation as the sum over m cubes centered around\\nm data points xn. If we replace the cube by any smooth kernel function h(·)\\nthis recovers (2.26).\\nThere exists a large variety of diﬀerent kernels which can be used for the\\nkernel density estimate. [Sil86] has a detailed description of the properties\\nof a number of kernels. Popular choices are\\nh(x) = (2π)− 1\\n2 e− 1\\n2 x2\\nGaussian kernel (2.28)\\nh(x) = 1\\n2 e−|x| Laplace kernel (2.29)\\nh(x) = 3\\n4 max(0, 1 − x2) Epanechnikov kernel (2.30)\\nh(x) = 1\\n2 χ[−1,1](x) Uniform kernel (2.31)\\nh(x) = max(0, 1 − |x|) Triangle kernel . (2.32)\\nFurther kernels are the triweight and the quartic kernel which are basically\\npowers of the Epanechnikov kernel. For practical purposes the Gaussian ker-\\nnel (2.28) or the Epanechnikov kernel (2.30) are most suitable. In particular,\\nthe latter has the attractive property of compact support. This means that\\nfor any given density estimate at location x we will only need to evaluate\\nterms h(xi − x) for which the distance ∥xi − x∥ is less than r. Such expan-\\nsions are computationally much cheaper, in particular when we make use of\\nfast nearest neighbor search algorithms [GIM99, IM98]. Figure 2.7 has some\\nexamples of kernels.\\n2.2.3 Parameter Estimation\\nSo far we have not discussed the issue of parameter selection. It should be\\nevident from Figure 2.6, though, that it is quite crucial to choose a good\\nkernel width. Clearly, a kernel that is overly wide will oversmooth any ﬁne\\ndetail that there might be in the density. On the other hand, a very narrow\\nkernel will not be very useful, since it will be able to make statements only\\nabout the locations where we actually observed data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9075c288-e7a9-45a4-b4bc-c8a3620d1de5', embedding=None, metadata={'page_label': '55', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Parzen Windows 55\\n40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100 1100.00\\n0.05\\n0.10\\n40\\n 50\\n 60\\n 70\\n 80\\n 90\\n 100 1100.00\\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\nFig. 2.5. Left: a naive density estimate given a sample of the weight of 18 persons.\\nRight: the underlying weight distribution.\\n406080100\\n0.000\\n0.025\\n0.050\\n406080100\\n0.000\\n0.025\\n0.050\\n406080100\\n0.000\\n0.025\\n0.050\\n406080100\\n0.000\\n0.025\\n0.050\\nFig. 2.6. Parzen windows density estimate associated with the 18 observations of\\nthe Figure above. From left to right: Gaussian kernel density estimate with kernel\\nof width 0 .3, 1, 3, and 10 respectively.\\n-2-1012\\n0.0\\n0.5\\n1.0\\n-2-1012\\n0.0\\n0.5\\n1.0\\n-2-1012\\n0.0\\n0.5\\n1.0\\n-2-1012\\n0.0\\n0.5\\n1.0\\nFig. 2.7. Some kernels for Parzen windows density estimation. From left to right:\\nGaussian kernel, Laplace kernel, Epanechikov kernel, and uniform density.\\nMoreover, there is the issue of choosing a suitable kernel function. The\\nfact that a large variety of them exists might suggest that this is a crucial\\nissue. In practice, this turns out not to be the case and instead, the choice\\nof a suitable kernel width is much more vital for good estimates. In other\\nwords, size matters, shape is secondary.\\nThe problem is that we do not know which kernel width is best for the\\ndata. If the problem is one-dimensional, we might hope to be able to eyeball\\nthe size of r. Obviously, in higher dimensions this approach fails. A second', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ce161cef-0086-4c0d-bdfc-c2d393abd745', embedding=None, metadata={'page_label': '56', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='56 2 Density Estimation\\noption would be to choose r such that the log-likelihood of the data is\\nmaximized. It is given by\\nlog\\nm∏\\ni=1\\np(xi) = −m log m +\\nm∑\\ni=1\\nlog\\nm∑\\nj=1\\nr−dh\\n(xi−xj\\nr\\n)\\n(2.33)\\nRemark 2.13 (Log-likelihood) We consider the logarithm of the likeli-\\nhood for reasons of computational stability to prevent numerical underﬂow.\\nWhile each term p(xi) might be within a suitable range, say 10−2, the prod-\\nuct of 1000 of such terms will easily exceed the exponent of ﬂoating point\\nrepresentations on a computer. Summing over the logarithm, on the other\\nhand, is perfectly feasible even for large numbers of observations.\\nUnfortunately computing the log-likelihood is equally infeasible: for decreas-\\ning r the only surviving terms in (2.33) are the functions h((xi − xi)/r) =\\nh(0), since the arguments of all other kernel functions diverge. In other\\nwords, the log-likelihood is maximized when p(x) is peaked exactly at the\\nlocations where we observed the data. The graph on the left of Figure 2.6\\nshows what happens in such a situation.\\nWhat we just experienced is a case of overﬁtting where our model is too\\nﬂexible. This led to a situation where our model was able to explain the\\nobserved data “unreasonably well”, simply because we were able to adjust\\nour parameters given the data. We will encounter this situation throughout\\nthe book. There exist a number of ways to address this problem.\\nValidation Set: We could use a subset of our set of observations as an\\nestimate of the log-likelihood. That is, we could partition the obser-\\nvations into X := {x1, . . . , xn} and X′ := {xn+1, . . . , xm} and use\\nthe second part for a likelihood score according to (2.33). The second\\nset is typically called a validation set.\\nn-fold Cross-validation: Taking this idea further, note that there is no\\nparticular reason why any given xi should belong to X or X′ respec-\\ntively. In fact, we could use all splits of the observations into sets\\nX and X′ to infer the quality of our estimate. While this is compu-\\ntationally infeasible, we could decide to split the observations into\\nn equally sized subsets, say X1, . . . ,Xn and use each of them as a\\nvalidation set at a time while the remainder is used to generate a\\ndensity estimate.\\nTypically n is chosen to be 10, in which case this procedure is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c2e41ac2-5d22-4faf-ac59-08273d85f99d', embedding=None, metadata={'page_label': '57', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Parzen Windows 57\\nreferred to as 10-fold cross-validation . It is a computationally at-\\ntractive procedure insofar as it does not require us to change the\\nbasic estimation algorithm. Nonetheless, computation can be costly.\\nLeave-one-out Estimator: At the extreme end of cross-validation we could\\nchoose n = m. That is, we only remove a single observation at a time\\nand use the remainder of the data for the estimate. Using the average\\nover the likelihood scores provides us with an even more ﬁne-grained\\nestimate. Denote by pi(x) the density estimate obtained by using\\nX := {x1, . . . , xm} without xi. For a Parzen windows estimate this\\nis given by\\npi(xi) = (m − 1)−1 ∑\\nj̸=i\\nr−dh\\n(xi−xj\\nr\\n)\\n= m\\nm−1\\n[\\np(xi) − r−dh(0)\\n]\\n.\\n(2.34)\\nNote that this is precisely the term r−dh(0) that is removed from\\nthe estimate. It is this term which led to divergent estimates for\\nr → 0. This means that the leave-one-out log-likelihood estimate\\ncan be computed easily via\\nL(X) = m log m\\nm−1 +\\nm∑\\ni=1\\nlog\\n[\\np(xi) − r−dh(0)\\n]\\n. (2.35)\\nWe then choose r such that L(X) is maximized. This strategy is very\\nrobust and whenever it can be implemented in a computationally\\neﬃcient manner, it is very reliable in performing model selection.\\nAn alternative, probably more of theoretical interest, is to choose the scale r\\na priori based on the amount of data we have at our disposition. Intuitively,\\nwe need a scheme which ensures that r → 0 as the number of observations\\nincreases m → ∞ . However, we need to ensure that this happens slowly\\nenough that the number of observations within ranger keeps on increasing in\\norder to ensure good statistical performance. For details we refer the reader\\nto [Sil86]. Chapter ?? discusses issues of model selection for estimators in\\ngeneral in considerably more detail.\\n2.2.4 Silverman’s Rule\\nAssume you are an aspiring demographer who wishes to estimate the popu-\\nlation density of a country, say Australia. You might have access to a limited\\ncensus which, for a random portion of the population determines where they\\nlive. As a consequence you will obtain a relatively high number of samples', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='50daa72c-2b1e-41c1-8e98-de5ae57a1f11', embedding=None, metadata={'page_label': '58', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='58 2 Density Estimation\\nFig. 2.8. Nonuniform density. Left: original density with samples drawn from the\\ndistribution. Middle: density estimate with a uniform kernel. Right: density estimate\\nusing Silverman’s adjustment.\\nof city dwellers, whereas the number of people living in the countryside is\\nlikely to be very small.\\nIf we attempt to perform density estimation using Parzen windows, we\\nwill encounter an interesting dilemma: in regions of high density (i.e. the\\ncities) we will want to choose a narrow kernel width to allow us to model\\nthe variations in population density accurately. Conversely, in the outback,\\na very wide kernel is preferable, since the population there is very low.\\nUnfortunately, this information is exactly what a density estimator itself\\ncould tell us. In other words we have a chicken and egg situation where\\nhaving a good density estimate seems to be necessary to come up with a\\ngood density estimate.\\nFortunately this situation can be addressed by realizing that we do not\\nactually need to know the density but rather a rough estimate of the latter.\\nThis can be obtained by using information about the average distance of the\\nk nearest neighbors of a point. One of Silverman’s rules of thumb [Sil86] is\\nto choose ri as\\nri = c\\nk\\n∑\\nx∈kN N(xi)\\n∥x − xi∥ . (2.36)\\nTypically c is chosen to be 0 .5 and k is small, e.g. k = 9 to ensure that the\\nestimate is computationally eﬃcient. The density estimate is then given by\\np(x) = 1\\nm\\nm∑\\ni=1\\nr−d\\ni h\\n(\\nx−xi\\nri\\n)\\n. (2.37)\\nFigure 2.8 shows an example of such a density estimate. It is clear that a\\nlocality dependent kernel width is better than choosing a uniformly constant\\nkernel density estimate. However, note that this increases the computational\\ncomplexity of performing a density estimate, since ﬁrst the k nearest neigh-\\nbors need to be found before the density estimate can be carried out.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2e4d0375-f169-4426-9a2d-d014cd90ff11', embedding=None, metadata={'page_label': '59', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.2 Parzen Windows 59\\n2.2.5 Watson-Nadaraya Estimator\\nNow that we are able to perform density estimation we may use it to perform\\nclassiﬁcation and regression. This leads us to an eﬀective method for non-\\nparametric data analysis, the Watson-Nadaraya estimator [Wat64, Nad65].\\nThe basic idea is very simple: assume that we have a binary classiﬁcation\\nproblem, i.e. we need to distinguish between two classes. Provided that we\\nare able to compute density estimates p(x) given a set of observations X we\\ncould appeal to Bayes rule to obtain\\np(y|x) = p(x|y)p(y)\\np(x) =\\nmy\\nm · 1\\nmy\\n∑\\ni:yi=y r−dh\\n(xi−x\\nr\\n)\\n1\\nm\\n∑m\\ni=1 r−dh\\n(xi−x\\nr\\n) . (2.38)\\nHere we only take the sum over all xi with label yi = y in the numerator.\\nThe advantage of this approach is that it is very cheap to design such an\\nestimator. After all, we only need to compute sums. The downside, similar\\nto that of the k-nearest neighbor classiﬁer is that it may require sums (or\\nsearch) over a large number of observations. That is, evaluation of (2.38) is\\npotentially an O(m) operation. Fast tree based representations can be used\\nto accelerate this [BKL06, KM00], however their behavior depends signiﬁ-\\ncantly on the dimensionality of the data. We will encounter computationally\\nmore attractive methods at a later stage.\\nFor binary classiﬁcation (2.38) can be simpliﬁed considerably. Assume\\nthat y ∈ {±1}. For p(y = 1|x) > 0.5 we will choose that we should estimate\\ny = 1 and in the converse case we would estimate y = −1. Taking the\\ndiﬀerence between twice the numerator and the denominator we can see\\nthat the function\\nf(x) =\\n∑\\ni yih\\n(xi−x\\nr\\n)\\n∑\\ni h\\n(xi−x\\nr\\n) =\\n∑\\ni\\nyi\\nh\\n(xi−x\\nr\\n)\\n∑\\ni h\\n(xi−x\\nr\\n)=:\\n∑\\ni\\nyiwi(x) (2.39)\\ncan be used to achieve the same goal since f(x) > 0 ⇐ ⇒p(y = 1|x) > 0.5.\\nNote that f(x) is a weighted combination of the labels yi associated with\\nweights wi(x) which depend on the proximity of x to an observation xi.\\nIn other words, (2.39) is a smoothed-out version of the k-nearest neighbor\\nclassiﬁer of Section 1.3.2. Instead of drawing a hard boundary at thek closest\\nobservation we use a soft weighting scheme with weights wi(x) depending\\non which observations are closest.\\nNote furthermore that the numerator of (2.39) is very similar to the simple\\nclassiﬁer of Section 1.3.3. In fact, for kernels k(x, x′) such as the Gaussian\\nRBF kernel, which are also kernels in the sense of a Parzen windows den-\\nsity estimate, i.e. k(x, x′) = r−dh\\n(\\nx−x′\\nr\\n)\\nthe two terms are identical. This', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7111e2b1-95ae-4a5e-a5f8-568d66b7fd75', embedding=None, metadata={'page_label': '60', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='60 2 Density Estimation\\nFig. 2.9. Watson Nadaraya estimate. Left: a binary classiﬁer. The optimal solution\\nwould be a straight line since both classes were drawn from a normal distribution\\nwith the same variance. Right: a regression estimator. The data was generated from\\na sinusoid with additive noise. The regression tracks the sinusoid reasonably well.\\nmeans that the Watson Nadaraya estimator provides us with an alternative\\nexplanation as to why (1.24) leads to a usable classiﬁer.\\nIn the same fashion as the Watson Nadaraya classiﬁer extends the k-\\nnearest neighbor classiﬁer we also may construct a Watson Nadaraya re-\\ngression estimator by replacing the binary labels yi by real-valued values\\nyi ∈ R to obtain the regression estimator ∑\\ni yiwi(x). Figure 2.9 has an ex-\\nample of the workings of both a regression estimator and a classiﬁer. They\\nare easy to use and they work well for moderately dimensional data.\\n2.3 Exponential Families\\nDistributions from the exponential family are some of the most versatile\\ntools for statistical inference. Gaussians, Poisson, Gamma and Wishart dis-\\ntributions all form part of the exponential family. They play a key role in\\ndealing with graphical models, classiﬁcation, regression and conditional ran-\\ndom ﬁelds which we will encounter in later parts of this book. Some of the\\nreasons for their popularity are that they lead to convex optimization prob-\\nlems and that they allow us to describe probability distributions by linear\\nmodels.\\n2.3.1 Basics\\nDensities from the exponential family are deﬁned by\\np(x; θ) := p0(x) exp (⟨φ(x), θ⟩ − g(θ)) . (2.40)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='87f9f1c7-a888-4bbd-b1c7-b77d06517b82', embedding=None, metadata={'page_label': '61', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Exponential Families 61\\nHere p0(x) is a density on X and is often called the base measure, φ(x) is\\na map from x to the suﬃcient statistics φ(x). θ is commonly referred to as\\nthe natural parameter. It lives in the space dual to φ(x). Moreover, g(θ) is a\\nnormalization constant which ensures that p(x) is properly normalized. g is\\noften referred to as the log-partition function. The name stems from physics\\nwhere Z = eg(θ) denotes the number of states of a physical ensemble. g can\\nbe computed as follows:\\ng(θ) = log\\n∫\\nX\\nexp (⟨φ(x), θ⟩) dx. (2.41)\\nExample 2.5 (Binary Model) Assume that X = {0; 1} and that φ(x) =\\nx. In this case we have g(θ) = log\\n[\\ne0 + eθ]\\n= log\\n[\\n1 + eθ]\\n. It follows that\\np(x = 0; θ) = 1\\n1+eθ and p(x = 1; θ) = eθ\\n1+eθ . In other words, by choosing\\ndiﬀerent values of θ one can recover diﬀerent Bernoulli distributions.\\nOne of the convenient properties of exponential families is that the log-\\npartition function g can be used to generate moments of the distribution\\nitself simply by taking derivatives.\\nTheorem 2.14 (Log partition function) The function g(θ) is convex.\\nMoreover, the distribution p(x; θ) satisﬁes\\n∇θg(θ) = Ex [φ(x)] and ∇2\\nθg(θ) = Varx [φ(x)] . (2.42)\\nProof Note that ∇2\\nθg(θ) = Varx [φ(x)] implies that g is convex, since the\\ncovariance matrix is positive semideﬁnite. To show (2.42) we expand\\n∇θg(θ) =\\n∫\\nX φ(x) exp⟨φ(x), θ⟩ dx∫\\nX exp ⟨φ(x), θ⟩ =\\n∫\\nφ(x)p(x; θ)dx = Ex [φ(x)] . (2.43)\\nNext we take the second derivative to obtain\\n∇2\\nθg(θ) =\\n∫\\nX\\nφ(x) [φ(x) − ∇θg(θ)]⊤ p(x; θ)dx (2.44)\\n= Ex\\n[\\nφ(x)φ(x)⊤\\n]\\n− Ex [φ(x)] Ex [φ(x)]⊤ (2.45)\\nwhich proves the claim. For the ﬁrst equality we used (2.43). For the second\\nline we used the deﬁnition of the variance.\\nOne may show that higher derivatives ∇n\\nθ g(θ) generate higher order cu-\\nmulants of φ(x) under p(x; θ). This is why g is often also referred as the\\ncumulant-generating function. Note that in general, computation of g(θ)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f724ede6-5b64-4858-956b-4ce49d23b1b7', embedding=None, metadata={'page_label': '62', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='62 2 Density Estimation\\nis nontrivial since it involves solving a highdimensional integral. For many\\ncases, in fact, the computation is NP hard, for instance when X is the do-\\nmain of permutations [FJ95]. Throughout the book we will discuss a number\\nof approximation techniques which can be applied in such a case.\\nLet us brieﬂy illustrate (2.43) using the binary model of Example 2.5.\\nWe have that ∇θ = eθ\\n1+eθ and ∇2\\nθ = eθ\\n(1+eθ)2 . This is exactly what we would\\nhave obtained from direct computation of the mean p(x = 1; θ) and variance\\np(x = 1; θ) − p(x = 1; θ)2 subject to the distribution p(x; θ).\\n2.3.2 Examples\\nA large number of densities are members of the exponential family. Note,\\nhowever, that in statistics it is not common to express them in the dot\\nproduct formulation for historic reasons and for reasons of notational com-\\npactness. We discuss a number of common densities below and show why\\nthey can be written in terms of an exponential family. A detailed description\\nof the most commonly occurring types are given in a table.\\nGaussian Let x, µ ∈ Rd and let Σ ∈ Rd×d where Σ ≻ 0, that is, Σ is a\\npositive deﬁnite matrix. In this case the normal distribution can be\\nexpressed via\\np(x) = (2π)−d\\n2 |Σ|− 1\\n2 exp\\n(\\n−1\\n2(x − µ)⊤Σ−1(x − µ)\\n)\\n(2.46)\\n= exp\\n(\\nx⊤ [\\nΣ−1µ\\n]\\n+ tr\\n([\\n−1\\n2 xx⊤\\n] [\\nΣ−1])\\n− c(µ, Σ)\\n)\\nwhere c(µ, Σ) = 1\\n2 µ⊤Σ−1µ + d\\n2 log 2π + 1\\n2 log |Σ|. By combining the\\nterms in x into φ(x) := (x, − 1\\n2 xx⊤) we obtain the suﬃcient statistics\\nof x. The corresponding linear coeﬃcients (Σ−1µ, Σ−1) constitute the\\nnatural parameter θ. All that remains to be done to express p(x) in\\nterms of (2.40) is to rewrite g(θ) in terms of c(µ, Σ). The summary\\ntable on the following page contains details.\\nMultinomial Another popular distribution is one over k discrete events.\\nIn this case X = {1, . . . , k} and we have in completely generic terms\\np(x) = πx where πx ≥ 0 and ∑\\nx πx = 1. Now denote by ex ∈ Rk the\\nx-th unit vector of the canonical basis, that is ⟨ex, ex′⟩ = 1 if x = x′\\nand 0 otherwise. In this case we may rewrite p(x) via\\np(x) = πx = exp (⟨ex, log π⟩) (2.47)\\nwhere log π = (log π1, . . . ,log πk). In other words, we have succeeded', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c9df947a-a733-4f39-827d-96ea33da4a88', embedding=None, metadata={'page_label': '63', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Exponential Families 63\\nin rewriting the distribution as a member of the exponential family\\nwhere φ(x) = ex and where θ = log π. Note that in this deﬁnition θ\\nis restricted to a k −1 dimensional manifold (the k dimensional prob-\\nability simplex). If we relax those constraints we need to ensure that\\np(x) remains normalized. Details are given in the summary table.\\nPoisson This distribution is often used to model distributions over discrete\\nevents. For instance, the number of raindrops which fall on a given\\nsurface area in a given amount of time, the number of stars in a\\ngiven volume of space, or the number of Prussian soldiers killed by\\nhorse-kicks in the Prussian cavalry all follow this distribution. It is\\ngiven by\\np(x) = e−λλx\\nx! = 1\\nx! exp (x log λ − λ) where x ∈ N0 . (2.48)\\nBy deﬁning φ(x) = x we obtain an exponential families model. Note\\nthat things are a bit less trivial here since 1\\nx! is the nonuniform\\ncounting measure on N0. The case of the uniform measure which\\nleads to the exponential distribution is discussed in Problem 2.16.\\nThe reason why many discrete processes follow the Poisson distri-\\nbution is that it can be seen as the limit over the average of a large\\nnumber of Bernoulli draws: denote by z ∈ {0, 1} a random variable\\nwith p(z = 1) = α. Moreover, denote by zn the sum over n draws\\nfrom this random variable. In this case zn follows the multinomial\\ndistribution with p(zn = k) =\\n(n\\nk\\n)\\nαk(1 − α)n−k. Now assume that\\nwe let n → ∞ such that the expected value of zn remains constant.\\nThat is, we rescale α = λ\\nn. In this case we have\\np(zn = k) = n!\\n(n − k)!k!\\nλk\\nnk\\n(\\n1 − λ\\nn\\n)n−k\\n(2.49)\\n= λk\\nk!\\n(\\n1 − λ\\nn\\n)n [\\nn!\\nnk(n − k)!\\n(\\n1 − λ\\nn\\n)k]\\nFor n → ∞ the second term converges to e−λ. The third term con-\\nverges to 1, since we have a product of only 2 k terms, each of which\\nconverge to 1. Using the exponential families notation we may check\\nthat E[x] = λ and that moreover also Var[x] = λ.\\nBeta This is a distribution on the unit interval X = [0 , 1] which is very\\nversatile when it comes to modelling unimodal and bimodal distri-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='64207d11-a2ad-429c-99ad-af59c9a862bf', embedding=None, metadata={'page_label': '64', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='64 2 Density Estimation\\n0\\n 5\\n 10\\n 15\\n 20\\n 25 300.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8 1.00.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nFig. 2.10. Left: Poisson distributions with λ = {1, 3, 10}. Right: Beta distributions\\nwith a = 2 and b ∈ { 1, 2, 3, 5, 7}. Note how with increasing b the distribution\\nbecomes more peaked close to the origin.\\nbutions. It is given by\\np(x) = xa−1(1 − x)b−1 Γ(a + b)\\nΓ(a)Γ(b) . (2.50)\\nTaking logarithms we see that this, too, is an exponential families\\ndistribution, since p(x) = exp(( a − 1) logx + (b − 1) log(1 − x) +\\nlog Γ(a + b) − log Γ(a) − log Γ(b)).\\nFigure 2.10 has a graphical description of the Poisson distribution and the\\nBeta distribution. For a more comprehensive list of exponential family dis-\\ntributions see the table below and [Fel71, FT94, MN83]. In principle any\\nmap φ(x), domain X with underlying measure µ are suitable, as long as the\\nlog-partition function g(θ) can be computed eﬃciently.\\nTheorem 2.15 (Convex feasible domain) The domain of deﬁnition Θ\\nof g(θ) is convex.\\nProof By construction g is convex and diﬀerentiable everywhere. Hence the\\nbelow-sets for all values c with {x|g(x) ≤ c} exist. Consequently the domain\\nof deﬁnition is convex.\\nHaving a convex function is very valuable when it comes to parameter infer-\\nence since convex minimization problems have unique minimum values and\\nglobal minima. We will discuss this notion in more detail when designing\\nmaximum likelihood estimators.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ef72446b-9f10-4d0f-8fab-6da05b6da9fd', embedding=None, metadata={'page_label': '65', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.3 Exponential Families 65\\nName Domain X Measure φ(x) g(θ) Domain Θ\\nBernoulli {0, 1} Counting x log\\n(\\n1 + eθ)\\nR\\nMultinomial {1..N } Counting ex log ∑N\\ni=1 eθi RN\\nExponential N+\\n0 Counting x − log\\n(\\n1 − eθ)\\n(−∞, 0)\\nPoisson N+\\n0\\n1\\nx! x e θ R\\nLaplace [0 , ∞) Lebesgue x log θ (−∞, 0)\\nGaussian R Lebesgue\\n(\\nx, − 1\\n2 x2) 1\\n2 log 2π − 1\\n2 log θ2 + 1\\n2\\nθ2\\n1\\nθ2\\nR ×(0, ∞)\\nRn Lebesgue\\n(\\nx, − 1\\n2 xx⊤) n\\n2 log 2π − 1\\n2 log |θ2| + 1\\n2 θ⊤\\n1 θ−1\\n2 θ1 Rn ×Cn\\nInverse Normal [0 , ∞) x− 3\\n2\\n(\\n−x, − 1\\nx\\n) 1\\n2 log π − 2√θ1θ2 − 1\\n2 log θ2 (0, ∞)2\\nBeta [0 , 1] 1\\nx(1−x) (log x, log (1 − x)) log Γ(θ1)Γ(θ2)\\nΓ(θ1+θ2) R2\\nGamma [0 , ∞) 1\\nx (log x, −x) log Γ( θ1) − θ1 log θ2 (0, ∞)2\\nWishart Cn |X|−n+1\\n2\\n(\\nlog |x|, − 1\\n2 x\\n)\\n−θ1 log |θ2| + θ1n log 2 R ×Cn\\n+ ∑n\\ni=1 log Γ\\n(\\nθ1 + 1−i\\n2\\n)\\nDirichlet Sn (∏n\\ni=1 xi)−1 (log x1, . . . ,log xn) ∑n\\ni=1 log Γ(θi) − log Γ (∑n\\ni=1 θi) ( R+)n\\nInverse χ2 R+ e− 1\\n2x − log x (θ − 1) log 2 + log(θ − 1) (0 , ∞)\\nLogarithmic N 1\\nx x log(− log(1 − eθ)) ( −∞, 0)\\nConjugate Θ Lebesgue ( θ, −g(θ)) generic\\nSn denotes the probability simplex in n dimensions. Cn is the cone of positive semideﬁnite matrices in Rn×n.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='903e51c0-586a-4873-a001-40f191d29c51', embedding=None, metadata={'page_label': '66', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='66 2 Density Estimation\\n2.4 Estimation\\nIn many statistical problems the challenge is to estimate parameters of in-\\nterest. For instance, in the context of exponential families, we may want\\nto estimate a parameter ˆθ such that it is close to the “true” parameter θ∗\\nin the distribution. While the problem is fully general, we will describe the\\nrelevant steps in obtaining estimates for the special case of the exponential\\nfamily. This is done for two reasons — ﬁrstly, exponential families are an\\nimportant special case and we will encounter slightly more complex variants\\non the reasoning in later chapters of the book. Secondly, they are of a suﬃ-\\nciently simple form that we are able to show a range of diﬀerent techniques.\\nIn more advanced applications only a small subset of those methods may be\\npractically feasible. Hence exponential families provide us with a working\\nexample based on which we can compare the consequences of a number of\\ndiﬀerent techniques.\\n2.4.1 Maximum Likelihood Estimation\\nWhenever we have a distribution p(x; θ) parametrized by some parameter\\nθ we may use data to ﬁnd a value of θ which maximizes the likelihood that\\nthe data would have been generated by a distribution with this choice of\\nparameter.\\nFor instance, assume that we observe a set of temperature measurements\\nX = {x1, . . . , xm}. In this case, we could try ﬁnding a normal distribution\\nsuch that the likelihood p(X; θ) of the data under the assumption of a normal\\ndistribution is maximized. Note that this does not imply in any way that the\\ntemperature measurements are actually drawn from a normal distribution.\\nInstead, it means that we are attempting to ﬁnd the Gaussian which ﬁts the\\ndata in the best fashion.\\nWhile this distinction may appear subtle, it is critical: we do not assume\\nthat our model accurately reﬂects reality. Instead, we simply try doing the\\nbest possible job at modeling the data given a speciﬁed model class. Later\\nwe will encounter alternative approaches at estimation, namely Bayesian\\nmethods, which make the assumption that our model ought to be able to\\ndescribe the data accurately.\\nDeﬁnition 2.16 (Maximum Likelihood Estimator) For a model p(·; θ)\\nparametrized by θ and observations X the maximum likelihood estimator\\n(MLE) is\\nˆθML[X] := argmax\\nθ\\np(X; θ). (2.51)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4d0ff22e-18ae-4899-bc41-d5e0cb0a68cc', embedding=None, metadata={'page_label': '67', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Estimation 67\\nIn the context of exponential families this leads to the following procedure:\\ngiven m observations drawn iid from some distribution, we can express the\\njoint likelihood as\\np(X; θ) =\\nm∏\\ni=1\\np(xi; θ) =\\nm∏\\ni=1\\nexp (⟨φ(xi), θ⟩ − g(θ)) (2.52)\\n= exp (m (⟨µ[X], θ⟩ − g(θ))) (2.53)\\nwhere µ[X] := 1\\nm\\nm∑\\ni=1\\nφ(xi). (2.54)\\nHere µ[X] is the empirical average of the map φ(x). Maximization of p(X; θ)\\nis equivalent to minimizing the negative log-likelihood − log p(X; θ). The\\nlatter is a common practical choice since for independently drawn data,\\nthe product of probabilities decomposes into the sum of the logarithms of\\nindividual likelihoods. This leads to the following objective function to be\\nminimized\\n− log p(X; θ) = m [g(θ) − ⟨θ, µ[X]⟩] (2.55)\\nSince g(θ) is convex and ⟨θ, µ[X]⟩ is linear in θ, it follows that minimization\\nof (2.55) is a convex optimization problem. Using Theorem 2.14 and the ﬁrst\\norder optimality condition ∇θg(θ) = µ[X] for (2.55) implies that\\nθ = [∇θg]−1 (µ[X]) or equivalently Ex∼p(x;θ)[φ(x)] = ∇θg(θ) = µ[X].\\n(2.56)\\nPut another way, the above conditions state that we aim to ﬁnd the distribu-\\ntion p(x; θ) which has the same expected value of φ(x) as what we observed\\nempirically via µ[X]. Under very mild technical conditions a solution to\\n(2.56) exists.\\nIn general, (2.56) cannot be solved analytically. In certain special cases,\\nthough, this is easily possible. We discuss two such choices in the following:\\nMultinomial and Poisson distributions.\\nExample 2.6 (Poisson Distribution) For the Poisson distribution1 where\\np(x; θ) = 1\\nx! exp(θx − eθ) it follows that g(θ) = eθ and φ(x) = x. This allows\\n1 Often the Poisson distribution is speciﬁed using λ := log θ as its rate parameter. In this case we\\nhave p(x; λ) = λxe−λ/x! as its parametrization. The advantage of the natural parametrization\\nusing θ is that we can directly take advantage of the properties of the log-partition function as\\ngenerating the cumulants of x.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='edd4937b-7ca5-42f3-8236-df0313c21316', embedding=None, metadata={'page_label': '68', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='68 2 Density Estimation\\nus to solve (2.56) in closed form using\\n∇θg(θ) = eθ = 1\\nm\\nm∑\\ni=1\\nxi and hence θ = log\\nm∑\\ni=1\\nxi − log m. (2.57)\\nExample 2.7 (Multinomial Distribution) For the multinomial distri-\\nbution the log-partition function is given by g(θ) = log ∑N\\ni=1 eθi, hence we\\nhave that\\n∇ig(θ) = eθi\\n∑N\\nj=1 eθj\\n= 1\\nm\\nm∑\\nj=1\\n{xj = i} . (2.58)\\nIt is easy to check that (2.58) is satisﬁed for eθi = ∑m\\nj=1 {xj = i}. In other\\nwords, the MLE for a discrete distribution simply given by the empirical\\nfrequencies of occurrence.\\nThe multinomial setting also exhibits two rather important aspects of ex-\\nponential families: ﬁrstly, choosing θi = c + log ∑m\\ni=1 {xj = i} for any c ∈ R\\nwill lead to an equivalent distribution. This is the case since the suﬃcient\\nstatistic φ(x) is not minimal. In our context this means that the coordinates\\nof φ(x) are linearly dependent — for any x we have that ∑\\nj[φ(x)]j = 1,\\nhence we could eliminate one dimension. This is precisely the additional\\ndegree of freedom which is reﬂected in the scaling freedom in θ.\\nSecondly, for data where some events do not occur at all, the expression\\nlog\\n[∑m\\nj=1 {xj = i}\\n]\\n= log 0 is ill deﬁned. This is due to the fact that this\\nparticular set of counts occurs on the boundary of the convex set within\\nwhich the natural parameters θ are well deﬁned. We will see how diﬀerent\\ntypes of priors can alleviate the issue.\\nUsing the MLE is not without problems. As we saw in Figure 2.1, conver-\\ngence can be slow, since we are not using any side information. The latter\\ncan provide us with problems which are both numerically better conditioned\\nand which show better convergence, provided that our assumptions are ac-\\ncurate. Before discussing a Bayesian approach to estimation, let us discuss\\nbasic statistical properties of the estimator.\\n2.4.2 Bias, Variance and Consistency\\nWhen designing any estimator ˆθ(X) we would like to obtain a number of\\ndesirable properties: in general it should not be biased towards a particular\\nsolution unless we have good reason to believe that this solution should\\nbe preferred. Instead, we would like the estimator to recover, at least on', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='97598de6-581d-44fd-86d8-8dd3eff37b0e', embedding=None, metadata={'page_label': '69', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Estimation 69\\naverage, the “correct” parameter, should it exist. This can be formalized in\\nthe notion of an unbiased estimator.\\nSecondly, we would like that, even if no correct parameter can be found,\\ne.g. when we are trying to ﬁt a Gaussian distribution to data which is not\\nnormally distributed, that we will converge to the best possible parameter\\nchoice as we obtain more data. This is what is understood by consistency.\\nFinally, we would like the estimator to achieve low bias and near-optimal\\nestimates as quickly as possible. The latter is measured by the eﬃciency\\nof an estimator. In this context we will encounter the Cram´ er-Rao bound\\nwhich controls the best possible rate at which an estimator can achieve this\\ngoal. Figure 2.11 gives a pictorial description.\\nFig. 2.11. Left: unbiased estimator; the estimates, denoted by circles have as mean\\nthe true parameter, as denoted by a star. Middle: consistent estimator. While the\\ntrue model is not within the class we consider (as denoted by the ellipsoid), the\\nestimates converge to the white star which is the best model within the class that\\napproximates the true model, denoted by the solid star. Right: diﬀerent estimators\\nhave diﬀerent regions of uncertainty, as made explicit by the ellipses around the\\ntrue parameter (solid star).\\nDeﬁnition 2.17 (Unbiased Estimator) An estimator ˆθ[X] is unbiased\\nif for all θ where X ∼ p(X; θ) we have EX[ˆθ[X]] = θ.\\nIn other words, in expectation the parameter estimate matches the true pa-\\nrameter. Note that this only makes sense if a true parameter actually exists.\\nFor instance, if the data is Poisson distributed and we attempt modeling it\\nby a Gaussian we will obviously not obtain unbiased estimates.\\nFor ﬁnite sample sizes MLE is often biased. For instance, for the normal\\ndistribution the variance estimates carry bias O(m−1). See problem 2.19\\nfor details. In general, under fairly mild conditions, MLE is asymptotically\\nunbiased [DGL96]. We prove this for exponential families. For more general\\nsettings the proof depends on the dimensionality and smoothness of the\\nfamily of densities that we have at our disposition.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4f60b981-3dda-4031-a272-f92f10ee029c', embedding=None, metadata={'page_label': '70', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='70 2 Density Estimation\\nTheorem 2.18 (MLE for Exponential Families) Assume that X is an\\nm-sample drawn iid from p(x; θ). The estimate ˆθ[X] = g−1(µ[X]) is asymp-\\ntotically normal with\\nm− 1\\n2 [ˆθ[X] − θ] → N(0,\\n[\\n∇2\\nθg(θ)\\n]−1\\n). (2.59)\\nIn other words, the estimate ˆθ[X] is asymptotically normal, it converges to\\nthe true parameter θ, and moreover, the variance at the correct parameter\\nis given by the inverse of the covariance matrix of the data, as given by the\\nsecond derivative of the log-partition function ∇2\\nθg(θ).\\nProof Denote by µ = ∇θg(θ) the true mean. Moreover, note that ∇2\\nθg(θ) is\\nthe covariance of the data drawn from p(x; θ). By the central limit theorem\\n(Theorem 2.3) we have that n− 1\\n2 [µ[X] − µ] → N(0, ∇2\\nθg(θ)).\\nNow note that ˆθ[X] = [ ∇θg]−1 (µ[X]). Therefore, by the delta method\\n(Theorem 2.5) we know that ˆθ[X] is also asymptotically normal. Moreover,\\nby the inverse function theorem the Jacobian ofg−1 satisﬁes ∇µ [∇θg]−1 (µ) =[\\n∇2\\nθg(θ)\\n]−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\\nNow that we established the asymptotic properties of the MLE for exponen-\\ntial families it is only natural to ask how much variation one may expect in\\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\\nTheorem 2.19 (Cram´ er and Rao [Rao73])Assume that X is drawn from\\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I\\nthe Fisher information matrix and by B the variance of ˆθ[X] where\\nI := Cov [∇θ log p(X; θ)] and B := Var\\n[\\nˆθ[X]\\n]\\n. (2.60)\\nIn this case det IB ≥ 1 for all estimators ˆθ[X].\\nProof We prove the claim for the scalar case. The extension to matrices is\\nstraightforward. Using the Cauchy-Schwarz inequality we have\\nCov2\\n[\\n∇θ log p(X; θ), ˆθ[X]\\n]\\n≤ Var [∇θ log p(X; θ)] Var\\n[\\nˆθ[X]\\n]\\n= IB. (2.61)\\nNote that at the true parameter the expected log-likelihood score vanishes\\nEX[∇θ log p(X; θ)] = ∇θ\\n∫\\np(X; θ)dX = ∇θ1 = 0. (2.62)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='619daa5e-1064-491c-bed6-8d462cc6a248', embedding=None, metadata={'page_label': '71', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Estimation 71\\nHence we may simplify the covariance formula by dropping the means via\\nCov\\n[\\n∇θ log p(X; θ), ˆθ[X]\\n]\\n= EX\\n[\\n∇θ log p(X; θ)ˆθ[X]\\n]\\n=\\n∫\\np(X; θ)ˆθ(X)∇θ log p(X; θ)dθ\\n= ∇θ\\n∫\\np(X; θ)ˆθ(X)dX = ∇θθ = 1.\\nHere the last equality follows since we may interchange integration by X\\nand the derivative with respect to θ.\\nThe Cram´ er-Rao theorem implies that there is a limit to how well we may\\nestimate a parameter given ﬁnite amounts of data. It is also a yardstick by\\nwhich we may measure how eﬃciently an estimator uses data. Formally, we\\ndeﬁne the eﬃciency as the quotient between actual performance and the\\nCram´ er-Rao bound via\\ne := 1/det IB. (2.63)\\nThe closer e is to 1, the lower the variance of the corresponding estimator\\nˆθ(X). Theorem 2.18 implies that for exponential families MLE is asymptot-\\nically eﬃcient. It turns out to be generally true.\\nTheorem 2.20 (Eﬃciency of MLE [Cra46, GW92, Ber85]) The max-\\nimum likelihood estimator is asymptotically eﬃcient ( e = 1).\\nSo far we only discussed the behavior of ˆθ[X] whenever there exists a true θ\\ngenerating p(θ; X). If this is not true, we need to settle for less: how wellˆθ[X]\\napproaches the best possible choice of within the given model class. Such\\nbehavior is referred to as consistency. Note that it is not possible to deﬁne\\nconsistency per se . For instance, we may ask whether ˆθ converges to the\\noptimal parameter θ∗, or whether p(x; ˆθ) converges to the optimal density\\np(x; θ∗), and with respect to which norm. Under fairly general conditions\\nthis turns out to be true for ﬁnite-dimensional parameters and smoothly\\nparametrized densities. See [DGL96, vdG00] for proofs and further details.\\n2.4.3 A Bayesian Approach\\nThe analysis of the Maximum Likelihood method might suggest that in-\\nference is a solved problem. After all, in the limit, MLE is unbiased and it\\nexhibits as small variance as possible. Empirical results using aﬁnite amount\\nof data, as present in Figure 2.1 indicate otherwise.\\nWhile not making any assumptions can lead to interesting and general', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='31ccdb8b-d18e-4987-9db5-442d1ca148b7', embedding=None, metadata={'page_label': '72', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='72 2 Density Estimation\\ntheorems, it ignores the fact that in practice we almost always have some\\nidea about what to expect of our solution. It would be foolish to ignore such\\nadditional information. For instance, when trying to determine the voltage\\nof a battery, it is reasonable to expect a measurement in the order of 1 .5V\\nor less. Consequently such prior knowledge should be incorporated into the\\nestimation process. In fact, the use of side information to guide estimation\\nturns out to be the tool to building estimators which work well in high\\ndimensions.\\nRecall Bayes’ rule (1.15) which states that p(θ|x) = p(x|θ)p(θ)\\np(x) . In our con-\\ntext this means that if we are interested in the posterior probability of θ\\nassuming a particular value, we may obtain this using the likelihood (often\\nreferred to as evidence) of x having been generated by θ via p(x|θ) and our\\nprior belief p(θ) that θ might be chosen in the distribution generating x.\\nObserve the subtle but important diﬀerence to MLE: instead of treating θ\\nas a parameter of a density model, we treat θ as an unobserved random\\nvariable which we may attempt to infer given the observations X.\\nThis can be done for a number of diﬀerent purposes: we might want to\\ninfer the most likely value of the parameter given the posterior distribution\\np(θ|X). This is achieved by\\nˆθMAP(X) := argmax\\nθ\\np(θ|X) = argmin\\nθ\\n− log p(X|θ) − log p(θ). (2.64)\\nThe second equality follows since p(X) does not depend on θ. This estimator\\nis also referred to as the Maximum a Posteriori, or MAP estimator. It diﬀers\\nfrom the maximum likelihood estimator by adding the negative log-prior\\nto the optimization problem. For this reason it is sometimes also referred\\nto as Penalized MLE. Eﬀectively we are penalizing unlikely choices θ via\\n− log p(θ).\\nNote that using ˆθMAP(X) as the parameter of choice is not quite accurate.\\nAfter all, we can only infer a distribution over θ and in general there is no\\nguarantee that the posterior is indeed concentrated around its mode. A more\\naccurate treatment is to use the distribution p(θ|X) directly via\\np(x|X) =\\n∫\\np(x|θ)p(θ|X)dθ. (2.65)\\nIn other words, we integrate out the unknown parameter θ and obtain the\\ndensity estimate directly. As we will see, it is generally impossible to solve\\n(2.65) exactly, an important exception being conjugate priors. In the other\\ncases one may resort to sampling from the posterior distribution to approx-\\nimate the integral.\\nWhile it is possible to design a wide variety of prior distributions, this book', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7315c102-3feb-40bb-b4a3-7de275911974', embedding=None, metadata={'page_label': '73', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Estimation 73\\nfocuses on two important families: norm-constrained prior and conjugate\\npriors. We will encounter them throughout, the former sometimes in the\\nguise of regularization and Gaussian Processes, the latter in the context of\\nexchangeable models such as the Dirichlet Process.\\nNorm-constrained priors take on the form\\np(θ) ∝ exp(−λ ∥θ − θ0∥d\\np) for p, d ≥ 1 and λ > 0. (2.66)\\nThat is, they restrict the deviation of the parameter value θ from some guess\\nθ0. The intuition is that extreme values of θ are much less likely than more\\nmoderate choices of θ which will lead to more smooth and even distributions\\np(x|θ).\\nA popular choice is the Gaussian prior which we obtain for p = d = 1\\nand λ = 1/2σ2. Typically one sets θ0 = 0 in this case. Note that in (2.66)\\nwe did not spell out the normalization of p(θ) — in the context of MAP\\nestimation this is not needed since it simply becomes a constant oﬀset in\\nthe optimization problem (2.64). We have\\nˆθMAP[X] = argmin\\nθ\\nm [g(θ) − ⟨θ, µ[X]⟩] + λ ∥θ − θ0∥d\\np (2.67)\\nFor d, p ≥ 1 and λ ≥ 0 the resulting optimization problem is convex and it\\nhas a unique solution. Moreover, very eﬃcient algorithms exist to solve this\\nproblem. We will discuss this in detail in Chapter 3. Figure 2.12 shows the\\nregions of equal prior probability for a range of diﬀerent norm-constrained\\npriors.\\nAs can be seen from the diagram, the choice of the norm can have profound\\nconsequences on the solution. That said, as we will show in Chapter ??, the\\nestimate ˆθMAP is well concentrated and converges to the optimal solution\\nunder fairly general conditions.\\nAn alternative to norm-constrained priors are conjugate priors. They are\\ndesigned such that the posterior p(θ|X) has the same functional form as the\\nprior p(θ). In exponential families such priors are deﬁned via\\np(θ|n, ν) = exp (⟨nν, θ⟩ − ng(θ) − h(ν, n)) where (2.68)\\nh(ν, n) = log\\n∫\\nexp (⟨nν, θ⟩ − ng(θ)) dθ. (2.69)\\nNote that p(θ|n, ν) itself is a member of the exponential family with the\\nfeature map φ(θ) = (θ, −g(θ)). Hence h(ν, n) is convex in (nν, n). Moreover,\\nthe posterior distribution has the form\\np(θ|X) ∝ p(X|θ)p(θ|n, ν) ∝ exp (⟨mµ[X] + nν, θ⟩ − (m + n)g(θ)) . (2.70)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4b59f37b-157d-4671-9665-cde5e229de8b', embedding=None, metadata={'page_label': '74', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='74 2 Density Estimation\\nFig. 2.12. From left to right: regions of equal prior probability in R2 for priors using\\nthe ℓ1, ℓ2 and ℓ∞ norm. Note that only the ℓ2 norm is invariant with regard to the\\ncoordinate system. As we shall see later, the ℓ1 norm prior leads to solutions where\\nonly a small number of coordinates is nonzero.\\nThat is, the posterior distribution has the same form as a conjugate prior\\nwith parameters mµ[X]+nν\\nm+n and m+ n. In other words, n acts like a phantom\\nsample size and ν is the corresponding mean parameter. Such an interpreta-\\ntion is reasonable given our desire to design a prior which, when combined\\nwith the likelihood remains in the same model class: we treat prior knowl-\\nedge as having observed virtual data beforehand which is then added to the\\nactual set of observations. In this sense data and prior become completely\\nequivalent — we obtain our knowledge either from actual observations or\\nfrom virtual observations which describe our belief into how the data gen-\\neration process is supposed to behave.\\nEq. (2.70) has the added beneﬁt of allowing us to provide an exact nor-\\nmalized version of the posterior. Using (2.68) we obtain that\\np(θ|X) = exp\\n(\\n⟨mµ[X] + nν, θ⟩ − (m + n)g(θ) − h\\n(\\nmµ[X]+nν\\nm+n , m + n\\n))\\n.\\nThe main remaining challenge is to compute the normalization h for a range\\nof important conjugate distributions. The table on the following page pro-\\nvides details. Besides attractive algebraic properties, conjugate priors also\\nhave a second advantage — the integral (2.65) can be solved exactly:\\np(x|X) =\\n∫\\nexp (⟨φ(x), θ⟩ − g(θ)) ×\\nexp\\n(\\n⟨mµ[X] + nν, θ⟩ − (m + n)g(θ) − h\\n(\\nmµ[X]+nν\\nm+n , m + n\\n))\\ndθ\\nCombining terms one may check that the integrand amounts to the normal-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4965c1b5-1831-4303-a5d6-04f6782de5fd', embedding=None, metadata={'page_label': '75', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.4 Estimation 75\\nization in the conjugate distribution, albeit φ(x) added. This yields\\np(x|X) = exp\\n(\\nh\\n(\\nmµ[X]+nν+φ(x)\\nm+n+1 , m + n + 1\\n)\\n− h\\n(\\nmµ[X]+nν\\nm+n , m + n\\n))\\nSuch an expansion is very useful whenever we would like to draw x from\\np(x|X) without the need to obtain an instantiation of the latent variable θ.\\nWe provide explicit expansions in appendix 2. [GS04] use the fact that θ\\ncan be integrated out to obtain what is called a collapsed Gibbs sampler for\\ntopic models [BNJ03].\\n2.4.4 An Example\\nAssume we would like to build a language model based on available doc-\\numents. For instance, a linguist might be interested in estimating the fre-\\nquency of words in Shakespeare’s collected works, or one might want to\\ncompare the change with respect to a collection of webpages. While mod-\\nels describing documents by treating them as bags of words which all have\\nbeen obtained independently of each other are exceedingly simple, they are\\nvaluable for quick-and-dirty content ﬁltering and categorization, e.g. a spam\\nﬁlter on a mail server or a content ﬁlter for webpages.\\nHence we model a document d as a multinomial distribution: denote by\\nwi for i ∈ { 1, . . . , md} the words in d. Moreover, denote by p(w|θ) the\\nprobability of occurrence of word w, then under the assumption that the\\nwords are independently drawn, we have\\np(d|θ) =\\nmd∏\\ni=1\\np(wi|θ). (2.71)\\nIt is our goal to ﬁnd parameters θ such that p(d|θ) is accurate. For a given\\ncollection D of documents denote by mw the number of counts for word w\\nin the entire collection. Moreover, denote by m the total number of words\\nin the entire collection. In this case we have\\np(D|θ) =\\n∏\\ni\\np(di|θ) =\\n∏\\nw\\np(w|θ)mw . (2.72)\\nFinding suitable parameters θ given D proceeds as follows: In a maximum\\nlikelihood model we set\\np(w|θ) = mw\\nm . (2.73)\\nIn other words, we use the empirical frequency of occurrence as our best\\nguess and the suﬃcient statistic ofD is φ(w) = ew, where ew denotes the unit\\nvector which is nonzero only for the “coordinate” w. Hence µ[D]w = mw\\nm .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3e971a9d-dd9c-4001-bb83-5bd21c0e2c70', embedding=None, metadata={'page_label': '76', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='76 2 Density Estimation\\nWe know that the conjugate prior of the multinomial model is a Dirichlet\\nmodel. It follows from (2.70) that the posterior mode is obtained by replacing\\nµ[D] by mµ[D]+nν\\nm+n . Denote by nw := νw · n the pseudo-counts arising from\\nthe conjugate prior with parameters ( ν, n). In this case we will estimate the\\nprobability of the word w as\\np(w|θ) = mw + nw\\nm + n = mw + nνw\\nm + n . (2.74)\\nIn other words, we add the pseudo counts nw to the actual word counts mw.\\nThis is particularly useful when the document we are dealing with is brief,\\nthat is, whenever we have little data: it is quite unreasonable to infer from\\na webpage of approximately 1000 words that words not occurring in this\\npage have zero probability. This is exactly what is mitigated by means of\\nthe conjugate prior ( ν, n).\\nFinally, let us consider norm-constrained priors of the form (2.66). In this\\ncase, the integral required for\\np(D) =\\n∫\\np(D|θ)p(θ)dθ\\n∝\\n∫\\nexp\\n(\\n−λ ∥θ − θ0∥d\\np + m ⟨µ[D], θ⟩ − mg(θ)\\n)\\ndθ\\nis intractable and we need to resort to an approximation. A popular choice\\nis to replace the integral by p(D|θ∗) where θ∗ maximizes the integrand. This\\nis precisely the MAP approximation of (2.64). Hence, in order to perform\\nestimation we need to solve\\nminimize\\nθ\\ng(θ) − ⟨µ[D], θ⟩ + λ\\nm ∥θ − θ0∥d\\np . (2.75)\\nA very simple strategy for minimizing (2.75) is gradient descent. That is for\\na given value of θ we compute the gradient of the objective function and take\\na ﬁxed step towards its minimum. For simplicity assume that d = p = 2 and\\nλ = 1/2σ2, that is, we assume that θ is normally distributed with variance\\nσ2 and mean θ0. The gradient is given by\\n∇θ [− log p(D, θ)] = Ex∼p(x|θ)[φ(x)] − µ[D] + 1\\nmσ2 [θ − θ0] (2.76)\\nIn other words, it depends on the discrepancy between the mean of φ(x)\\nwith respect to our current model and the empirical average µ[X], and the\\ndiﬀerence between θ and the prior mean θ0.\\nUnfortunately, convergence of the procedure θ ← θ − η∇θ [. . .] is usually\\nvery slow, even if we adjust the steplength η eﬃciently. The reason is that\\nthe gradient need not point towards the minimum as the space is most likely', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6d6a3ba6-17e9-4b90-abff-6a8e883f45e5', embedding=None, metadata={'page_label': '77', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 77\\ndistorted. A better strategy is to use Newton’s method (see Chapter 3 for\\na detailed discussion and a convergence proof). It relies on a second order\\nTaylor approximation\\n− log p(D, θ + δ) ≈ − log p(D, θ) + ⟨δ, G⟩ + 1\\n2 δ⊤Hδ (2.77)\\nwhere G and H are the ﬁrst and second derivatives of − log p(D, θ) with\\nrespect to θ. The quadratic expression can be minimized with respect to δ\\nby choosing δ = −H−1G and we can fashion an update algorithm from this\\nby letting θ ← θ −H−1G. One may show (see Chapter 3) that Algorithm 2.1\\nis quadratically convergent. Note that the prior on θ ensures that H is well\\nconditioned even in the case where the variance ofφ(x) is not. In practice this\\nmeans that the prior ensures fast convergence of the optimization algorithm.\\nAlgorithm 2.1 Newton method for MAP estimation\\nNewtonMAP(D)\\nInitialize θ = θ0\\nwhile not converged do\\nCompute G = Ex∼p(x|θ)[φ(x)] − µ[D] + 1\\nmσ2 [θ − θ0]\\nCompute H = Varx∼p(x|θ)[φ(x)] + 1\\nmσ2 1\\nUpdate θ ← θ − H−1G\\nend while\\nreturn θ\\n2.5 Sampling\\nSo far we considered the problem of estimating the underlying probability\\ndensity, given a set of samples drawn from that density. Now let us turn to\\nthe converse problem, that is, how to generate random variables given the\\nunderlying probability density. In other words, we want to design a random\\nvariable generator. This is useful for a number of reasons:\\nWe may encounter probability distributions where optimization over suit-\\nable model parameters is essentially impossible and where it is equally im-\\npossible to obtain a closed form expression of the distribution. In these cases\\nit may still be possible to perform sampling to draw examples of the kind\\nof data we expect to see from the model. Chapter ?? discusses a number of\\ngraphical models where this problem arises.\\nSecondly, assume that we are interested in testing the performance of a\\nnetwork router under diﬀerent load conditions. Instead of introducing the\\nunder-development router in a live network and wreaking havoc, one could', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d2900e9c-54fa-480b-b726-f3df0b0bc482', embedding=None, metadata={'page_label': '78', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='78 2 Density Estimation\\nestimate the probability density of the network traﬃc under various load\\nconditions and build a model. The behavior of the network can then be\\nsimulated by using a probabilistic model. This involves drawing random\\nvariables from an estimated probability distribution.\\nCarrying on, suppose that we generate data packets by sampling and see\\nan anomalous behavior in your router. In order to reproduce and debug\\nthis problem one needs access to the same set of random packets which\\ncaused the problem in the ﬁrst place. In other words, it is often convenient\\nif our random variable generator is reproducible; At ﬁrst blush this seems\\nlike a contradiction. After all, our random number generator is supposed\\nto generate random variables. This is less of a contradiction if we consider\\nhow random numbers are generated in a computer — given a particular\\ninitialization (which typically depends on the state of the system, e.g. time,\\ndisk size, bios checksum, etc.) the random number algorithm produces a\\nsequence of numbers which, for all practical purposes, can be treated as iid.\\nA simple method is the linear congruential generator [PTVF94]\\nxi+1 = (axi + b) modc.\\nThe performance of these iterations depends signiﬁcantly on the choice of the\\nconstants a, b, c. For instance, the GNU C compiler usesa = 1103515245, b =\\n12345 and c = 232. In general b and c need to be relatively prime and a − 1\\nneeds to be divisible by all prime factors of c and by 4. It is very much\\nadvisable not to attempt implementing such generators on one’s own unless\\nit is absolutely necessary.\\nUseful desiderata for a pseudo random number generator (PRNG) are that\\nfor practical purposes it is statistically indistinguishable from a sequence of\\niid data. That is, when applying a number of statistical tests, we will accept\\nthe null-hypothesis that the random variables are iid. See Chapter ?? for\\na detailed discussion of statistical testing procedures for random variables.\\nIn the following we assume that we have access to a uniform RNG U[0, 1]\\nwhich draws random numbers uniformly from the range [0 , 1].\\n2.5.1 Inverse Transformation\\nWe now consider the scenario where we would like to draw from some dis-\\ntinctively non-uniform distribution. Whenever the latter is relatively simple\\nthis can be achieved by applying an inverse transform:\\nTheorem 2.21 For z ∼ p(z) with z ∈ Z and an injective transformation\\nφ : Z → X with inverse transform φ−1 on φ(Z) it follows that the random', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='36d07a92-62e3-4bf1-a236-ead595f6c164', embedding=None, metadata={'page_label': '79', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 79\\n1 2 3 4 5\\n0\\n0.1\\n0.2\\n0.3\\nDiscrete Probability Distribution\\n1 2 3 4 5 60\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nCumulative Density Function\\nFig. 2.13. Left: discrete probability distribution over 5 possible outcomes. Right:\\nassociated cumulative distribution function. When sampling, we draw x uniformly\\nat random from U[0, 1] and compute the inverse of F .\\nvariable x := φ(z) is drawn from\\n⏐⏐∇xφ−1(x)\\n⏐⏐ · p(φ−1(x)). Here\\n⏐⏐∇xφ−1(x)\\n⏐⏐\\ndenotes the determinant of the Jacobian of φ−1.\\nThis follows immediately by applying a variable transformation for a mea-\\nsure, i.e. we change dp(z) to dp(φ−1(x))\\n⏐⏐∇xφ−1(x)\\n⏐⏐. Such a conversion strat-\\negy is particularly useful for univariate distributions.\\nCorollary 2.22 Denote by p(x) a distribution on R with cumulative distri-\\nbution function F (x′) =\\n∫ x′\\n−∞ dp(x). Then the transformation x = φ(z) =\\nF−1(z) converts samples z ∼ U[0, 1] to samples drawn from p(x).\\nWe now apply this strategy to a number of univariate distributions. One of\\nthe most common cases is sampling from a discrete distribution.\\nExample 2.8 (Discrete Distribution) In the case of a discrete distribu-\\ntion over {1, . . . , k} the cumulative distribution function is a step-function\\nwith steps at {1, . . . , k} where the height of each step is given by the corre-\\nsponding probability of the event.\\nThe implementation works as follows: denote by p ∈ [0, 1]k the vector of\\nprobabilities and denote by f ∈ [0, 1]k with fi = fi−1 + pi and f1 = p1 the\\nsteps of the cumulative distribution function. Then for a random variable z\\ndrawn from U[0, 1] we obtain x = φ(z) := argmini {fi ≥ z}. See Figure 2.13\\nfor an example of a distribution over 5 events.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ea1751d3-acb1-4d34-b77f-582b8ca7feed', embedding=None, metadata={'page_label': '80', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='80 2 Density Estimation\\n0 2 4 6 8 100\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nExponential Distribution\\n0 2 4 6 8 100\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nCumulative Distribution Function\\nFig. 2.14. Left: Exponential distribution with λ = 1. Right: associated cumulative\\ndistribution function. When sampling, we draw x uniformly at random from U[0, 1]\\nand compute the inverse.\\nExample 2.9 (Exponential Distribution) The density of a Exponential-\\ndistributed random variable is given by\\np(x|λ) = λ exp(−λx) if λ > 0 and x ≥ 0. (2.78)\\nThis allows us to compute its cdf as\\nF (x|λ) = 1 − exp(−λx)if λ > 0 for x ≥ 0. (2.79)\\nTherefore to generate a Exponential random variable we draw z ∼ U[0, 1]\\nand solve x = φ(z) = F−1(z|λ) = −λ−1 log(1 − z). Since z and 1 − z are\\ndrawn from U[0, 1] we can simplify this to x = −λ−1 log z.\\nWe could apply the same reasoning to the normal distribution in order to\\ndraw Gaussian random variables. Unfortunately, the cumulative distribution\\nfunction of the Gaussian is not available in closed form and we would need\\nresort to rather nontrivial numerical techniques. It turns out that there exists\\na much more elegant algorithm which has its roots in Gauss’ proof of the\\nnormalization constant of the Normal distribution. This technique is known\\nas the Box-M¨ uller transform.\\nExample 2.10 (Box-M¨ uller Transform)Denote by X, Y independent Gaus-\\nsian random variables with zero mean and unit variance. We have\\np(x, y) = 1√\\n2π e− 1\\n2 x2 1√\\n2π e− 1\\n2 y2\\n= 1\\n2π e− 1\\n2 (x2+y2) (2.80)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e19562d0-bcbb-49ae-9aad-ecad1d8264e7', embedding=None, metadata={'page_label': '81', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 81\\n4\\n 3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\n 4\\n 5\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\nFig. 2.15. Red: true density of the standard normal distribution (red line) is con-\\ntrasted with the histogram of 20,000 random variables generated by the Box-M¨ uller\\ntransform.\\nThe key observation is that the joint distribution p(x, y) is radially symmet-\\nric, i.e. it only depends on the radius r2 = x2 + y2. Hence we may perform\\na variable substitution in polar coordinates via the map φ where\\nx = r cos θ and y = r sin θ hence (x, y) = φ−1(r, θ). (2.81)\\nThis allows us to express the density in terms of (r, θ) via\\np(r, θ) = p(φ−1(r, θ))\\n⏐⏐∇r,θφ−1(r, θ)\\n⏐⏐ = 1\\n2π e− 1\\n2 r2\\n⏐⏐⏐⏐\\n[ cos θ sin θ\\n−r sin θ r cos θ\\n]⏐⏐⏐⏐ = r\\n2π e− 1\\n2 r2\\n.\\nThe fact that p(r, θ) is constant in θ means that we can easily sample θ ∈\\n[0, 2π] by drawing a random variable, say zθ from U[0, 1] and rescaling it with\\n2π. To obtain a sampler for r we need to compute the cumulative distribution\\nfunction for p(r) = re− 1\\n2 r2\\n:\\nF (r′) =\\n∫ r′\\n0\\nre− 1\\n2 r2\\ndr = 1 − e− 1\\n2 r′2\\nand hence r = F−1(z) =\\n√\\n−2 log(1 − z).\\n(2.82)\\nObserving that z ∼ U[0, 1] implies that 1 − z ∼ U[0, 1] yields the following\\nsampler: draw zθ, zr ∼ U[0, 1] and compute x and y by\\nx =\\n√\\n−2 logzr cos 2πzθ and y =\\n√\\n−2 logzr sin 2πzθ.\\nNote that the Box-M¨ uller transform yields two independent Gaussian ran-\\ndom variables. See Figure 2.15 for an example of the sampler.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='335d4991-8a0f-4099-ba5d-82cb55238ee9', embedding=None, metadata={'page_label': '82', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='82 2 Density Estimation\\nExample 2.11 (Uniform distribution on the disc) A similar strategy\\ncan be employed when sampling from the unit disc. In this case the closed-\\nform expression of the distribution is simply given by\\np(x, y) =\\n{\\n1\\nπ if x2 + y2 ≤ 1\\n0 otherwise\\n(2.83)\\nUsing the variable transform (2.81) yields\\np(r, θ) = p(φ−1(r, θ))\\n⏐⏐∇r,θφ−1(r, θ)\\n⏐⏐ =\\n{\\nr\\nπ if r ≤ 1\\n0 otherwise\\n(2.84)\\nIntegrating out θ yields p(r) = 2 r for r ∈ [0, 1] with corresponding CDF\\nF (r) = r2 for r ∈ [0, 1]. Hence our sampler draws zr, zθ ∼ U[0, 1] and then\\ncomputes x = √zr cos 2πzθ and y = √zr sin 2πzθ.\\n2.5.2 Rejection Sampler\\nAll the methods for random variable generation that we looked at so far re-\\nquire intimate knowledge about the pdf of the distribution. We now describe\\na general purpose method, which can be used to generate samples from an\\narbitrary distribution. Let us begin with sampling from a set:\\nExample 2.12 (Rejection Sampler) Denote by X ⊆ X a set and let p be\\na density on X. Then a sampler for drawing from pX(x) ∝ p(x) for x ∈ X\\nand pX(x) = 0 for x ̸∈ X, that is, pX(x) = p(x|x ∈ X) is obtained by the\\nprocedure:\\nrepeat\\ndraw x ∼ p(x)\\nuntil x ∈ X\\nreturn x\\nThat is, the algorithm keeps on drawing from p until the random variable is\\ncontained in X. The probability that this occurs is clearly p(X). Hence the\\nlarger p(X) the higher the eﬃciency of the sampler. See Figure 2.16.\\nExample 2.13 (Uniform distribution on a disc) The procedure works\\ntrivially as follows: draw x, y ∼ U[0, 1]. Accept if (2x − 1)2 + (2y − 1)2 ≤ 1\\nand return sample (2x − 1, 2y − 1). This sampler has eﬃciency 4\\nπ since this\\nis the surface ratio between the unit square and the unit ball.\\nNote that this time we did not need to carry out any sophisticated measure', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c52f9dd1-666a-4f71-a514-fce5626027ba', embedding=None, metadata={'page_label': '83', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 83\\nFig. 2.16. Rejection sampler. Left: samples drawn from the uniform distribution on\\n[0, 1]2. Middle: the samples drawn from the uniform distribution on the unit disc\\nare all the points in the grey shaded area. Right: the same procedure allows us to\\nsample uniformly from arbitrary sets.\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n0.0\\n 0.2\\n 0.4\\n 0.6\\n 0.8\\n 1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nFig. 2.17. Accept reject sampling for the Beta(2 , 5) distribution. Left: Samples are\\ngenerated uniformly from the blue rectangle (shaded area). Only those samples\\nwhich fall under the red curve of the Beta(2 , 5) distribution (darkly shaded area)\\nare accepted. Right: The true density of the Beta(2 , 5) distribution (red line) is\\ncontrasted with the histogram of 10,000 samples drawn by the rejection sampler.\\ntransform. This mathematical convenience came at the expense of a slightly\\nless eﬃcient sampler — about 21% of all samples are rejected.\\nThe same reasoning that we used to obtain a hard accept/reject procedure\\ncan be used for a considerably more sophisticated rejection sampler. The\\nbasic idea is that if, for a given distributionp we can ﬁnd another distribution\\nq which, after rescaling, becomes an upper envelope on p, we can use q to\\nsample from and reject depending on the ratio between q and p.\\nTheorem 2.23 (Rejection Sampler) Denote by p and q distributions on\\nX and let c be a constant such that such that cq(x) ≥ p(x) for all x ∈ X.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d756bbc6-04ff-4a92-afdc-7daa953d1d4f', embedding=None, metadata={'page_label': '84', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='84 2 Density Estimation\\nThen the algorithm below draws from p with acceptance probability c−1.\\nrepeat\\ndraw x ∼ q(x) and t ∼ U[0, 1]\\nuntil ct ≤ p(x)\\nq(x)\\nreturn x\\nProof Denote by Z the event that the sample drawn from q is accepted.\\nThen by Bayes rule the probability Pr( x|Z) can be written as follows\\nPr(x|Z) = Pr(Z|x) Pr(x)\\nPr(Z) =\\np(x)\\ncq(x) · q(x)\\nc−1 = p(x) (2.85)\\nHere we used that Pr( Z) =\\n∫\\nPr(Z|x)q(x)dx =\\n∫\\nc−1p(x)dx = c−1.\\nNote that the algorithm of Example 2.12 is a special case of such a rejection\\nsampler — we majorize pX by the uniform distribution rescaled by 1\\np(X).\\nExample 2.14 (Beta distribution) Recall that the Beta(a, b) distribution,\\nas a member of the Exponential Family with suﬃcient statistics(log x, log(1−\\nx)), is given by\\np(x|a, b) = Γ(a + b)\\nΓ(a)Γ(b) xa−1(1 − x)b−1, (2.86)\\nFor given (a, b) one can verify (problem 2.25) that\\nM := argmax\\nx\\np(x|a, b) = a − 1\\na + b − 2 . (2.87)\\nprovided a > 1. Hence, if we use as proposal distribution the uniform distri-\\nbution U[0, 1] with scaling factor c = p(M |a, b) we may apply Theorem 2.23.\\nAs illustrated in Figure 2.17, to generate a sample from Beta(a, b) we ﬁrst\\ngenerate a pair (x, t), uniformly at random from the shaded rectangle. A\\nsample is retained if ct ≤ p(x|a, b), and rejected otherwise. The acceptance\\nrate of this sampler is 1\\nc .\\nExample 2.15 (Normal distribution) We may use the Laplace distri-\\nbution to generate samples from the Normal distribution. That is, we use\\nq(x|λ) = λ\\n2 e−λ|x| (2.88)\\nas the proposal distribution. For a normal distribution p = N(0, 1) with zero', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5334ed41-1dd4-4577-9611-35f966e3586e', embedding=None, metadata={'page_label': '85', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 85\\nmean and unit variance it turns out that choosing λ = 1 yields the most\\neﬃcient sampling scheme (see Problem 2.27) with\\np(x) ≤\\n√\\n2e\\nπ q(x|λ = 1)\\nAs illustrated in Figure 2.18, we ﬁrst generate x ∼ q(x|λ = 1) using the\\ninverse transform method (see Example 2.9 and Problem 2.21) and t ∼\\nU[0, 1]. If t ≤\\n√\\n2e/πp(x) we accept x, otherwise we reject it. The eﬃciency\\nof this scheme is √ π\\n2e.\\n−4 −2 0 2 40\\n0.2\\n0.4\\n0.6\\n√\\n2e\\nπ g(x|0, 1)\\np(x)\\nFig. 2.18. Rejection sampling for the Normal distribution (red curve). Samples are\\ngenerated uniformly from the Laplace distribution rescaled by\\n√\\n2e/π. Only those\\nsamples which fall under the red curve of the standard normal distribution (darkly\\nshaded area) are accepted.\\nWhile rejection sampling is fairly eﬃcient in low dimensions its eﬃciency is\\nunsatisfactory in high dimensions. This leads us to an instance of the curse of\\ndimensionality [Bel61]: the pdf of a d-dimensional Gaussian random variable\\ncentered at 0 with variance σ2 1 is given by\\np(x|σ2) = (2π)−d\\n2 σ−de− 1\\n2σ2∥x∥2\\nNow suppose that we want to draw from p(x|σ2) by sampling from another\\nGaussian q with slightly larger variance ρ2 > σ 2. In this case the ratio\\nbetween both distributions is maximized at 0 and it yields\\nc = q(0|σ2)\\np(0|ρ2) =\\n[ ρ\\nσ\\n]d', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='34b83a1b-97e0-4732-b801-1ec4b684237f', embedding=None, metadata={'page_label': '86', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='86 2 Density Estimation\\nIf suppose ρ\\nσ = 1.01, and d = 1000, we ﬁnd that c ≈ 20960. In other words,\\nwe need to generate approximately 21,000 samples on the average from q to\\ndraw a single sample from p. We will discuss a more sophisticated sampling\\nalgorithms, namely Gibbs Sampling, in Section ??. It allows us to draw from\\nrather nontrivial distributions as long as the distributions in small subsets\\nof random variables are simple enough to be tackled directly.\\nProblems\\nProblem 2.1 (Bias Variance Decomposition {1}) Prove that the vari-\\nance VarX[x] of a random variable can be written as EX[x2] − EX[x]2.\\nProblem 2.2 (Moment Generating Function {2}) Prove that the char-\\nacteristic function can be used to generate moments as given in (2.12). Hint:\\nuse the Taylor expansion of the exponential and apply the diﬀerential oper-\\nator before the expectation.\\nProblem 2.3 (Cumulative Error Function {2})\\nerf(x) =\\n√\\n2/π\\n∫ x\\n0\\ne−x2\\ndx. (2.89)\\nProblem 2.4 (Weak Law of Large Numbers {2}) In analogy to the proof\\nof the central limit theorem prove the weak law of large numbers. Hint: use\\na ﬁrst order Taylor expansion of eiωt = 1 +iωt + o(t) to compute an approx-\\nimation of the characteristic function. Next compute the limit m → ∞ for\\nφ ¯Xm. Finally, apply the inverse Fourier transform to associate the constant\\ndistribution at the mean µ with it.\\nProblem 2.5 (Rates and conﬁdence bounds {3}) Show that the rate\\nof hoeﬀding is tight — get bound from central limit theorem and compare to\\nthe hoeﬀding rate.\\nProblem 2.6 Why can’t we just use each chip on the wafer as a random\\nvariable? Give a counterexample. Give bounds if we actually were allowed to\\ndo this.\\nProblem 2.7 (Union Bound) Work on many bounds at the same time.\\nWe only have logarithmic penalty.\\nProblem 2.8 (Randomized Rounding {4}) Solve the linear system of\\nequations Ax = b for integral x.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a39d45c3-306c-4bc9-9ab3-9008e306092c', embedding=None, metadata={'page_label': '87', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 87\\nProblem 2.9 (Randomized Projections {3}) Prove that the random-\\nized projections converge.\\nProblem 2.10 (The Count-Min Sketch {5}) Prove the projection trick\\nProblem 2.11 (Parzen windows with triangle kernels {1}) Suppose\\nyou are given the following data: X = {2, 3, 3, 5, 5}. Plot the estimated den-\\nsity using a kernel density estimator with the following kernel:\\nk(u) =\\n{\\n0.5 − 0.25 ∗ |u| if |u| ≤ 2\\n0 otherwise.\\nProblem 2.12 Gaussian process link with Gaussian prior on natural pa-\\nrameters\\nProblem 2.13 Optimization for Gaussian regularization\\nProblem 2.14 Conjugate prior (student-t and wishart).\\nProblem 2.15 (Multivariate Gaussian {1}) Prove that Σ ≻ 0 is a nec-\\nessary and suﬃcient condition for the normal distribution to be well deﬁned.\\nProblem 2.16 (Discrete Exponential Distribution {2}) φ(x) = x and\\nuniform measure.\\nProblem 2.17 Exponential random graphs.\\nProblem 2.18 (Maximum Entropy Distribution) Show that exponen-\\ntial families arise as the solution of the maximum entropy estimation prob-\\nlem.\\nProblem 2.19 (Maximum Likelihood Estimates for Normal Distributions)\\nDerive the maximum likelihood estimates for a normal distribution, that is,\\nshow that they result in\\nˆµ = 1\\nm\\nm∑\\ni=1\\nxi and ˆσ2 = 1\\nm\\nm∑\\ni=1\\n(xi − ˆµ)2 (2.90)\\nusing the exponential families parametrization. Next show that while the\\nmean estimate ˆµ is unbiased, the variance estimate has a slight bias of O( 1\\nm).\\nTo see this, take the expectation with respect to ˆσ2.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='740cf6f1-256e-4fa4-a8e1-2e6d4d92b4f7', embedding=None, metadata={'page_label': '88', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='88 2 Density Estimation\\nProblem 2.20 (cdf of Logistic random variable {1}) Show that the cdf\\nof the Logistic random variable (??) is given by (??).\\nProblem 2.21 (Double-exponential (Laplace) distribution {1}) Use\\nthe inverse-transform method to generate a sample from the double-exponential\\n(Laplace) distribution (2.88).\\nProblem 2.22 (Normal random variables in polar coordinates {1})\\nIf X1 and X2 are standard normal random variables and let (R, θ) de-\\nnote the polar coordinates of the pair (X1, X2). Show that R2 ∼ χ2\\n2 and\\nθ ∼ Unif[0, 2π].\\nProblem 2.23 (Monotonically increasing mappings {1}) A mapping\\nT : R → R is one-to-one if, and only if, T is monotonically increasing, that\\nis, x > y implies that T (x) > T (y).\\nProblem 2.24 (Monotonically increasing multi-maps {2}) Let T : Rn →\\nRn be one-to-one. If X ∼ pX(x), then show that the distribution pY (y) of\\nY = T (X) can be obtained via (??).\\nProblem 2.25 (Argmax of the Beta(a, b) distribution {1}) Show that\\nthe mode of the Beta(a, b) distribution is given by (2.87).\\nProblem 2.26 (Accept reject sampling for the unit disk {2}) Give at\\nleast TWO diﬀerent accept-reject based sampling schemes to generate sam-\\nples uniformly at random from the unit disk. Compute their eﬃciency.\\nProblem 2.27 (Optimizing Laplace for Standard Normal {1}) Optimize\\nthe ratio p(x)/g(x|µ, σ), with respect to µ and σ, where p(x) is the standard\\nnormal distribution (??), and g(x|µ, σ) is the Laplace distribution (2.88).\\nProblem 2.28 (Normal Random Variable Generation {2}) The aim\\nof this problem is to write code to generate standard normal random vari-\\nables (??) by using diﬀerent methods. To do this generate U ∼ Unif[0, 1]\\nand apply\\n(i) the Box-Muller transformation outlined in Section ??.\\n(ii) use the following approximation to the inverse CDF\\nΦ−1(α) ≈ t − a0 + a1t\\n1 + b1t + b2t2 , (2.91)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d5aad9e6-00f1-4f29-bfdf-3d910cd3b011', embedding=None, metadata={'page_label': '89', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2.5 Sampling 89\\nwhere t2 = log(α−2) and\\na0 = 2.30753, a1 = 0.27061, b1 = 0.99229, b2 = 0.04481\\n(iii) use the method outlined in example 2.15.\\nPlot a histogram of the samples you generated to conﬁrm that they are nor-\\nmally distributed. Compare these diﬀerent methods in terms of the time\\nneeded to generate 1000 random variables.\\nProblem 2.29 (Non-standard Normal random variables {2}) Describe\\na scheme based on the Box-Muller transform to generate d dimensional nor-\\nmal random variables p(x|0, I). How can this be used to generate arbitrary\\nnormal random variables p(x|µ, Σ).\\nProblem 2.30 (Uniform samples from a disk {2}) Show how the ideas\\ndescribed in Section ?? can be generalized to draw samples uniformly at ran-\\ndom from an axis parallel ellipse: {(x, y) : x2\\n1\\na2 + x2\\n2\\nb2 ≤ 1}.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8db3993c-c183-4c36-ae39-3a7cb510ced3', embedding=None, metadata={'page_label': '90', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f2d0df7f-9839-43e3-982a-8f081092c67e', embedding=None, metadata={'page_label': '91', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3\\nOptimization\\nOptimization plays an increasingly important role in machine learning. For\\ninstance, many machine learning algorithms minimize a regularized risk\\nfunctional:\\nmin\\nf\\nJ(f) := λΩ(f) + Remp(f), (3.1)\\nwith the empirical risk\\nRemp(f) := 1\\nm\\nm∑\\ni=1\\nl(f(xi), yi). (3.2)\\nHere xi are the training instances and yi are the corresponding labels. l the\\nloss function measures the discrepancy between y and the predictions f(xi).\\nFinding the optimal f involves solving an optimization problem.\\nThis chapter provides a self-contained overview of some basic concepts and\\ntools from optimization, especially geared towards solving machine learning\\nproblems. In terms of concepts, we will cover topics related to convexity,\\nduality, and Lagrange multipliers. In terms of tools, we will cover a variety\\nof optimization algorithms including gradient descent, stochastic gradient\\ndescent, Newton’s method, and Quasi-Newton methods. We will also look\\nat some specialized algorithms tailored towards solving Linear Programming\\nand Quadratic Programming problems which often arise in machine learning\\nproblems.\\n3.1 Preliminaries\\nMinimizing an arbitrary function is, in general, very diﬃcult, but if the ob-\\njective function to be minimized is convex then things become considerably\\nsimpler. As we will see shortly, the key advantage of dealing with convex\\nfunctions is that a local optima is also the global optima. Therefore, well\\ndeveloped tools exist to ﬁnd the global minima of a convex function. Conse-\\nquently, many machine learning algorithms are now formulated in terms of\\nconvex optimization problems. We brieﬂy review the concept of convex sets\\nand functions in this section.\\n91', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='11f40965-f749-4d50-80c7-d541461a73c0', embedding=None, metadata={'page_label': '92', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='92 3 Optimization\\n3.1.1 Convex Sets\\nDeﬁnition 3.1 (Convex Set) A subset C of Rn is said to be convex if\\n(1 − λ)x + λy ∈ C whenever x ∈ C, y ∈ C and 0 < λ < 1.\\nIntuitively, what this means is that the line joining any two points x and y\\nfrom the set C lies inside C (see Figure 3.1). It is easy to see (Exercise 3.1)\\nthat intersections of convex sets are also convex.\\nFig. 3.1. The convex set (left) contains the line joining any two points that belong\\nto the set. A non-convex set (right) does not satisfy this property.\\nA vector sum ∑\\ni λixi is called a convex combination ifλi ≥ 0 and ∑\\ni λi =\\n1. Convex combinations are helpful in deﬁning a convex hull:\\nDeﬁnition 3.2 (Convex Hull) The convex hull, conv(X), of a ﬁnite sub-\\nset X = {x1, . . . , xn} of Rn consists of all convex combinations of x1, . . . , xn.\\n3.1.2 Convex Functions\\nLet f be a real valued function deﬁned on a set X ⊂ Rn. The set\\n{(x, µ) : x ∈ X, µ ∈ R, µ ≥ f(x)} (3.3)\\nis called the epigraph of f. The function f is deﬁned to be a convex function\\nif its epigraph is a convex set in Rn+1. An equivalent, and more commonly\\nused, deﬁnition (Exercise 3.5) is as follows (see Figure 3.2 for geometric\\nintuition):\\nDeﬁnition 3.3 (Convex Function) A function f deﬁned on a set X is\\ncalled convex if, for any x, x′ ∈ X and any 0 < λ < 1 such that λx + (1 −\\nλ)x′ ∈ X, we have\\nf(λx + (1 − λ)x′) ≤ λf(x) + (1 − λ)f(x′). (3.4)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a517152e-6e56-4bcd-8f8c-2250662fac88', embedding=None, metadata={'page_label': '93', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Preliminaries 93\\nA function f is called strictly convex if\\nf(λx + (1 − λ)x′) < λf (x) + (1 − λ)f(x′) (3.5)\\nwhenever x ̸= x′.\\nIn fact, the above deﬁnition can be extended to show that if f is a convex\\nfunction and λi ≥ 0 with ∑\\ni λi = 1 then\\nf\\n(∑\\ni\\nλixi\\n)\\n≤\\n∑\\ni\\nλif(xi). (3.6)\\nThe above inequality is called the Jensen’s inequality (problem ).\\n6\\n 4\\n 2\\n 0 2 4 6\\nx\\n0\\n200\\n400\\n600\\n800\\n1000f(x)\\n3\\n 2\\n 1\\n 0 1 2 3\\nx\\n1.5\\n1.0\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nf(x)\\nFig. 3.2. A convex function (left) satisﬁes (3.4); the shaded region denotes its epi-\\ngraph. A nonconvex function (right) does not satisfy (3.4).\\nIf f : X → R is diﬀerentiable, then f is convex if, and only if,\\nf(x′) ≥ f(x) +\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\nfor all x, x′ ∈ X. (3.7)\\nIn other words, the ﬁrst order Taylor approximation lower bounds the convex\\nfunction universally (see Figure 3.4). Here and in the rest of the chapter\\n⟨x, y⟩ denotes the Euclidean dot product between vectors x and y, that is,\\n⟨x, y⟩ :=\\n∑\\ni\\nxiyi. (3.8)\\nIf f is twice diﬀerentiable, then f is convex if, and only if, its Hessian is\\npositive semi-deﬁnite, that is,\\n∇2f(x) ⪰ 0. (3.9)\\nFor twice diﬀerentiable strictly convex functions, the Hessian matrix is pos-\\nitive deﬁnite, that is, ∇2f(x) ≻ 0. We brieﬂy summarize some operations\\nwhich preserve convexity:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='782d2346-99b9-42b4-b77f-39eee54eafe4', embedding=None, metadata={'page_label': '94', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='94 3 Optimization\\nAddition If f1 and f2 are convex, then f1 + f2 is also convex.\\nScaling If f is convex, then αf is convex for α > 0.\\nAﬃne Transform If f is convex, then g(x) = f(Ax + b) for some matrix\\nA and vector b is also convex.\\nAdding a Linear Function If f is convex, then g(x) = f(x)+⟨a, x⟩ for some vector\\na is also convex.\\nSubtracting a Linear Function If f is convex, then g(x) = f(x)−⟨a, x⟩ for some vector\\na is also convex.\\nPointwise Maximum If fi are convex, then g(x) = maxi fi(x) is also convex.\\nScalar Composition If f(x) = h(g(x)), then f is convex if a) g is convex,\\nand h is convex, non-decreasing or b) g is concave, and\\nh is convex, non-increasing.\\n-3 -2 -1  0  1  2  3-3\\n-2\\n-1\\n 0\\n 1\\n 2\\n 3\\n 0\\n 2\\n 4\\n 6\\n 8\\n 10\\n 12\\n 14\\n 16\\n 18\\n-3-2-1 0 1 2 3\\n-3\\n-2\\n-1\\n 0\\n 1\\n 2\\n 3\\nFig. 3.3. Left: Convex Function in two variables. Right: the corresponding convex\\nbelow-sets {x|f(x) ≤ c}, for diﬀerent values of c. This is also called a contour plot.\\nThere is an intimate relation between convex functions and convex sets.\\nFor instance, the following lemma show that the below sets (level sets) of\\nconvex functions, sets for which f(x) ≤ c, are convex.\\nLemma 3.4 (Below-Sets of Convex Functions) Denote by f : X → R\\na convex function. Then the set\\nXc := {x | x ∈ X and f(x) ≤ c}, for all c ∈ R, (3.10)\\nis convex.\\nProof For any x, x′ ∈ Xc, we have f(x), f(x′) ≤ c. Moreover, since f is\\nconvex, we also have\\nf(λx + (1 − λ)x′) ≤ λf(x) + (1 − λ)f(x′) ≤ c for all 0 < λ < 1. (3.11)\\nHence, for all 0 < λ < 1, we have ( λx + (1 − λ)x′) ∈ Xc, which proves the\\nclaim. Figure 3.3 depicts this situation graphically.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='742bd595-13d2-4515-859b-8ae03950fa34', embedding=None, metadata={'page_label': '95', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Preliminaries 95\\nAs we hinted in the introduction of this chapter, minimizing an arbitrary\\nfunction on a (possibly not even compact) set of arguments can be a diﬃcult\\ntask, and will most likely exhibit many local minima. In contrast, minimiza-\\ntion of a convex objective function on a convex set exhibits exactly oneglobal\\nminimum. We now prove this property.\\nTheorem 3.5 (Minima on Convex Sets) If the convex function f : X →\\nR attains its minimum, then the set of x ∈ X, for which the minimum value\\nis attained, is a convex set. Moreover, if f is strictly convex, then this set\\ncontains a single element.\\nProof Denote by c the minimum of f on X. Then the set Xc := {x|x ∈\\nX and f(x) ≤ c} is clearly convex.\\nIf f is strictly convex, then for any two distinct x, x′ ∈ Xc and any 0 <\\nλ < 1 we have\\nf(λx + (1 − λ)x′) < λf (x) + (1 − λ)f(x′) = λc + (1 − λ)c = c,\\nwhich contradicts the assumption that f attains its minimum on Xc. There-\\nfore Xc must contain only a single element.\\nAs the following lemma shows, the minimum point can be characterized\\nprecisely.\\nLemma 3.6 Let f : X → R be a diﬀerentiable convex function. Then x is\\na minimizer of f, if, and only if,\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\n≥ 0 for all x′. (3.12)\\nProof To show the forward implication, suppose that x is the optimum\\nbut (3.12) does not hold, that is, there exists an x′ for which\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\n< 0.\\nConsider the line segment z(λ) = (1 − λ)x + λx′, with 0 < λ < 1. Since X\\nis convex, z(λ) lies in X. On the other hand,\\nd\\ndλ f(z(λ))\\n⏐⏐⏐⏐\\nλ=0\\n=\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\n< 0,\\nwhich shows that for small values of λ we have f(z(λ)) < f (x), thus showing\\nthat x is not optimal.\\nThe reverse implication follows from (3.7) by noting that f(x′) ≥ f(x),\\nwhenever (3.12) holds.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dcd48045-c350-4e95-ab89-f60b6bb9bce0', embedding=None, metadata={'page_label': '96', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='96 3 Optimization\\nOne way to ensure that (3.12) holds is to set ∇f(x) = 0. In other words,\\nminimizing a convex function is equivalent to ﬁnding a x such that ∇f(x) =\\n0. Therefore, the ﬁrst order conditions are both necessary and suﬃcient\\nwhen minimizing a convex function.\\n3.1.3 Subgradients\\nSo far, we worked with diﬀerentiable convex functions. The subgradient is a\\ngeneralization of gradients appropriate for convex functions, including those\\nwhich are not necessarily smooth.\\nDeﬁnition 3.7 (Subgradient) Suppose x is a point where a convex func-\\ntion f is ﬁnite. Then a subgradient is the normal vector of any tangential\\nsupporting hyperplane of f at x. Formally µ is called a subgradient of f at\\nx if, and only if,\\nf(x′) ≥ f(x) +\\n⟨\\nx′ − x, µ\\n⟩\\nfor all x′. (3.13)\\nThe set of all subgradients at a point is called the subdiﬀerential, and is de-\\nnoted by ∂xf(x). If this set is not empty then f is said to be subdiﬀerentiable\\nat x. On the other hand, if this set is a singleton then, the function is said\\nto be diﬀerentiable at x. In this case we use ∇f(x) to denote the gradient\\nof f. Convex functions are subdiﬀerentiable everywhere in their domain. We\\nnow state some simple rules of subgradient calculus:\\nAddition ∂x(f1(x) + f2(x)) = ∂xf1(x) + ∂xf2(x)\\nScaling ∂xαf(x) = α∂xf(x), for α > 0\\nAﬃne Transform If g(x) = f(Ax + b) for some matrix A and vector b,\\nthen ∂xg(x) = A⊤∂yf(y).\\nPointwise Maximum If g(x) = maxi fi(x) then ∂g(x) = conv( ∂xfi′) where\\ni′ ∈ argmaxi fi(x).\\nThe deﬁnition of a subgradient can also be understood geometrically. As\\nillustrated by Figure 3.4, a diﬀerentiable convex function is always lower\\nbounded by its ﬁrst order Taylor approximation. This concept can be ex-\\ntended to non-smooth functions via subgradients, as Figure 3.5 shows.\\nBy using more involved concepts, the proof of Lemma 3.6 can be extended\\nto subgradients. In this case, minimizing a convex nonsmooth function en-\\ntails ﬁnding a x such that 0 ∈ ∂f (x).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='90e09d0d-8746-49de-929a-97da9a78d264', embedding=None, metadata={'page_label': '97', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Preliminaries 97\\n3.1.4 Strongly Convex Functions\\nWhen analyzing optimization algorithms, it is sometimes easier to work with\\nstrongly convex functions, which generalize the deﬁnition of convexity.\\nDeﬁnition 3.8 (Strongly Convex Function) A convex function f is σ-\\nstrongly convex if, and only if, there exists a constant σ > 0 such that the\\nfunction f(x) − σ\\n2 ∥x∥2 is convex.\\nThe constant σ is called the modulus of strong convexity of f. If f is twice\\ndiﬀerentiable, then there is an equivalent, and perhaps easier, deﬁnition of\\nstrong convexity: f is strongly convex if there exists a σ such that\\n∇2f(x) ⪰ σI. (3.14)\\nIn other words, the smallest eigenvalue of the Hessian of f is uniformly\\nlower bounded by σ everywhere. Some important examples of strongly con-\\nvex functions include:\\nExample 3.1 (Squared Euclidean Norm) The function f(x) = λ\\n2 ∥x∥2\\nis λ-strongly convex.\\nExample 3.2 (Negative Entropy) Let ∆n = {x s.t. ∑\\ni xi = 1 and xi ≥ 0}\\nbe the n dimensional simplex, and f : ∆n → R be the negative entropy:\\nf(x) =\\n∑\\ni\\nxi log xi. (3.15)\\nThen f is 1-strongly convex with respect to the ∥·∥1 norm on the simplex\\n(see Problem 3.7).\\nIf f is a σ-strongly convex function then one can show the following prop-\\nerties (Exercise 3.8). Here x, x′ are arbitrary and µ ∈ ∂f (x) and µ′ ∈ ∂f (x′).\\nf(x′) ≥ f(x) +\\n⟨\\nx′ − x, µ\\n⟩\\n+ σ\\n2\\n\\ued79\\ued79x′ − x\\n\\ued79\\ued792 (3.16)\\nf(x′) ≤ f(x) +\\n⟨\\nx′ − x, µ\\n⟩\\n+ 1\\n2σ\\n\\ued79\\ued79µ′ − µ\\n\\ued79\\ued792 (3.17)\\n⟨\\nx − x′, µ − µ′⟩\\n≥ σ\\n\\ued79\\ued79x − x′\\ued79\\ued792 (3.18)\\n⟨\\nx − x′, µ − µ′⟩\\n≤ 1\\nσ\\n\\ued79\\ued79µ − µ′\\ued79\\ued792 . (3.19)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a2b471b2-b54d-4fe0-8eda-b76acdc091cd', embedding=None, metadata={'page_label': '98', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='98 3 Optimization\\n3.1.5 Convex Functions with Lipschitz Continous Gradient\\nA somewhat symmetric concept to strong convexity is the Lipschitz conti-\\nnuity of the gradient. As we will see later they are connected by Fenchel\\nduality.\\nDeﬁnition 3.9 (Lipschitz Continuous Gradient) A diﬀerentiable con-\\nvex function f is said to have a Lipschitz continuous gradient, if there exists\\na constant L > 0, such that\\n\\ued79\\ued79∇f(x) − ∇f(x′)\\n\\ued79\\ued79 ≤ L\\n\\ued79\\ued79x − x′\\ued79\\ued79 ∀x, x′. (3.20)\\nAs before, if f is twice diﬀerentiable, then there is an equivalent, and perhaps\\neasier, deﬁnition of Lipschitz continuity of the gradient: f has a Lipschitz\\ncontinuous gradient strongly convex if there exists a L such that\\nLI ⪰ ∇2f(x). (3.21)\\nIn other words, the largest eigenvalue of the Hessian of f is uniformly upper\\nbounded by L everywhere. If f has a Lipschitz continuous gradient with\\nmodulus L, then one can show the following properties (Exercise 3.9).\\nf(x′) ≤ f(x) +\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\n+ L\\n2\\n\\ued79\\ued79x − x′\\ued79\\ued792 (3.22)\\nf(x′) ≥ f(x) +\\n⟨\\nx′ − x, ∇f(x)\\n⟩\\n+ 1\\n2L\\n\\ued79\\ued79∇f(x) − ∇f(x′)\\n\\ued79\\ued792 (3.23)\\n⟨\\nx − x′, ∇f(x) − ∇f(x′)\\n⟩\\n≤ L\\n\\ued79\\ued79x − x′\\ued79\\ued792 (3.24)\\n⟨\\nx − x′, ∇f(x) − ∇f(x′)\\n⟩\\n≥ 1\\nL\\n\\ued79\\ued79∇f(x) − ∇f(x′)\\n\\ued79\\ued792 . (3.25)\\n3.1.6 Fenchel Duality\\nThe Fenchel conjugate of a function f is given by\\nf∗(x∗) = sup\\nx\\n{⟨x, x∗⟩ − f(x)} . (3.26)\\nEven if f is not convex, the Fechel conjugate which is written as a supremum\\nover linear functions is always convex. Some rules for computing Fenchel\\nduals are summarized in Table 3.1.6. If f is convex and its epigraph (3.3) is\\na closed convex set, then f∗∗ = f. If f and f∗ are convex, then they satisfy\\nthe so-called Fenchel-Young inequality\\nf(x) + f∗(x∗) ≥ ⟨x, x∗⟩ for all x, x∗. (3.27)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='304b5974-eeeb-49bb-8d14-29961d5c20e8', embedding=None, metadata={'page_label': '99', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Preliminaries 99\\nFig. 3.4. A convex function is always lower bounded by its ﬁrst order Taylor ap-\\nproximation. This is true even if the function is not diﬀerentiable (see Figure 3.5)\\n4\\n 3\\n 2\\n 1\\n 0\\n 1\\n 2\\n 3\\n 4\\n1\\n0\\n1\\n2\\n3\\n4\\n5\\nFig. 3.5. Geometric intuition of a subgradient. The nonsmooth convex function\\n(solid blue) is only subdiﬀerentiable at the “kink” points. We illustrate two of its\\nsubgradients (dashed green and red lines) at a “kink” point which are tangential to\\nthe function. The normal vectors to these lines are subgradients. Observe that the\\nﬁrst order Taylor approximations obtained by using the subgradients lower bounds\\nthe convex function.\\nThis inequality becomes an equality whenever x∗ ∈ ∂f (x), that is,\\nf(x) + f∗(x∗) = ⟨x, x∗⟩ for all x and x∗ ∈ ∂f (x). (3.28)\\nStrong convexity (Section 3.1.4) and Lipschitz continuity of the gradient', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2a7576e3-9b65-4ac6-9a1d-6b9d315d81d7', embedding=None, metadata={'page_label': '100', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='100 3 Optimization\\nTable 3.1. Rules for computing Fenchel Duals\\nScalar Addition If g(x) = f(x) + α then g∗(x∗) = f∗(x∗) − α.\\nFunction Scaling If α > 0 and g(x) = αf(x) then g∗(x∗) = αf∗(x∗/α).\\nParameter Scaling If α ̸= 0 and g(x) = f(αx) then g∗(x∗) = f∗(x∗/α)\\nLinear Transformation If A is an invertible matrix then (f ◦A)∗ = f∗ ◦(A−1)∗.\\nShift If g(x) = f(x − x0) then g∗(x∗) = f∗(x∗) + ⟨x∗, x0⟩.\\nSum If g(x) = f1(x) + f2(x) then g∗(x∗) =\\ninf {f∗\\n1 (x∗\\n1) + f∗\\n2 (x∗\\n2) s.t. x∗\\n1 + x∗\\n2 = x∗}.\\nPointwise Inﬁmum If g(x) = inf fi(x) then g∗(x∗) = supi f∗\\ni (x∗).\\n(Section 3.1.5) are related by Fenchel duality according to the following\\nlemma, which we state without proof.\\nLemma 3.10 (Theorem 4.2.1 and 4.2.2 [HUL93])\\n(i) If f is σ-strongly convex, then f∗ has a Lipschitz continuous gradient\\nwith modulus 1\\nσ .\\n(ii) If f is convex and has a Lipschitz continuous gradient with modulus\\nL, then f∗ is 1\\nL-strongly convex.\\nNext we describe some convex functions and their Fenchel conjugates.\\nExample 3.3 (Squared Euclidean Norm) Whenever f(x) = 1\\n2 ∥x∥2 we\\nhave f∗(x∗) = 1\\n2 ∥x∗∥2, that is, the squared Euclidean norm is its own con-\\njugate.\\nExample 3.4 (Negative Entropy) The Fenchel conjugate of the negative\\nentropy (3.15) is\\nf∗(x∗) = log\\n∑\\ni\\nexp(x∗\\ni ).\\n3.1.7 Bregman Divergence\\nLet f be a diﬀerentiable convex function. The Bregman divergence deﬁned\\nby f is given by\\n∆f(x, x′) = f(x) − f(x′) −\\n⟨\\nx − x′, ∇f(x′)\\n⟩\\n. (3.29)\\nAlso see Figure 3.6. Here are some well known examples.\\nExample 3.5 (Square Euclidean Norm) Set f(x) = 1\\n2 ∥x∥2. Clearly,\\n∇f(x) = x and therefore\\n∆f(x, x′) = 1\\n2 ∥x∥2 − 1\\n2\\n\\ued79\\ued79x′\\ued79\\ued792 −\\n⟨\\nx − x′, x′⟩\\n= 1\\n2\\n\\ued79\\ued79x − x′\\ued79\\ued792 .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c908a7f5-0c32-4156-90ae-967e315dafa8', embedding=None, metadata={'page_label': '101', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.1 Preliminaries 101\\nf(x′ )\\nf(x)\\nf(x′ ) +\\n⟨\\nx−x′ ,∇f(x′ )\\n⟩\\n∆f(x,x′ )\\nFig. 3.6. f(x) is the value of the function atx, while f(x′)+⟨x − x′, ∇f(x′)⟩ denotes\\nthe ﬁrst order Taylor expansion of f around x′, evaluated at x. The diﬀerence\\nbetween these two quantities is the Bregman divergence, as illustrated.\\nExample 3.6 (Relative Entropy) Let f be the un-normalized entropy\\nf(x) =\\n∑\\ni\\n(xi log xi − xi) . (3.30)\\nOne can calculate ∇f(x) = log x, where log x is the component wise loga-\\nrithm of the entries of x, and write the Bregman divergence\\n∆f(x, x′) =\\n∑\\ni\\nxi log xi −\\n∑\\ni\\nxi −\\n∑\\ni\\nx′\\ni log x′\\ni +\\n∑\\ni\\nx′\\ni −\\n⟨\\nx − x′, log x′⟩\\n=\\n∑\\ni\\n(\\nxi log\\n(xi\\nx′\\ni\\n)\\n+ x′\\ni − xi\\n)\\n.\\nExample 3.7 ( p-norm) Let f be the square p-norm\\nf(x) = 1\\n2 ∥x∥2\\np = 1\\n2\\n(∑\\ni\\nxp\\ni\\n)2/p\\n. (3.31)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1053fcf7-14d8-4554-bceb-35de8c9e5092', embedding=None, metadata={'page_label': '102', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='102 3 Optimization\\nWe say that the q-norm is dual to the p-norm whenever 1\\np + 1\\nq = 1. One can\\nverify (Problem 3.12) that the i-th component of the gradient ∇f(x) is\\n∇xif(x) = sign(xi) |xi|p−1\\n∥x∥p−2\\np\\n. (3.32)\\nThe corresponding Bregman divergence is\\n∆f(x, x′) = 1\\n2 ∥x∥2\\np − 1\\n2\\n\\ued79\\ued79x′\\ued79\\ued792\\np −\\n∑\\ni\\n(xi − x′\\ni)sign(x′\\ni) |x′\\ni|p−1\\n∥x′∥p−2\\np\\n.\\nThe following properties of the Bregman divergence immediately follow:\\n• ∆f(x, x′) is convex in x.\\n• ∆f(x, x′) ≥ 0.\\n• ∆f may not be symmetric, that is, in general ∆ f(x, x′) ̸= ∆f(x′, x).\\n• ∇ x∆f(x, x′) = ∇f(x) − ∇f(x′).\\nThe next lemma establishes another important property.\\nLemma 3.11 The Bregman divergence (3.29) deﬁned by a diﬀerentiable\\nconvex function f satisﬁes\\n∆f(x, y) + ∆f(y, z) − ∆f(x, z) = ⟨∇f(z) − ∇f(y), x − y⟩ . (3.33)\\nProof\\n∆f(x, y) + ∆f(y, z) = f(x) − f(y) − ⟨x − y, ∇f(y)⟩ + f(y) − f(z) − ⟨y − z, ∇f(z)⟩\\n= f(x) − f(z) − ⟨x − y, ∇f(y)⟩ − ⟨y − z, ∇f(z)⟩\\n= ∆f(x, z) + ⟨∇f(z) − ∇f(y), x − y⟩ .\\n3.2 Unconstrained Smooth Convex Minimization\\nIn this section we will describe various methods to minimize a smooth convex\\nobjective function.\\n3.2.1 Minimizing a One-Dimensional Convex Function\\nAs a warm up let us consider the problem of minimizing a smooth one di-\\nmensional convex function J : R → R in the interval [ L, U]. This seemingly', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7bd1d108-4980-4d37-bd76-f2004d4416cd', embedding=None, metadata={'page_label': '103', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 103\\nAlgorithm 3.1 Interval Bisection\\n1: Input: L, U, precision ϵ\\n2: Set t = 0, a0 = L and b0 = U\\n3: while (bt − at) · J′(U) > ϵ do\\n4: if J′( at+bt\\n2 ) > 0 then\\n5: at+1 = at and bt+1 = at+bt\\n2\\n6: else\\n7: at+1 = at+bt\\n2 and bt+1 = bt\\n8: end if\\n9: t = t + 1\\n10: end while\\n11: Return: at+bt\\n2\\nsimple problem has many applications. As we will see later, many optimiza-\\ntion methods ﬁnd a direction of descent and minimize the objective function\\nalong this direction 1; this subroutine is called a line search. Algorithm 3.1\\ndepicts a simple line search routine based on interval bisection.\\nBefore we show that Algorithm 3.1 converges, let us ﬁrst derive an im-\\nportant property of convex functions of one variable. For a diﬀerentiable\\none-dimensional convex function J (3.7) reduces to\\nJ(w) ≥ J(w′) + (w − w′) · J′(w′), (3.34)\\nwhere J′(w) denotes the gradient of J. Exchanging the role of w and w′ in\\n(3.34), we can write\\nJ(w′) ≥ J(w) + (w′ − w) · J′(w). (3.35)\\nAdding the above two equations yields\\n(w − w′) · (J′(w) − J′(w′)) ≥ 0. (3.36)\\nIf w ≥ w′, then this implies that J′(w) ≥ J′(w′). In other words, the gradient\\nof a one dimensional convex function is monotonically non-decreasing.\\nRecall that minimizing a convex function is equivalent to ﬁnding w∗ such\\nthat J′(w∗) = 0. Furthermore, it is easy to see that the interval bisection\\nmaintains the invariant J′(at) < 0 and J′(bt) > 0. This along with the\\nmonotonicity of the gradient suﬃces to ensure that w∗ ∈ (at, bt). Setting\\nw = w∗ in (3.34), and using the monotonicity of the gradient allows us to\\n1 If the objective function is convex, then the one dimensional function obtained by restricting\\nit along the search direction is also convex (Exercise 3.10).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='28c0290c-e4d9-4f95-b9cb-d71d80c77678', embedding=None, metadata={'page_label': '104', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='104 3 Optimization\\nwrite for any w′ ∈ (at, bt)\\nJ(w′) − J(w∗) ≤ (w′ − w∗) · J′(w′) ≤ (bt − at) · J′(U). (3.37)\\nSince we halve the interval (at, bt) at every iteration, it follows that (bt−at) =\\n(U − L)/2t. Therefore\\nJ(w′) − J(w∗) ≤ (U − L) · J′(U)\\n2t , (3.38)\\nfor all w′ ∈ (at, bt). In other words, to ﬁnd an ϵ-accurate solution, that is,\\nJ(w′) − J(w∗) ≤ ϵ we only need log(U − L) + logJ′(U) + log(1/ϵ) < t itera-\\ntions. An algorithm which converges to an ϵ accurate solution in O(log(1/ϵ))\\niterations is said to be linearly convergent.\\nFor multi-dimensional objective functions, one cannot rely on the mono-\\ntonicity property of the gradient. Therefore, one needs more sophisticated\\noptimization algorithms, some of which we now describe.\\n3.2.2 Coordinate Descent\\nCoordinate descent is conceptually the simplest algorithm for minimizing a\\nmultidimensional smooth convex function J : Rn → R. At every iteration\\nselect a coordinate, say i, and update\\nwt+1 = wt − ηtei. (3.39)\\nHere ei denotes the i-th basis vector, that is, a vector with one at thei-th co-\\nordinate and zeros everywhere else, whileηt ∈ R is a non-negative scalar step\\nsize. One could, for instance, minimize the one dimensional convex function\\nJ(wt − ηei) to obtain the stepsize ηt. The coordinates can either be selected\\ncyclically, that is, 1, 2, . . . , n,1, 2, . . .or greedily, that is, the coordinate which\\nyields the maximum reduction in function value.\\nEven though coordinate descent can be shown to converge if J has a Lip-\\nschitz continuous gradient [LT92], in practice it can be quite slow. However,\\nif a high precision solution is not required, as is the case in some machine\\nlearning applications, coordinate descent is often used because a) the cost\\nper iteration is very low and b) the speed of convergence may be acceptable\\nespecially if the variables are loosely coupled.\\n3.2.3 Gradient Descent\\nGradient descent (also widely known as steepest descent) is an optimization\\ntechnique for minimizing multidimensional smooth convex objective func-\\ntions of the form J : Rn → R. The basic idea is as follows: Given a location', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b88dc0ff-b2dd-4eb5-a404-4d094a6de09a', embedding=None, metadata={'page_label': '105', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 105\\nwt at iteration t, compute the gradient ∇J(wt), and update\\nwt+1 = wt − ηt∇J(wt), (3.40)\\nwhere ηt is a scalar stepsize. See Algorithm 3.2 for details. Diﬀerent variants\\nof gradient descent depend on how ηt is chosen:\\nExact Line Search: Since J(wt − η∇J(wt)) is a one dimensional convex\\nfunction in η, one can use the Algorithm 3.1 to compute:\\nηt = argmin\\nη\\nJ(wt − η∇J(wt)). (3.41)\\nInstead of the simple bisecting line search more sophisticated line searches\\nsuch as the More-Thuente line search or the golden bisection rule can also\\nbe used to speed up convergence (see [NW99] Chapter 3 for an extensive\\ndiscussion).\\nInexact Line Search: Instead of minimizing J(wt − η∇J(wt)) we could\\nsimply look for a stepsize which results in suﬃcient decrease in the objective\\nfunction value. One popular set of suﬃcient decrease conditions is the Wolfe\\nconditions\\nJ(wt+1) ≤ J(wt) + c1ηt ⟨∇J(wt), wt+1 − wt⟩ (suﬃcient decrease) (3.42)\\n⟨∇J(wt+1), wt+1 − wt⟩ ≥ c2 ⟨∇J(wt), wt+1 − wt⟩ (curvature) (3.43)\\nwith 0 < c1 < c2 < 1 (see Figure 3.7). The Wolfe conditions are also called\\nthe Armijio-Goldstein conditions. If only suﬃcient decrease (3.42) alone is\\nenforced, then it is called the Armijio rule.\\nacceptable stepsize\\nacceptable stepsize\\nFig. 3.7. The suﬃcient decrease condition (left) places an upper bound on the\\nacceptable stepsizes while the curvature condition (right) places a lower bound on\\nthe acceptable stepsizes.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e05886e3-ae8c-43e0-a38d-2fe8f20ad359', embedding=None, metadata={'page_label': '106', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='106 3 Optimization\\nAlgorithm 3.2 Gradient Descent\\n1: Input: Initial point w0, gradient norm tolerance ϵ\\n2: Set t = 0\\n3: while ∥∇J(wt)∥ ≥ ϵ do\\n4: wt+1 = wt − ηt∇J(wt)\\n5: t = t + 1\\n6: end while\\n7: Return: wt\\nDecaying Stepsize: Instead of performing a line search at every itera-\\ntion, one can use a stepsize which decays according to a ﬁxed schedule, for\\nexample, ηt = 1/\\n√\\nt. In Section 3.2.4 we will discuss the decay schedule and\\nconvergence rates of a generalized version of gradient descent.\\nFixed Stepsize: Suppose J has a Lipschitz continuous gradient with mod-\\nulus L. Using (3.22) and the gradient descent update wt+1 = wt − ηt∇J(wt)\\none can write\\nJ(wt+1) ≤ J(wt) + ⟨∇J(wt), wt+1 − wt⟩ + L\\n2 ∥wt+1 − wt∥ (3.44)\\n= J(wt) − ηt ∥∇J(wt)∥2 + Lη2\\nt\\n2 ∥∇J(wt)∥2 . (3.45)\\nMinimizing (3.45) as a function of ηt clearly shows that the upper bound on\\nJ(wt+1) is minimized when we set ηt = 1\\nL, which is the ﬁxed stepsize rule.\\nTheorem 3.12 Suppose J has a Lipschitz continuous gradient with modu-\\nlus L. Then Algorithm 3.2 with a ﬁxed stepsize ηt = 1\\nL will return a solution\\nwt with ∥∇J(wt)∥ ≤ ϵ in at most O(1/ϵ2) iterations.\\nProof Plugging in ηt = 1\\nL and rearranging (3.45) obtains\\n1\\n2L ∥∇J(wt)∥2 ≤ J(wt) − J(wt+1) (3.46)\\nSumming this inequality\\n1\\n2L\\nT∑\\nt=0\\n∥∇J(wt)∥2 ≤ J(w0) − J(wT ) ≤ J(w0) − J(w∗),\\nwhich clearly shows that ∥∇J(wt)∥ → 0 as t → ∞ . Furthermore, we can\\nwrite the following simple inequality:\\n∥∇J(wT )∥ ≤\\n√\\n2L(J(w0) − J(w∗))\\nT + 1 .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5747a8cd-4c2b-47c7-a4e7-03d5d2c7c2df', embedding=None, metadata={'page_label': '107', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 107\\nSolving for\\n√\\n2L(J(w0) − J(w∗))\\nT + 1 = ϵ\\nshows that T is O(1/ϵ2) as claimed.\\nIf in addition to having a Lipschitz continuous gradient, if J is σ-strongly\\nconvex, then more can be said. First, one can translate convergence in\\n∥∇J(wt)∥ to convergence in function values. Towards this end, use (3.17) to\\nwrite\\nJ(wt) ≤ J(w∗) + 1\\n2σ ∥∇J(wt)∥2 .\\nTherefore, it follows that whenever ∥∇J(wt)∥ < ϵ we have J(wt) − J(w∗) <\\nϵ2/2σ. Furthermore, we can strengthen the rates of convergence.\\nTheorem 3.13 Assume everything as in Theorem 3.12. Moreover assume\\nthat J is σ-strongly convex, and let c := 1 − σ\\nL. Then J(wt) − J(w∗) ≤ ϵ\\nafter at most\\nlog((J(w0) − J(w∗))/ϵ)\\nlog(1/c) (3.47)\\niterations.\\nProof Combining (3.46) with ∥∇J(wt)∥2 ≥ 2σ(J(wt) − J(w∗)), and using\\nthe deﬁnition of c one can write\\nc(J(wt) − J(w∗)) ≥ J(wt+1) − J(w∗).\\nApplying the above equation recursively\\ncT (J(w0) − J(w∗)) ≥ J(wT ) − J(w∗).\\nSolving for\\nϵ = cT (J(w0) − J(w∗))\\nand rearranging yields (3.47).\\nWhen applied to practical problems which are not strongly convex gra-\\ndient descent yields a low accuracy solution within a few iterations. How-\\never, as the iterations progress the method “stalls” and no further increase\\nin accuracy is obtained because of the O(1/ϵ2) rates of convergence. On\\nthe other hand, if the function is strongly convex, then gradient descent\\nconverges linearly, that is, in O(log(1/ϵ)) iterations. However, the number', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2ceadcdc-1ff0-4948-afff-c83f1e5ddc36', embedding=None, metadata={'page_label': '108', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='108 3 Optimization\\nof iterations depends inversely on log(1 /c). If we approximate log(1 /c) =\\n− log(1 − σ/L) ≈ σ/L, then it shows that convergence depends on the ratio\\nL/σ. This ratio is called the condition number of a problem. If the problem\\nis well conditioned, i.e., σ ≈ L then gradient descent converges extremely\\nfast. In contrast, if σ ≪ L then gradient descent requires many iterations.\\nThis is best illustrated with an example: Consider the quadratic objective\\nfunction\\nJ(w) = 1\\n2 w⊤Aw − bw, (3.48)\\nwhere A ∈ Rn×n is a symmetric positive deﬁnite matrix, and b ∈ Rn is any\\narbitrary vector.\\nRecall that a twice diﬀerentiable function is σ-strongly convex and has a\\nLipschitz continuous gradient with modulus L if and only if its Hessian sat-\\nisﬁes LI ⪰ ∇2J(w) ⪰ σI (see (3.14) and (3.21)). In the case of the quadratic\\nfunction (3.48) ∇2J(w) = A and hence σ = λmin and L = λmax, where λmin\\n(respectively λmax) denotes the minimum (respectively maximum) eigen-\\nvalue of A. One can thus change the condition number of the problem by\\nvarying the eigen-spectrum of the matrix A. For instance, if we set A to\\nthe n × n identity matrix, then λmax = λmin = 1 and hence the problem is\\nwell conditioned. In this case, gradient descent converges very quickly to the\\noptimal solution. We illustrate this behavior on a two dimensional quadratic\\nfunction in Figure 3.8 (right).\\nOn the other hand, if we choose A such that λmax ≫ λmin then the\\nproblem (3.48) becomes ill-conditioned. In this case gradient descent exhibits\\nzigzagging and slow convergence as can be seen in Figure 3.8 (left). Because\\nof these shortcomings, gradient descent is not widely used in practice. A\\nnumber of diﬀerent algorithms we described below can be understood as\\nexplicitly or implicitly changing the condition number of the problem to\\naccelerate convergence.\\n3.2.4 Mirror Descent\\nOne way to motivate gradient descent is to use the following quadratic ap-\\nproximation of the objective function\\nQt(w) := J(wt) + ⟨∇J(wt), w − wt⟩ + 1\\n2(w − wt)⊤(w − wt), (3.49)\\nwhere, as in the previous section, ∇J(·) denotes the gradient of J. Mini-\\nmizing this quadratic model at every iteration entails taking gradients with', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1087e06e-4ad5-480c-a4ea-716296d308b9', embedding=None, metadata={'page_label': '109', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 109\\nFig. 3.8. Convergence of gradient descent with exact line search on two quadratic\\nproblems (3.48). The problem on the left is ill-conditioned, whereas the problem\\non the right is well-conditioned. We plot the contours of the objective function,\\nand the steps taken by gradient descent. As can be seen gradient descent converges\\nfast on the well conditioned problem, while it zigzags and takes many iterations to\\nconverge on the ill-conditioned problem.\\nrespect to w and setting it to zero, which gives\\nw − wt := −∇J(wt). (3.50)\\nPerforming a line search along the direction −∇J(wt) recovers the familiar\\ngradient descent update\\nwt+1 = wt − ηt∇J(wt). (3.51)\\nThe closely related mirror descent method replaces the quadratic penalty\\nin (3.49) by a Bregman divergence deﬁned by some convex function f to\\nyield\\nQt(w) := J(wt) + ⟨∇J(wt), w − wt⟩ + ∆f(w, wt). (3.52)\\nComputing the gradient, setting it to zero, and using∇w∆f(w, wt) = ∇f(w)−\\n∇f(wt), the minimizer of the above model can be written as\\n∇f(w) − ∇f(wt) = −∇J(wt). (3.53)\\nAs before, by using a stepsize ηt the resulting updates can be written as\\nwt+1 = ∇f−1(∇f(wt) − ηt∇J(wt)). (3.54)\\nIt is easy to verify that choosing f(·) = 1\\n2 ∥·∥2 recovers the usual gradient\\ndescent updates. On the other hand if we choose f to be the un-normalized\\nentropy (3.30) then ∇f(·) = log and therefore (3.54) specializes to\\nwt+1 = exp(log(wt) − ηt∇J(wt)) = wt exp(−ηt∇J(wt)), (3.55)\\nwhich is sometimes called the Exponentiated Gradient (EG) update.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='76fe9a36-b743-481f-8952-8212c98eb7f0', embedding=None, metadata={'page_label': '110', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='110 3 Optimization\\nTheorem 3.14 Let J be a convex function and J(w∗) denote its minimum\\nvalue. The mirror descent updates (3.54) with a σ-strongly convex function\\nf satisfy\\n∆f(w∗, w1) + 1\\n2σ\\n∑\\nt η2\\nt ∥∇J(wt)∥2\\n∑\\nt ηt\\n≥ min\\nt\\nJ(wt) − J(w∗).\\nProof Using the convexity of J (see (3.7)) and (3.54) we can write\\nJ(w∗) ≥ J(wt) + ⟨w∗ − wt, ∇J(wt)⟩\\n≥ J(wt) − 1\\nηt\\n⟨w∗ − wt, f(wt+1) − f(wt)⟩ .\\nNow applying Lemma 3.11 and rearranging\\n∆f(w∗, wt) − ∆f(w∗, wt+1) + ∆f(wt, wt+1) ≥ ηt(J(wt) − J(w∗)).\\nSumming over t = 1, . . . , T\\n∆f(w∗, w1) − ∆f(w∗, wT +1) +\\n∑\\nt\\n∆f(wt, wt+1) ≥\\n∑\\nt\\nηt(J(wt) − J(w∗)).\\nNoting that ∆ f(w∗, wT +1) ≥ 0, J(wt) − J(w∗) ≥ mint J(wt) − J(w∗), and\\nrearranging it follows that\\n∆f(w∗, w1) + ∑\\nt ∆f(wt, wt+1)∑\\nt ηt\\n≥ min\\nt\\nJ(wt) − J(w∗). (3.56)\\nUsing (3.17) and (3.54)\\n∆f(wt, wt+1) ≤ 1\\n2σ ∥∇f(wt) − ∇f(wt+1)∥2 = 1\\n2σ η2\\nt ∥∇J(wt)∥2 . (3.57)\\nThe proof is completed by plugging in (3.57) into (3.56).\\nCorollary 3.15 If J has a Lipschitz continuous gradient with modulus L,\\nand the stepsizes ηt are chosen as\\nηt =\\n√\\n2σ∆f(w∗, w1)\\nL\\n1√\\nt then (3.58)\\nmin\\n1≤t≤T\\nJ(wt) − J(w∗) ≤ L\\n√\\n2∆f(w∗, w1)\\nσ\\n1√\\nT\\n.\\nProof Since ∇J is Lipschitz continuous\\nmin\\n1≤t≤T\\nJ(wt) − J(w∗) ≤ ∆f(w∗, w1) + 1\\n2σ\\n∑\\nt η2\\nt L2\\n∑\\nt ηt\\n.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='928a7efa-5791-48dd-bc58-539d0b5febf8', embedding=None, metadata={'page_label': '111', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 111\\nPlugging in (3.58) and using Problem 3.15\\nmin\\n1≤t≤T\\nJ(wt) − J(w∗) ≤ L\\n√\\n∆f(w∗, w1)\\n2σ\\n(1 + ∑\\nt\\n1\\nt )∑\\nt\\n1√\\nt\\n≤ L\\n√\\n∆f(w∗, w1)\\n2σ\\n1√\\nT\\n.\\n3.2.5 Conjugate Gradient\\nLet us revisit the problem of minimizing the quadratic objective function\\n(3.48). Since ∇J(w) = Aw − b, at the optimum ∇J(w) = 0 (see Lemma 3.6)\\nand hence\\nAw = b. (3.59)\\nIn fact, the Conjugate Gradient (CG) algorithm was ﬁrst developed as a\\nmethod to solve the above linear system.\\nAs we already saw, updating w along the negative gradient direction may\\nlead to zigzagging. Therefore CG uses the so-called conjugate directions.\\nDeﬁnition 3.16 (Conjugate Directions) Non zero vectors pt and pt′ are\\nsaid to be conjugate with respect to a symmetric positive deﬁnite matrix A\\nif p⊤\\nt′ Apt = 0 if t ̸= t′.\\nConjugate directions {p0, . . . , pn−1} are linearly independent and form a\\nbasis. To see this, suppose the pt’s are not linearly independent. Then there\\nexists non-zero coeﬃcients σt such that ∑\\nt σtpt = 0. The pt’s are conjugate\\ndirections, therefore p⊤\\nt′ A(∑\\nt σtpt) = ∑\\nt σtp⊤\\nt′ Apt = σt′p⊤\\nt′ Apt′ = 0 for all t′.\\nSince A is positive deﬁnite this implies that σt′ = 0 for all t′, a contradiction.\\nAs it turns out, the conjugate directions can be generated iteratively as\\nfollows: Starting with any w0 ∈ Rn deﬁne p0 = −g0 = b − Aw0, and set\\nαt = − g⊤\\nt pt\\np⊤\\nt Apt\\n(3.60a)\\nwt+1 = wt + αtpt (3.60b)\\ngt+1 = Awt+1 − b (3.60c)\\nβt+1 = g⊤\\nt+1Apt\\np⊤\\nt Apt\\n(3.60d)\\npt+1 = −gt+1 + βt+1pt (3.60e)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5a9764bf-2e74-4de2-a03f-a2bca2497534', embedding=None, metadata={'page_label': '112', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='112 3 Optimization\\nThe following theorem asserts that the pt generated by the above procedure\\nare indeed conjugate directions.\\nTheorem 3.17 Suppose the t-th iterate generated by the conjugate gradient\\nmethod (3.60) is not the solution of (3.59), then the following properties\\nhold:\\nspan{g0, g1, . . . , gt} = span{g0, Ag0, . . . , Atg0}. (3.61)\\nspan{p0, p1, . . . , pt} = span{g0, Ag0, . . . , Atg0}. (3.62)\\np⊤\\nj gt = 0 for all j < t (3.63)\\np⊤\\nj Apt = 0 for all j < t. (3.64)\\nProof The proof is by induction. The induction hypothesis holds trivially\\nat t = 0. Assuming that (3.61) to (3.64) hold for some t, we prove that they\\ncontinue to hold for t + 1.\\nStep 1: We ﬁrst prove that (3.63) holds. Using (3.60c), (3.60b) and (3.60a)\\np⊤\\nj gt+1 = p⊤\\nj (Awt+1 − b)\\n= p⊤\\nj (Awt + αtpt − b)\\n= p⊤\\nj\\n(\\nAwt − g⊤\\nt pt\\np⊤\\nt Apt\\nApt − b\\n)\\n= p⊤\\nj gt −\\np⊤\\nj Apt\\np⊤\\nt Apt\\ng⊤\\nt pt.\\nFor j = t, both terms cancel out, while for j < t both terms vanish due to\\nthe induction hypothesis.\\nStep 2: Next we prove that (3.61) holds. Using (3.60c) and (3.60b)\\ngt+1 = Awt+1 − b = Awt + αtApt − b = gt + αtApt.\\nBy our induction hypothesis, gt ∈ span{g0, Ag0, . . . , Atg0}, while Apt ∈\\nspan{Ag0, A2g0, . . . , At+1g0}. Combining the two we conclude that gt+1 ∈\\nspan{g0, Ag0, . . . , At+1g0}. On the other hand, we already showed that gt+1\\nis orthogonal to {p0, p1, . . . , pt}. Therefore, gt+1 /∈ span{p0, p1, . . . , pt}. Thus\\nour induction assumption implies that gt+1 /∈ span{g0, Ag0, . . . , Atg0}. This\\nallows us to conclude that span{g0, g1, . . . , gt+1} = span{g0, Ag0, . . . , At+1g0}.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='035c2a7e-c800-4e83-8dd3-e17ef6ccd546', embedding=None, metadata={'page_label': '113', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 113\\nStep 3 We now prove (3.64) holds. Using (3.60e)\\np⊤\\nt+1Apj = −g⊤\\nt+1Apj + βt+1p⊤\\nt Apj.\\nBy the deﬁnition of βt+1 (3.60d) the above expression vanishes for j = t. For\\nj < t , the ﬁrst term is zero because Apj ∈ span{p0, p1, . . . , pj+1}, a subspace\\northogonal to gt+1 as already shown in Step 1. The induction hypothesis\\nguarantees that the second term is zero.\\nStep 4 Clearly, (3.61) and (3.60e) imply (3.62). This concludes the proof.\\nA practical implementation of (3.60) requires two more observations:\\nFirst, using (3.60e) and (3.63)\\n−g⊤\\nt pt = g⊤\\nt gt − βtg⊤\\nt pt−1 = g⊤\\nt gt.\\nTherefore (3.60a) simpliﬁes to\\nαt = g⊤\\nt gt\\np⊤\\nt Apt\\n. (3.65)\\nSecond, using (3.60c) and (3.60b)\\ngt+1 − gt = A(wt+1 − wt) = αtApt.\\nBut gt ∈ span{p0, . . . , pt}, a subspace orthogonal togt+1 by (3.63). Therefore\\ng⊤\\nt+1Apt = 1\\nαt (g⊤\\nt+1gt+1). Substituting this back into (3.60d) and using (3.65)\\nyields\\nβt+1 = g⊤\\nt+1gt+1\\ng⊤\\nt gt\\n. (3.66)\\nWe summarize the CG algorithm in Algorithm 3.3. Unlike gradient descent\\nwhose convergence rates for minimizing the quadratic objective function\\n(3.48) depend upon the condition number of A, as the following theorem\\nshows, the CG iterates converge in at most n steps.\\nTheorem 3.18 The CG iterates (3.60) converge to the minimizer of (3.48)\\nafter at most n steps.\\nProof Let w denote the minimizer of (3.48). Since the pt’s form a basis\\nw − w0 = σ0p0 + . . . + σn−1pn−1,\\nfor some scalars σt. Our proof strategy will be to show that the coeﬃcients', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ac2feeb6-03d5-4416-971c-e3fe20639118', embedding=None, metadata={'page_label': '114', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='114 3 Optimization\\nAlgorithm 3.3 Conjugate Gradient\\n1: Input: Initial point w0, residual norm tolerance ϵ\\n2: Set t = 0, g0 = Aw0 − b, and p0 = −g0\\n3: while ∥Awt − b∥ ≥ ϵ do\\n4: αt = g⊤\\nt gt\\np⊤\\nt Apt\\n5: wt+1 = wt + αtpt\\n6: gt+1 = gt + αtApt\\n7: βt+1 =\\ng⊤\\nt+1gt+1\\ng⊤\\nt gt\\n8: pt+1 = −gt+1 + βt+1pt\\n9: t = t + 1\\n10: end while\\n11: Return: wt\\nσt coincide with αt deﬁned in (3.60a). Towards this end premultiply with\\np⊤\\nt A and use conjugacy to obtain\\nσt = p⊤\\nt A(w − w0)\\np⊤\\nt Apt\\n. (3.67)\\nOn the other hand, following the iterative process (3.60b) from w0 until wt\\nyields\\nwt − w0 = α0p0 + . . . + αt−1pt−1.\\nAgain premultiplying with p⊤\\nt A and using conjugacy\\np⊤\\nt A(wt − w0) = 0. (3.68)\\nSubstituting (3.68) into (3.67) produces\\nσt = p⊤\\nt A(w − wt)\\np⊤\\nt Apt\\n= − g⊤\\nt pt\\np⊤\\nt Apt\\n, (3.69)\\nthus showing that σt = αt.\\nObserve that the gt+1 computed via (3.60c) is nothing but the gradient of\\nJ(wt+1). Furthermore, consider the following one dimensional optimization\\nproblem:\\nmin\\nα∈R\\nφt(α) := J(wt + αpt).\\nDiﬀerentiating φt with respect to α\\nφ′\\nt(α) = p⊤\\nt (Awt + αApt − b) = p⊤\\nt (gt + αApt).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5838a77f-2ea5-4821-9275-5157aae5d24d', embedding=None, metadata={'page_label': '115', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 115\\nThe gradient vanishes if we set α = − g⊤\\nt pt\\np⊤\\nt Apt\\n, which recovers (3.60a). In other\\nwords, every iteration of CG minimizes J(w) along a conjugate direction pt.\\nContrast this with gradient descent which minimizesJ(w) along the negative\\ngradient direction gt at every iteration.\\nIt is natural to ask if this idea of generating conjugate directions and\\nminimizing the objective function along these directions can be applied to\\ngeneral convex functions. The main diﬃculty here is that Theorems 3.17 and\\n3.18 do not hold. In spite of this, extensions of CG are eﬀective even in this\\nsetting. Basically the update rules for gt and pt remain the same, but the\\nparameters αt and βt are computed diﬀerently. Table 3.2 gives an overview\\nof diﬀerent extensions. See [NW99, Lue84] for details.\\nTable 3.2. Non-Quadratic modiﬁcations of Conjugate Gradient Descent\\nGeneric Method Compute Hessian Kt := ∇2J(wt) and update αt\\nand βt with\\nαt = − g⊤\\nt pt\\np⊤\\nt Ktpt\\nand βt = −\\ng⊤\\nt+1Ktpt\\np⊤\\nt Ktpt\\nFletcher-Reeves Set αt = argminα J(wt + αpt) and βt =\\ng⊤\\nt+1gt+1\\ng⊤\\nt gt\\n.\\nPolak-Ribi` ere Set αt = argminα J(wt + αpt), yt = gt+1 − gt, and\\nβt = y⊤\\nt gt+1\\ng⊤\\nt gt\\n.\\nIn practice, Polak-Ribi` ere tends to be better than\\nFletcher-Reeves.\\nHestenes-Stiefel Set αt = argminα J(wt + αpt), yt = gt+1 − gt, and\\nβt = y⊤\\nt gt+1\\ny⊤\\nt pt\\n.\\n3.2.6 Higher Order Methods\\nRecall the motivation for gradient descent as the minimizer of the quadratic\\nmodel\\nQt(w) := J(wt) + ⟨∇J(wt), w − wt⟩ + 1\\n2(w − wt)⊤(w − wt),\\nThe quadratic penalty in the above equation uniformly penalizes deviation\\nfrom wt in diﬀerent dimensions. When the function is ill-conditioned one\\nwould intuitively want to penalize deviations in diﬀerent directions diﬀer-\\nently. One way to achieve this is by using the Hessian, which results in the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='412615c7-b291-49c4-a3ae-92d63b9d6ff8', embedding=None, metadata={'page_label': '116', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='116 3 Optimization\\nAlgorithm 3.4 Newton’s Method\\n1: Input: Initial point w0, gradient norm tolerance ϵ\\n2: Set t = 0\\n3: while ∥∇J(wt)∥ > ϵ do\\n4: Compute pt := −∇2J(wt)−1∇J(wt)\\n5: Compute ηt = argminη J(wt + ηpt) e.g., via Algorithm 3.1.\\n6: wt+1 = wt + ηtpt\\n7: t = t + 1\\n8: end while\\n9: Return: wt\\nfollowing second order Taylor approximation:\\nQt(w) := J(wt) + ⟨∇J(wt), w − wt⟩ + 1\\n2(w − wt)⊤∇2J(wt)(w − wt).\\n(3.70)\\nOf course, this requires that J be twice diﬀerentiable. We will also assume\\nthat J is strictly convex and hence its Hessian is positive deﬁnite and in-\\nvertible. Minimizing Qt by taking gradients with respect to w and setting it\\nzero obtains\\nw − wt := −∇2J(wt)−1∇J(wt), (3.71)\\nSince we are only minimizing a model of the objective function, we perform\\na line search along the descent direction (3.71) to compute the stepsize ηt,\\nwhich yields the next iterate:\\nwt+1 = wt − ηt∇2J(wt)−1∇J(wt). (3.72)\\nDetails can be found in Algorithm 3.4.\\nSuppose w∗ denotes the minimum of J(w). We say that an algorithm\\nexhibits quadratic convergence if the sequences of iterates {wk} generated\\nby the algorithm satisﬁes:\\n∥wk+1 − w∗∥ ≤ C ∥wk − w∗∥2 (3.73)\\nfor some constant C > 0. We now show that Newton’s method exhibits\\nquadratic convergence close to the optimum.\\nTheorem 3.19 (Quadratic convergence of Newton’s Method) Suppose\\nJ is twice diﬀerentiable, strongly convex, and the Hessian of J is bounded\\nand Lipschitz continuous with modulus M in a neighborhood of the so-\\nlution w∗. Furthermore, assume that\\n\\ued79\\ued79∇2J(w)−1\\ued79\\ued79 ≤ N. The iterations', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='de17297f-a830-4bc5-a232-7ef9145d0ee5', embedding=None, metadata={'page_label': '117', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 117\\nwt+1 = wt − ∇2J(wt)−1∇J(wt) converge quadratically to w∗, the minimizer\\nof J.\\nProof First notice that\\n∇J(wt) − ∇J(w∗) =\\n∫ 1\\n0\\n∇2J(wt + t(w∗ − wt))(wt − w∗)dt. (3.74)\\nNext using the fact that ∇2J(wt) is invertible and the gradient vanishes at\\nthe optimum (∇J(w∗) = 0), write\\nwt+1 − w∗ = wt − w∗ − ∇2J(wt)−1∇J(wt)\\n= ∇2J(wt)−1[∇2J(wt)(wt − w∗) − (∇J(wt) − ∇J(w∗))]. (3.75)\\nUsing (3.75), (3.74), and the Lipschitz continuity of ∇2J\\n\\ued79\\ued79∇J(wt) − ∇J(w∗) − ∇2J(wt)(wt − w∗)\\n\\ued79\\ued79\\n=\\n\\ued79\\ued79\\ued79\\ued79\\n∫ 1\\n0\\n[∇2J(wt + t(wt − w∗)) − ∇2J(wt)](wt − w∗)dt\\n\\ued79\\ued79\\ued79\\ued79\\n≤\\n∫ 1\\n0\\n\\ued79\\ued79[∇2J(wt + t(wt − w∗)) − ∇2J(wt)]\\n\\ued79\\ued79 ∥(wt − w∗)∥ dt\\n≤ ∥wt − w∗∥2\\n∫ 1\\n0\\nM t dt = M\\n2 ∥wt − w∗∥2 . (3.76)\\nFinally use (3.75) and (3.76) to conclude that\\n∥wt+1 − w∗∥ ≤ M\\n2\\n\\ued79\\ued79∇2J(wt)−1\\ued79\\ued79 ∥wt − w∗∥2 ≤ N M\\n2 ∥wt − w∗∥2.\\nNewton’s method as we described it suﬀers from two major problems.\\nFirst, it applies only to twice diﬀerentiable, strictly convex functions. Sec-\\nond, it involves computing and inverting of the n × n Hessian matrix at\\nevery iteration, thus making it computationally very expensive. Although\\nNewton’s method can be extended to deal with positive semi-deﬁnite Hes-\\nsian matrices, the computational burden often makes it unsuitable for large\\nscale applications. In such cases one resorts to Quasi-Newton methods.\\n3.2.6.1 Quasi-Newton Methods\\nUnlike Newton’s method, which computes the Hessian of the objective func-\\ntion at every iteration, quasi-Newton methods never compute the Hessian;\\nthey approximate it from past gradients. Since they do not require the ob-\\njective function to be twice diﬀerentiable, quasi-Newton methods are much', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fe182ff6-7d43-45f0-8c4f-13d352efd358', embedding=None, metadata={'page_label': '118', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='118 3 Optimization\\n6\\n 4\\n 2\\n 0\\n 2\\n 4\\n 6\\n400\\n200\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\nFig. 3.9. The blue solid line depicts the one dimensional convex function J(w) =\\nw4 + 20w2 + w. The green dotted-dashed line represents the ﬁrst order Taylor\\napproximation to J(w), while the red dashed line represents the second order Taylor\\napproximation, both evaluated at w = 2.\\nmore widely applicable. They are widely regarded as the workhorses of\\nsmooth nonlinear optimization due to their combination of computational ef-\\nﬁciency and good asymptotic convergence. The most popular quasi-Newton\\nalgorithm is BFGS, named after its discoverers Broyde, Fletcher, Goldfarb,\\nand Shanno. In this section we will describe BFGS and its limited memory\\ncounterpart LBFGS.\\nSuppose we are given a smooth (not necessarily strictly) convex objective\\nfunction J : Rn → R and a current iterate wt ∈ Rn. Just like Newton’s\\nmethod, BFGS forms a local quadratic model of the objective function, J:\\nQt(w) := J(wt) + ⟨∇J(wt), w − wt⟩ + 1\\n2(w − wt)⊤Ht(w − wt). (3.77)\\nUnlike Newton’s method which uses the Hessian to build its quadratic model\\n(3.70), BFGS uses the matrix Ht ≻ 0, which is a positive-deﬁnite estimate\\nof the Hessian. A quasi-Newton direction of descent is found by minimizing\\nQt(w):\\nw − wt = −H−1\\nt ∇J(wt). (3.78)\\nThe stepsize ηt > 0 is found by a line search obeying the Wolfe conditions', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0c2da615-914d-4074-ae0d-76009722a739', embedding=None, metadata={'page_label': '119', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 119\\n(3.42) and (3.43). The ﬁnal update is given by\\nwt+1 = wt − ηtH−1\\nt ∇J(wt). (3.79)\\nGiven wt+1 we need to update our quadratic model (3.77) to\\nQt+1(w) := J(wt+1) + ⟨∇J(wt+1), w − wt+1⟩ + 1\\n2(w − wt+1)⊤Ht+1(w − wt+1).\\n(3.80)\\nWhen updating our model it is reasonable to expect that the gradient of\\nQt+1 should match the gradient of J at wt and wt+1. Clearly,\\n∇Qt+1(w) = ∇J(wt+1) + Ht+1(w − wt+1), (3.81)\\nwhich implies that ∇Qt+1(wt+1) = ∇J(wt+1), and hence our second con-\\ndition is automatically satisﬁed. In order to satisfy our ﬁrst condition, we\\nrequire\\n∇Qt+1(wt) = ∇J(wt+1) + Ht+1(wt − wt+1) = ∇J(wt). (3.82)\\nBy rearranging, we obtain the so-called secant equation:\\nHt+1st = yt, (3.83)\\nwhere st := wt+1 − wt and yt := ∇J(wt+1) − ∇J(wt) denote the most recent\\nstep along the optimization trajectory in parameter and gradient space,\\nrespectively. Since Ht+1 is a positive deﬁnite matrix, pre-multiplying the\\nsecant equation by st yields the curvature condition\\ns⊤\\nt yt > 0. (3.84)\\nIf the curvature condition is satisﬁed, then there are an inﬁnite number\\nof matrices Ht+1 which satisfy the secant equation (the secant equation\\nrepresents n linear equations, but the symmetric matrix Ht+1 has n(n+1)/2\\ndegrees of freedom). To resolve this issue we choose the closest matrix to\\nHt which satisﬁes the secant equation. The key insight of the BFGS comes\\nfrom the observation that the descent direction computation (3.78) involves\\nthe inverse matrix Bt := H−1\\nt . Therefore, we choose a matrix Bt+1 := H−1\\nt+1\\nsuch that it is close to Bt and also satisﬁes the secant equation:\\nmin\\nB\\n∥B − Bt∥ (3.85)\\ns. t. B = B⊤ and Byt = st. (3.86)\\nIf the matrix norm ∥·∥ is appropriately chosen [NW99], then it can be shown\\nthat\\nBt+1 = (1 −ρtsty⊤\\nt )Bt(1 −ρtyts⊤\\nt ) + ρtsts⊤\\nt , (3.87)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a785b4ac-f76b-4b76-af7b-0ff06333a958', embedding=None, metadata={'page_label': '120', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='120 3 Optimization\\nAlgorithm 3.5 LBFGS\\n1: Input: Initial point w0, gradient norm tolerance ϵ > 0\\n2: Set t = 0 and B0 = I\\n3: while ∥∇J(wt)∥ > ϵ do\\n4: pt = −Bt∇J(wt)\\n5: Find ηt that obeys (3.42) and (3.43)\\n6: st = ηtpt\\n7: wt+1 = wt + st\\n8: yt := ∇J(wt+1) − ∇J(wt)\\n9: if t = 0 : Bt := s⊤\\nt yt\\ny⊤\\nt yt\\nI\\n10: ρt = (s⊤\\nt yt)−1\\n11: Bt+1 = (I − ρtsty⊤\\nt )Bt(I − ρtyts⊤\\nt ) + ρtsts⊤\\nt\\n12: t = t + 1\\n13: end while\\n14: Return: wt\\nwhere ρt := ( y⊤\\nt st)−1. In other words, the matrix Bt is modiﬁed via an\\nincremental rank-two update, which is very eﬃcient to compute, to obtain\\nBt+1.\\nThere exists an interesting connection between the BFGS update (3.87)\\nand the Hestenes-Stiefel variant of Conjugate gradient. To see this assume\\nthat an exact line search was used to computewt+1, and therefore s⊤\\nt ∇J(wt+1) =\\n0. Furthermore, assume that Bt = 1, and use (3.87) to write\\npt+1 = −Bt+1∇J(wt+1) = −∇J(wt+1) + y⊤\\nt ∇J(wt+1)\\ny⊤\\nt st\\nst, (3.88)\\nwhich recovers the Hestenes-Stiefel update (see (3.60e) and Table 3.2).\\nLimited-memory BFGS (LBFGS) is a variant of BFGS designed for solv-\\ning large-scale optimization problems where the O(d2) cost of storing and\\nupdating Bt would be prohibitively expensive. LBFGS approximates the\\nquasi-Newton direction (3.78) directly from the last m pairs of st and yt via\\na matrix-free approach. This reduces the cost to O(md) space and time per\\niteration, with m freely chosen. Details can be found in Algorithm 3.5.\\n3.2.6.2 Spectral Gradient Methods\\nAlthough spectral gradient methods do not use the Hessian explicitly, they\\nare motivated by arguments very reminiscent of the Quasi-Newton methods.\\nRecall the update rule (3.79) and secant equation (3.83). Suppose we want', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='43279723-1ae2-430d-a375-2b666836d283', embedding=None, metadata={'page_label': '121', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 121\\na very simple matrix which approximates the Hessian. Speciﬁcally, we want\\nHt+1 = αt+1I (3.89)\\nwhere αt+1 is a scalar and I denotes the identity matrix. Then the secant\\nequation (3.83) becomes\\nαt+1st = yt. (3.90)\\nIn general, the above equation cannot be solved. Therefore we use the αt+1\\nwhich minimizes ∥αt+1st − yt∥2 which yields the Barzilai-Borwein (BB) step-\\nsize\\nαt+1 = s⊤\\nt yt\\ns⊤\\nt st\\n. (3.91)\\nAs it turns out, αt+1 lies between the minimum and maximum eigenvalue of\\nthe average Hessian in the direction st, hence the name Spectral Gradient\\nmethod. The parameter update (3.79) is now given by\\nwt+1 = wt − 1\\nαt\\n∇J(wt). (3.92)\\nA practical implementation uses safeguards to ensure that the stepsize αt+1\\nis neither too small nor too large. Given 0 < αmin < αmax < ∞ we compute\\nαt+1 = min\\n(\\nαmax, max\\n(\\nαmin, s⊤\\nt yt\\ns⊤\\nt st\\n))\\n. (3.93)\\nOne of the peculiar features of spectral gradient methods is their use\\nof a non-monotone line search. In all the algorithms we have seen so far,\\nthe stepsize is chosen such that the objective function J decreases at every\\niteration. In contrast, non-monotone line searches employ a parameter M ≥\\n1 and ensure that the objective function decreases in every M iterations. Of\\ncourse, setting M = 1 results in the usual monotone line search. Details can\\nbe found in Algorithm 3.6.\\n3.2.7 Bundle Methods\\nThe methods we discussed above are applicable for minimizing smooth, con-\\nvex objective functions. Some regularized risk minimization problems involve\\na non-smooth objective function. In such cases, one needs to use bundle\\nmethods. In order to lay the ground for bundle methods we ﬁrst describe\\ntheir precursor the cutting plane method [Kel60]. Cutting plane method is\\nbased on a simple observation: A convex function is bounded from below by', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c623f389-cd0d-4564-a84b-0bdc59c3f0ae', embedding=None, metadata={'page_label': '122', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='122 3 Optimization\\nAlgorithm 3.6 Spectral Gradient Method\\n1: Input: w0, M ≥ 1, αmax > α min > 0, γ ∈ (0, 1), 1 > σ 2 > σ 1 > 0,\\nα0 ∈ [αmin, αmax], and ϵ > 0\\n2: Initialize: t = 0\\n3: while ∥∇J(wt)∥ > ϵ do\\n4: λ = 1\\n5: while TRUE do\\n6: dt = − 1\\nαt ∇J(wt)\\n7: w+ = wt + λdt\\n8: δ = ⟨dt, ∇J(wt)⟩\\n9: if J(w+) ≤ min0≤j≤min(t,M−1) J(xt−j) + γλδ then\\n10: wt+1 = w+\\n11: st = wt+1 − wt\\n12: yt = ∇J(wt+1) − ∇J(wt)\\n13: break\\n14: else\\n15: λtmp = − 1\\n2 λ2δ/(J(w+) − J(wt) − λδ)\\n16: if λtmp > σ1 and λtmp < σ2λ then\\n17: λ = λtmp\\n18: else\\n19: λ = λ/2\\n20: end if\\n21: end if\\n22: end while\\n23: αt+1 = min(αmax, max(αmin, s⊤\\nt yt\\ns⊤\\nt st\\n))\\n24: t = t + 1\\n25: end while\\n26: Return: wt\\nits linearization (i.e., ﬁrst order Taylor approximation). See Figures 3.4 and\\n3.5 for geometric intuition, and recall (3.7) and (3.13):\\nJ(w) ≥ J(w′) +\\n⟨\\nw − w′, s′⟩\\n∀w and s′ ∈ ∂J (w′). (3.94)\\nGiven subgradients s1, s2, . . . , st evaluated at locations w0, w1, . . . , wt−1, we\\ncan construct a tighter (piecewise linear) lower bound for J as follows (also\\nsee Figure 3.10):\\nJ(w) ≥ JCP\\nt (w) := max\\n1≤i≤t\\n{J(wi−1) + ⟨w − wi−1, si⟩}. (3.95)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='742ec67a-7b4a-4e70-be8f-805c9d3f3f36', embedding=None, metadata={'page_label': '123', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.2 Unconstrained Smooth Convex Minimization 123\\nGiven iterates {wi}t−1\\ni=0, the cutting plane method minimizes JCP\\nt to obtain\\nthe next iterate wt:\\nwt := argmin\\nw\\nJCP\\nt (w). (3.96)\\nThis iteratively reﬁnes the piecewise linear lower bound JCP and allows us\\nto get close to the minimum of J (see Figure 3.10 for an illustration).\\nIf w∗ denotes the minimizer of J, then clearly each J(wi) ≥ J(w∗) and\\nhence min 0≤i≤t J(wi) ≥ J(w∗). On the other hand, since J ≥ JCP\\nt it fol-\\nlows that J(w∗) ≥ JCP\\nt (wt). In other words, J(w∗) is sandwiched between\\nmin0≤i≤t J(wi) and JCP\\nt (wt) (see Figure 3.11 for an illustration). The cutting\\nplane method monitors the monotonically decreasing quantity\\nϵt := min\\n0≤i≤t\\nJ(wi) − JCP\\nt (wt), (3.97)\\nand terminates whenever ϵt falls below a predeﬁned threshold ϵ. This ensures\\nthat the solution J(wt) is ϵ optimum, that is, J(wt) ≤ J(w∗) + ϵ.\\nFig. 3.10. A convex function (blue solid curve) is bounded from below by its lin-\\nearizations (dashed lines). The gray area indicates the piecewise linear lower bound\\nobtained by using the linearizations. We depict a few iterations of the cutting plane\\nmethod. At each iteration the piecewise linear lower bound is minimized and a new\\nlinearization is added at the minimizer (red rectangle). As can be seen, adding more\\nlinearizations improves the lower bound.\\nAlthough cutting plane method was shown to be convergent [Kel60], it is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6a75396a-f26d-474a-9be7-8584c0d7eddb', embedding=None, metadata={'page_label': '124', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='124 3 Optimization\\nFig. 3.11. A convex function (blue solid curve) with four linearizations evaluated at\\nfour diﬀerent locations (magenta circles). The approximation gap ϵ3 at the end of\\nfourth iteration is indicated by the height of the cyan horizontal bandi.e., diﬀerence\\nbetween lowest value of J(w) evaluated so far and the minimum of J CP\\n4 (w) (red\\ndiamond).\\nwell known (see e.g., [LNN95, Bel05]) that it can be very slow when new\\niterates move too far away from the previous ones ( i.e., causing unstable\\n“zig-zag” behavior in the iterates). In fact, in the worst case the cutting\\nplane method might require exponentially many steps to converge to an ϵ\\noptimum solution.\\nBundle methods stabilize CPM by augmenting the piecewise linear lower\\n(e.g., JCP\\nt (w) in (3.95)) with a prox-function ( i.e., proximity control func-\\ntion) which prevents overly large steps in the iterates [Kiw90]. Roughly\\nspeaking, there are 3 popular types of bundle methods, namely, proximal\\n[Kiw90], trust region [SZ92], and level set [LNN95]. All three versions use\\n1\\n2 ∥·∥2 as their prox-function, but diﬀer in the way they compute the new\\niterate:\\nproximal: wt := argmin\\nw\\n{ ζt\\n2 ∥w − ˆwt−1∥2 + JCP\\nt (w)}, (3.98)\\ntrust region: wt := argmin\\nw\\n{JCP\\nt (w) | 1\\n2 ∥w − ˆwt−1∥2 ≤ κt}, (3.99)\\nlevel set: wt := argmin\\nw\\n{1\\n2 ∥w − ˆwt−1∥2 | JCP\\nt (w) ≤ τt}, (3.100)\\nwhere ˆwt−1 is the current prox-center, and ζt, κt, and τt are positive trade-\\noﬀ parameters of the stabilization. Although (3.98) can be shown to be\\nequivalent to (3.99) for appropriately chosen ζt and κt, tuning ζt is rather\\ndiﬃcult while a trust region approach can be used for automatically tuning', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5ae66cc7-4813-4869-bdcb-1fa5e21abb03', embedding=None, metadata={'page_label': '125', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Constrained Optimization 125\\nκt. Consequently the trust region algorithm BT of [SZ92] is widely used in\\npractice.\\n3.3 Constrained Optimization\\nSo far our focus was on unconstrained optimization problems. Many ma-\\nchine learning problems involve constraints, and can often be written in the\\nfollowing canonical form:\\nmin\\nw\\nJ(w) (3.101a)\\ns. t. ci(w) ≤ 0 for i ∈ I (3.101b)\\nei(w) = 0 for i ∈ E (3.101c)\\nwhere both ci and ei are convex functions. We say that w is feasible if and\\nonly if it satisﬁes the constraints, that is, ci(w) ≤ 0 for i ∈ I and ei(w) = 0\\nfor i ∈ E.\\nRecall that w is the minimizer of an unconstrained problem if and only if\\n∥∇J(w)∥ = 0 (see Lemma 3.6). Unfortunately, when constraints are present\\none cannot use this simple characterization of the solution. For instance, the\\nw at which ∥∇J(w)∥ = 0 may not be a feasible point. To illustrate, consider\\nthe following simple minimization problem (see Figure 3.12):\\nmin\\nw\\n1\\n2 w2 (3.102a)\\ns. t. 1 ≤ w ≤ 2. (3.102b)\\nClearly, 1\\n2 w2 is minimized at w = 0, but because of the presence of the con-\\nstraints, the minimum of (3.102) is attained at w = 1 where ∇J(w) = w is\\nequal to 1. Therefore, we need other ways to detect convergence. In Section\\n3.3.1 we discuss some general purpose algorithms based on the concept of or-\\nthogonal projection. In Section 3.3.2 we will discuss Lagrange duality, which\\ncan be used to further characterize the solutions of constrained optimization\\nproblems.\\n3.3.1 Projection Based Methods\\nSuppose we are interested in minimizing a smooth convex function of the\\nfollowing form:\\nmin\\nw∈Ω\\nJ(w), (3.103)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='55e9c57c-131f-45c9-9bfb-abc8f90f1fed', embedding=None, metadata={'page_label': '126', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='126 3 Optimization\\n6\\n 4\\n 2\\n 0 2 4 6\\nw\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14J(w)\\nFig. 3.12. The unconstrained minimum of the quadratic function 1\\n2 w2 is attained\\nat w = 0 (red circle). But, if we enforce the constraints 1 ≤ w ≤ 2 (illustrated by\\nthe shaded area) then the minimizer is attained at w = 1 (green diamond).\\nwhere Ω is a convex feasible region. For instance, Ω may be described by\\nconvex functions ci and ei as in (3.101). The algorithms we describe in this\\nsection are applicable when Ω is a relatively simple set onto which we can\\ncompute an orthogonal projection. Given a point w′ and a feasible region\\nΩ, the orthogonal projection PΩ(w′) of w′ on Ω is deﬁned as\\nPΩ(w′) := argmin\\nw∈Ω\\n\\ued79\\ued79w′ − w\\n\\ued79\\ued792 . (3.104)\\nGeometrically speaking, PΩ(w′) is the closest point to w′ in Ω. Of course, if\\nw′ ∈ Ω then PΩ(w′) = w′.\\nWe are interested in ﬁnding an approximate solution of (3.103), that is,\\na w ∈ Ω such that\\nJ(w) − min\\nw∈Ω\\nJ(w) = J(w) − J∗ ≤ ϵ, (3.105)\\nfor some pre-deﬁned tolerance ϵ > 0. Of course, J∗ is unknown and hence the\\ngap J(w) − J∗ cannot be computed in practice. Furthermore, as we showed\\nin Section 3.3, for constrained optimization problems ∥∇J(w)∥ does not\\nvanish at the optimal solution. Therefore, we will use the following stopping', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c0b04184-4fa0-4f6a-9259-5614750ab7e3', embedding=None, metadata={'page_label': '127', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Constrained Optimization 127\\nAlgorithm 3.7 Basic Projection Based Method\\n1: Input: Initial point w0 ∈ Ω, and projected gradient norm tolerance\\nϵ > 0\\n2: Initialize: t = 0\\n3: while ∥PΩ(wt − ∇J(wt)) − wt∥ > ϵ do\\n4: Find direction of descent dt\\n5: wt+1 = PΩ(wt + ηtdt)\\n6: t = t + 1\\n7: end while\\n8: Return: wt\\ncriterion in our algorithms\\n∥PΩ(wt − ∇J(wt)) − wt∥ ≤ ϵ. (3.106)\\nThe intuition here is as follows: If wt − ∇ J(wt) ∈ Ω then PΩ(wt −\\n∇J(wt)) = wt if, and only if, ∇J(wt) = 0, that is, wt is the global minimizer\\nof J(w). On the other hand, if wt − ∇J(wt) /∈ Ω but PΩ(wt − ∇J(wt)) = wt,\\nthen the constraints are preventing us from making any further progress\\nalong the descent direction −∇J(wt) and hence we should stop.\\nThe basic projection based method is described in Algorithm 3.7. Any\\nunconstrained optimization algorithm can be used to generate the direction\\nof descent dt. A line search is used to ﬁnd the stepsize ηt. The updated\\nparameter wt − ηtdt is projected onto Ω to obtain wt+1. If dt is chosen to\\nbe the negative gradient direction −∇J(wt), then the resulting algorithm\\nis called the projected gradient method. One can show that the rates of\\nconvergence of gradient descent with various line search schemes is also\\npreserved by projected gradient descent.\\n3.3.2 Lagrange Duality\\nLagrange duality plays a central role in constrained convex optimization.\\nThe basic idea here is to augment the objective function (3.101) with a\\nweighted sum of the constraint functions by deﬁning the Lagrangian:\\nL(w, α, β) = J(w) +\\n∑\\ni∈I\\nαici(w) +\\n∑\\ni∈E\\nβiei(w) (3.107)\\nfor αi ≥ 0 and βi ∈ R. In the sequel, we will refer to α (respectively β) as the\\nLagrange multipliers associated with the inequality (respectively equality)\\nconstraints. Furthermore, we will call α and β dual feasible if and only if', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d92fc10c-8f7b-418e-846e-a92fc7be5faa', embedding=None, metadata={'page_label': '128', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='128 3 Optimization\\nαi ≥ 0 and βi ∈ R. The Lagrangian satisﬁes the following fundamental\\nproperty, which makes it extremely useful for constrained optimization.\\nTheorem 3.20 The Lagrangian (3.107) of (3.101) satisﬁes\\nmax\\nα≥0,β\\nL(w, α, β) =\\n{\\nJ(w) if w is feasible\\n∞ otherwise.\\nIn particular, if J∗ denotes the optimal value of (3.101), then\\nJ∗ = min\\nw\\nmax\\nα≥0,β\\nL(w, α, β).\\nProof First assume that w is feasible, that is, ci(w) ≤ 0 for i ∈ I and\\nei(w) = 0 for i ∈ E. Since αi ≥ 0 we have\\n∑\\ni∈I\\nαici(w) +\\n∑\\ni∈E\\nβiei(w) ≤ 0, (3.108)\\nwith equality being attained by setting αi = 0 whenever ci(w) < 0. Conse-\\nquently,\\nmax\\nα≥0,β\\nL(w, α, β) = max\\nα≥0,β\\nJ(w) +\\n∑\\ni∈I\\nαici(w) +\\n∑\\ni∈E\\nβiei(w) = J(w)\\nwhenever w is feasible. On the other hand, if w is not feasible then either\\nci′(w) > 0 or ei′(w) ̸= 0 for some i′. In the ﬁrst case simply let αi′ → ∞ to\\nsee that max α≥0,β L(w, α, β) → ∞. Similarly, when ei′(w) ̸= 0 let βi′ → ∞\\nif ei′(w) > 0 or βi′ → −∞ if ei′(w) < 0 to arrive at the same conclusion.\\nIf deﬁne the Lagrange dual function\\nD(α, β) = min\\nw\\nL(w, α, β), (3.109)\\nfor α ≥ 0 and β, then one can prove the following property, which is often\\ncalled as weak duality.\\nTheorem 3.21 (Weak Duality) The Lagrange dual function (3.109) sat-\\nisﬁes\\nD(α, β) ≤ J(w)\\nfor all feasible w and α ≥ 0 and β. In particular\\nD∗ := max\\nα≥0,β\\nmin\\nw\\nL(w, α, β) ≤ min\\nw\\nmax\\nα≥0,β\\nL(w, α, β) = J∗. (3.110)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='253a8ab0-6072-4f3c-9628-744cac2d8349', embedding=None, metadata={'page_label': '129', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Constrained Optimization 129\\nProof As before, observe that whenever w is feasible\\n∑\\ni∈I\\nαici(w) +\\n∑\\ni∈E\\nβiei(w) ≤ 0.\\nTherefore\\nD(α, β) = min\\nw\\nL(w, α, β) = min\\nw\\nJ(w) +\\n∑\\ni∈I\\nαici(w) +\\n∑\\ni∈E\\nβiei(w) ≤ J(w)\\nfor all feasible w and α ≥ 0 and β. In particular, one can choose w to be\\nthe minimizer of (3.101) and α ≥ 0 and β to be maximizers of D(α, β) to\\nobtain (3.110).\\nWeak duality holds for any arbitrary function, not-necessarily convex. When\\nthe objective function and constraints are convex, and certain technical con-\\nditions, also known as Slater’s conditions hold, then we can say more.\\nTheorem 3.22 (Strong Duality) Supposed the objective function f and\\nconstraints ci for i ∈ I and ei for i ∈ E in (3.101) are convex and the\\nfollowing constraint qualiﬁcation holds:\\nThere exists a w such that ci(w) < 0 for all i ∈ I.\\nThen the Lagrange dual function (3.109) satisﬁes\\nD∗ := max\\nα≥0,β\\nmin\\nw\\nL(w, α, β) = min\\nw\\nmax\\nα≥0,β\\nL(w, α, β) = J∗. (3.111)\\nThe proof of the above theorem is quite technical and can be found in\\nany standard reference (e.g., [BV04]). Therefore we will omit the proof and\\nproceed to discuss various implications of strong duality. First note that\\nmin\\nw\\nmax\\nα≥0,β\\nL(w, α, β) = max\\nα≥0,β\\nmin\\nw\\nL(w, α, β). (3.112)\\nIn other words, one can switch the order of minimization over w with max-\\nimization over α and β. This is called the saddle point property of convex\\nfunctions.\\nSuppose strong duality holds. Given any α ≥ 0 and β such that D(α, β) >\\n−∞ and a feasible w we can immediately write the duality gap\\nJ(w) − J∗ = J(w) − D∗ ≤ J(w) − D(α, β),\\nwhere J∗ and D∗ were deﬁned in (3.111). Below we show that if w∗ is primal\\noptimal and ( α∗, β∗) are dual optimal then J(w∗) − D(α∗, β∗) = 0. This\\nprovides a non-heuristic stopping criterion for constrained optimization: stop\\nwhen J(w) − D(α, β) ≤ ϵ, where ϵ is a pre-speciﬁed tolerance.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f3877b4e-dc60-4af6-b85b-5322bbc753a8', embedding=None, metadata={'page_label': '130', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='130 3 Optimization\\nSuppose the primal and dual optimal values are attained at w∗ and\\n(α∗, β∗) respectively, and consider the following line of argument:\\nJ(w∗) = D(α∗, β∗) (3.113a)\\n= min\\nw\\nJ(w) +\\n∑\\ni∈I\\nα∗\\ni ci(w) +\\n∑\\ni∈E\\nβ∗\\ni ej(w) (3.113b)\\n≤ J(w∗) +\\n∑\\ni∈I\\nα∗\\ni ci(w∗) +\\n∑\\ni∈E\\nβ∗\\ni ei(w∗) (3.113c)\\n≤ J(w∗). (3.113d)\\nTo write (3.113a) we used strong duality, while (3.113c) obtains by setting\\nw = w∗ in (3.113c). Finally, to obtain (3.113d) we used the fact that w∗ is\\nfeasible and hence (3.108) holds. Since (3.113) holds with equality, one can\\nconclude that the following complementary slackness condition :\\n∑\\ni∈I\\nα∗\\ni ci(w∗) +\\n∑\\ni∈E\\nβ∗\\ni ei(w∗) = 0.\\nIn other words, α∗\\ni ci(w∗) = 0 or equivalently α∗\\ni = 0 whenever ci(w) < 0.\\nFurthermore, since w∗ minimizes L(w, α∗, β∗) over w, it follows that its\\ngradient must vanish at w∗, that is,\\n∇J(w∗) +\\n∑\\ni∈I\\nα∗\\ni ∇ci(w∗) +\\n∑\\ni∈E\\nβ∗\\ni ∇ei(w∗) = 0.\\nPutting everything together, we obtain\\nci(w∗) ≤ 0 ∀i ∈ I (3.114a)\\nej(w∗) = 0 ∀i ∈ E (3.114b)\\nα∗\\ni ≥ 0 (3.114c)\\nα∗\\ni ci(w∗) = 0 (3.114d)\\n∇J(w∗) +\\n∑\\ni∈I\\nα∗\\ni ∇ci(w∗) +\\n∑\\ni∈E\\nβ∗\\ni ∇ei(w∗) = 0. (3.114e)\\nThe above conditions are called the KKT conditions. If the primal problem is\\nconvex, then the KKT conditions are both necessary and suﬃcient. In other\\nwords, if ˆw and (ˆα, ˆβ) satisfy (3.114) then ˆw and (ˆα, ˆβ) are primal and dual\\noptimal with zero duality gap. To see this note that the ﬁrst two conditions\\nshow that ˆw is feasible. Since αi ≥ 0, L(w, α, β) is convex in w. Finally the\\nlast condition states that ˆ w minimizes L(w, ˆα, ˆβ). Since ˆαici( ˆw) = 0 and', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a87fad08-216e-42e3-a894-893eaac0e01f', embedding=None, metadata={'page_label': '131', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Constrained Optimization 131\\nej( ˆw) = 0, we have\\nD(ˆα, ˆβ) = min\\nw\\nL(w, ˆα, ˆβ)\\n= J( ˆw) +\\nn∑\\ni=1\\nˆαici( ˆw) +\\nm∑\\nj=1\\nˆβjej( ˆw)\\n= J( ˆw).\\n3.3.3 Linear and Quadratic Programs\\nSo far we discussed general constrained optimization problems. Many ma-\\nchine learning problems have special structure which can be exploited fur-\\nther. We discuss the implication of duality for two such problems.\\n3.3.3.1 Linear Programming\\nAn optimization problem with a linear objective function and (both equality\\nand inequality) linear constraints is said to be a linear program (LP). A\\ncanonical linear program is of the following form:\\nmin\\nw\\nc⊤w (3.115a)\\ns. t. Aw = b, w ≥ 0. (3.115b)\\nHere w and c are n dimensional vectors, while b is a m dimensional vector,\\nand A is a m × n matrix with m < n .\\nSuppose we are given a LP of the form:\\nmin\\nw\\nc⊤w (3.116a)\\ns. t. Aw ≥ b, (3.116b)\\nwe can transform it into a canonical LP by introducing non-negative slack\\nvariables\\nmin\\nw,ξ\\nc⊤w (3.117a)\\ns. t. Aw − ξ = b, ξ ≥ 0. (3.117b)\\nNext, we split w into its positive and negative parts w+ and w− respec-\\ntively by setting w+\\ni = max(0, wi) and w−\\ni = max(0, −wi). Using these new', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='668ddc68-276e-4abb-a604-04edd247536e', embedding=None, metadata={'page_label': '132', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='132 3 Optimization\\nvariables we rewrite (3.117) as\\nmin\\nw+,w−, ξ\\n\\uf8ee\\n\\uf8f0\\nc\\n−c\\n0\\n\\uf8f9\\n\\uf8fb\\n⊤ \\uf8ee\\n\\uf8f0\\nw+\\nw−\\nξ\\n\\uf8f9\\n\\uf8fb (3.118a)\\ns. t.\\n[\\nA −A −I\\n]\\n\\uf8ee\\n\\uf8f0\\nw+\\nw−\\nξ\\n\\uf8f9\\n\\uf8fb = b,\\n\\uf8ee\\n\\uf8f0\\nw+\\nw−\\nξ\\n\\uf8f9\\n\\uf8fb ≥ 0, (3.118b)\\nthus yielding a canonical LP (3.115) in the variables w+, w− and ξ.\\nBy introducing non-negative Lagrange multipliers α and β one can write\\nthe Lagrangian of (3.115) as\\nL(w, β, s) = c⊤w + β⊤(Aw − b) − α⊤w. (3.119)\\nTaking gradients with respect to the primal and dual variables and setting\\nthem to zero obtains\\nA⊤β − α = c (3.120a)\\nAw = b (3.120b)\\nα⊤w = 0 (3.120c)\\nw ≥ 0 (3.120d)\\nα ≥ 0. (3.120e)\\nCondition (3.120c) can be simpliﬁed by noting that both w and α are con-\\nstrained to be non-negative, therefore α⊤w = 0 if, and only if, αiwi = 0 for\\ni = 1, . . . , n.\\nUsing (3.120a), (3.120c), and (3.120b) we can write\\nc⊤w = (A⊤β − α)⊤w = β⊤Aw = β⊤b.\\nSubstituting this into (3.115) and eliminating the primal variable w yields\\nthe following dual LP\\nmax\\nα,β\\nb⊤β (3.121a)\\ns.t. A⊤β − α = c, α ≥ 0. (3.121b)\\nAs before, we let β+ = max( β, 0) and β− = max(0 , −β) and convert the', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1168e840-718c-4539-8348-9df9e5c8ab01', embedding=None, metadata={'page_label': '133', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3 Constrained Optimization 133\\nabove LP into the following canonical LP\\nmax\\nα,β+,β−\\n\\uf8ee\\n\\uf8f0\\nb\\n−b\\n0\\n\\uf8f9\\n\\uf8fb\\n⊤ \\uf8ee\\n\\uf8f0\\nβ+\\nβ−\\nα\\n\\uf8f9\\n\\uf8fb (3.122a)\\ns.t.\\n[\\nA⊤ −A⊤ −I\\n]\\n\\uf8ee\\n\\uf8f0\\nβ+\\nβ−\\nα\\n\\uf8f9\\n\\uf8fb = c,\\n\\uf8ee\\n\\uf8f0\\nβ+\\nβ−\\nα\\n\\uf8f9\\n\\uf8fb ≥ 0. (3.122b)\\nIt can be easily veriﬁed that the primal-dual problem is symmetric; by taking\\nthe dual of the dual we recover the primal (Problem 3.17). One important\\nthing to note however is that the primal (3.115) involves n variables and\\nn + m constraints, while the dual (3.122) involves 2 m + n variables and\\n4m + 2n constraints.\\n3.3.3.2 Quadratic Programming\\nAn optimization problem with a convex quadratic objective function and lin-\\near constraints is said to be a convex quadratic program (QP). The canonical\\nconvex QP can be written as follows:\\nmin\\nw\\n1\\n2 w⊤Gx + w⊤d (3.123a)\\ns.t. a⊤\\ni w = bi for i ∈ E (3.123b)\\na⊤\\ni w ≤ bi for i ∈ I (3.123c)\\nHere G ⪰ 0 is a n × n positive semi-deﬁnite matrix, E and I are ﬁnite set of\\nindices, while d and ai are n dimensional vectors, and bi are scalars.\\nAs a warm up let us consider the arguably simpler equality constrained\\nquadratic programs. In this case, we can stack the ai into a matrix A and\\nthe bi into a vector b to write\\nmin\\nw\\n1\\n2 w⊤Gw + w⊤d (3.124a)\\ns.t. Aw = b (3.124b)\\nBy introducing non-negative Lagrange multipliers β the Lagrangian of the\\nabove optimization problem can be written as\\nL(w, β) = 1\\n2 w⊤Gw + w⊤d + β(Aw − b). (3.125)\\nTo ﬁnd the saddle point of the Lagrangian we take gradients with respect', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d38350ba-49c1-47a7-8139-7f8d77e7877f', embedding=None, metadata={'page_label': '134', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='134 3 Optimization\\nto w and β and set them to zero. This obtains\\nGw + d + A⊤β = 0\\nAw = b.\\nPutting these two conditions together yields the following linear system of\\nequations\\n[ G A ⊤\\nA 0\\n] [ w\\nβ\\n]\\n=\\n[ −d\\nb\\n]\\n. (3.126)\\nThe matrix in the above equation is called the KKT matrix, and we can use\\nit to characterize the conditions under which (3.124) has a unique solution.\\nTheorem 3.23 Let Z be a n × (n − m) matrix whose columns form a basis\\nfor the null space of A, that is, AZ = 0 . If A has full row rank, and the\\nreduced-Hessian matrix Z⊤GZ is positive deﬁnite, then there exists a unique\\npair (w∗, β∗) which solves (3.126). Furthermore, w∗ also minimizes (3.124).\\nProof Note that a unique ( w∗, β∗) exists whenever the KKT matrix is\\nnon-singular. Suppose this is not the case, then there exist non-zero vectors\\na and b such that\\n[ G A ⊤\\nA 0\\n] [ a\\nb\\n]\\n= 0.\\nSince Aa = 0 this implies that a lies in the null space of A and hence there\\nexists a u such that a = Zu. Therefore\\n[\\nZu 0\\n] [ G A ⊤\\nA 0\\n] [ Zu\\n0\\n]\\n= u⊤Z⊤GZu = 0.\\nPositive deﬁniteness of Z⊤GZ implies that u = 0 and hence a = 0. On the\\nother hand, the full row rank of A and A⊤b = 0 implies that b = 0. In\\nsummary, both a and b are zero, a contradiction.\\nLet w ̸= w∗ be any other feasible point and ∆ w = w∗ − w. Since Aw∗ =\\nAw = b we have that A∆w = 0. Hence, there exists a non-zero u such that\\n∆w = Zu. The objective function J(w) can be written as\\nJ(w) = 1\\n2(w∗ − ∆w)⊤G(w∗ − ∆w) + (w∗ − ∆w)⊤d\\n= J(w∗) + 1\\n2∆w⊤G∆w − (Gw∗ + d)⊤∆w.\\nFirst note that 1\\n2∆w⊤G∆w = 1\\n2 u⊤Z⊤GZu > 0 by positive deﬁniteness of\\nthe reduced Hessian. Second, since w∗ solves (3.126) it follows that ( Gw∗ +', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='32d0f299-555a-4775-8964-3879b427126e', embedding=None, metadata={'page_label': '135', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.4 Stochastic Optimization 135\\nd)⊤∆w = β⊤A∆w = 0. Together these two observations imply that J(w) >\\nJ(w∗).\\nIf the technical conditions of the above theorem are met, then solving the\\nequality constrained QP (3.124) is equivalent to solving the linear system\\n(3.126). See [NW99] for a extensive discussion of algorithms that can be\\nused for this task.\\nNext we turn our attention to the general QP (3.123) which also contains\\ninequality constraints. The Lagrangian in this case can be written as\\nL(w, β) = 1\\n2 w⊤Gw + w⊤d +\\n∑\\ni∈I\\nαi(a⊤\\ni w − bi) +\\n∑\\ni∈E\\nβi(a⊤\\ni w − bi). (3.127)\\nLet w∗ denote the minimizer of (3.123). If we deﬁne the active set A(w∗) as\\nA(w∗) =\\n{\\ni s.t. i ∈ I and a⊤\\ni w∗ = bi\\n}\\n,\\nthen the KKT conditions (3.114) for this problem can be written as\\na⊤\\ni w − bi < 0 ∀i ∈ I \\\\ A(w∗) (3.128a)\\na⊤\\ni w − bi = 0 ∀i ∈ E ∪ A(w∗) (3.128b)\\nα∗\\ni ≥ 0 ∀i ∈ A(w∗) (3.128c)\\nGw∗ + d +\\n∑\\ni∈A(w∗)\\nα∗\\ni ai +\\n∑\\ni∈E\\nβiai = 0. (3.128d)\\nConceptually the main diﬃculty in solving (3.123) is in identifying the active\\nset A(w∗). This is because α∗\\ni = 0 for all i ∈ I \\\\ A(w∗). Most algorithms\\nfor solving (3.123) can be viewed as diﬀerent ways to identify the active set.\\nSee [NW99] for a detailed discussion.\\n3.4 Stochastic Optimization\\nRecall that regularized risk minimization involves a data-driven optimization\\nproblem in which the objective function involves the summation of loss terms\\nover a set of data to be modeled:\\nmin\\nf\\nJ(f) := λΩ(f) + 1\\nm\\nm∑\\ni=1\\nl(f(xi), yi).\\nClassical optimization techniques must compute this sum in its entirety for\\neach evaluation of the objective, respectively its gradient. As available data\\nsets grow ever larger, such “batch” optimizers therefore become increasingly\\nineﬃcient. They are also ill-suited for the incremental setting, where partial\\ndata must be modeled as it arrives.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2f0dfe31-c8ac-4dfe-a0b3-35e27d8d7b8d', embedding=None, metadata={'page_label': '136', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='136 3 Optimization\\nStochastic gradient-based methods, by contrast, work with gradient esti-\\nmates obtained from small subsamples (mini-batches) of training data. This\\ncan greatly reduce computational requirements: on large, redundant data\\nsets, simple stochastic gradient descent routinely outperforms sophisticated\\nsecond-order batch methods by orders of magnitude.\\nThe key idea here is that J(w) is replaced by an instantaneous estimate\\nJt which is computed from a mini-batch of size k comprising of a subset of\\npoints (xt\\ni, yt\\ni) with i = 1, . . . , k drawn from the dataset:\\nJt(w) = λΩ(w) + 1\\nk\\nk∑\\ni=1\\nl(w, xt\\ni, yt\\ni). (3.129)\\nSetting k = 1 obtains an algorithm which processes data points as they\\narrive.\\n3.4.1 Stochastic Gradient Descent\\nPerhaps the simplest stochastic optimization algorithm is Stochastic Gradi-\\nent Descent (SGD). The parameter update of SGD takes the form:\\nwt+1 = wt − ηt∇Jt(wt). (3.130)\\nIf Jt is not diﬀerentiable, then one can choose an arbitrary subgradient from\\n∂Jt(wt) to compute the update. It has been shown that SGD asymptotically\\nconverges to the true minimizer of J(w) if the stepsize ηt decays as O(1/\\n√\\nt).\\nFor instance, one could set\\nηt =\\n√ τ\\nτ + t , (3.131)\\nwhere τ > 0 is a tuning parameter. See Algorithm 3.8 for details.\\n3.4.1.1 Practical Considerations\\nOne simple yet eﬀective rule of thumb to tune τ is to select a small subset\\nof data, try various values of τ on this subset, and choose the τ that most\\nreduces the objective function.\\nIn some cases letting ηt to decay as O(1/t) has been found to be more\\neﬀective:\\nηt = τ\\nτ + t . (3.132)\\nThe free parameter τ > 0 can be tuned as described above. If Ω( w) is σ-\\nstrongly convex, then dividing the stepsize ηt by σλ yields good practical\\nperformance.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9d51957d-1247-48c9-b9b1-7f421b838cde', embedding=None, metadata={'page_label': '137', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.5 Nonconvex Optimization 137\\nAlgorithm 3.8 Stochastic Gradient Descent\\n1: Input: Maximum iterations T , batch size k, and τ\\n2: Set t = 0 and w0 = 0\\n3: while t < T do\\n4: Choose a subset of k data points ( xt\\ni, yt\\ni) and compute ∇Jt(wt)\\n5: Compute stepsize ηt =\\n√\\nτ\\nτ+t\\n6: wt+1 = wt − ηt∇Jt(wt)\\n7: t = t + 1\\n8: end while\\n9: Return: wT\\n3.5 Nonconvex Optimization\\nOur focus in the previous sections was on convex objective functions. Some-\\ntimes non-convex objective functions also arise in machine learning applica-\\ntions. These problems are signiﬁcantly harder and tools for minimizing such\\nobjective functions are not as well developed. We brieﬂy describe one algo-\\nrithm which can be applied whenever we can write the objective function as\\na diﬀerence of two convex functions.\\n3.5.1 Concave-Convex Procedure\\nAny function with a bounded Hessian can be decomposed into the diﬀerence\\nof two (non-unique) convex functions, that is, one can write\\nJ(w) = f(w) − g(w), (3.133)\\nwhere f and g are convex functions. Clearly, J is not convex, but there\\nexists a reasonably simple algorithm namely the Concave-Convex Procedure\\n(CCP) for ﬁnding a local minima of J. The basic idea is simple: In the\\ntth iteration replace g by its ﬁrst order Taylor expansion at wt, that is,\\ng(wt) + ⟨w − wt, ∇g(wt)⟩ and minimize\\nJt(w) = f(w) − g(wt) − ⟨w − wt, ∇g(wt)⟩ . (3.134)\\nTaking gradients and setting it to zero shows that Jt is minimized by setting\\n∇f(wt+1) = ∇g(wt). (3.135)\\nThe iterations of CCP on a toy minimization problem is illustrated in Figure\\n3.13, while the complete algorithm listing can be found in Algorithm 3.9.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='24737abb-fd87-48cf-b1fc-a84c5188d9db', embedding=None, metadata={'page_label': '138', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='138 3 Optimization\\n1.0\\n 1.5\\n 2.0\\n 2.5\\n 3.0\\n 3.5\\n 4.0\\n80\\n70\\n60\\n50\\n40\\n30\\n20\\n10\\n1.0\\n 1.5\\n 2.0\\n 2.5\\n 3.0\\n 3.5\\n 4.0\\n50\\n0\\n50\\n100\\n150\\n200\\nFig. 3.13. Given the function on the left we decompose it into the diﬀerence of two\\nconvex functions depicted on the right panel. The CCP algorithm generates iterates\\nby matching points on the two convex curves which have the same tangent vectors.\\nAs can be seen, the iterates approach the solution x = 2.0.\\nAlgorithm 3.9 Concave-Convex Procedure\\n1: Input: Initial point w0, maximum iterations T , convex functions f,g\\n2: Set t = 0\\n3: while t < T do\\n4: Set wt+1 = argminw f(w) − g(wt) − ⟨w − wt, ∇g(wt)⟩\\n5: t = t + 1\\n6: end while\\n7: Return: wT\\nTheorem 3.24 Let J be a function which can be decomposed into a diﬀer-\\nence of two convex functions e.g., (3.133). The iterates generated by (3.135)\\nmonotically decrease J. Furthermore, the stationary point of the iterates is\\na local minima of J.\\nProof Since f and g are convex\\nf(wt) ≥ f(wt+1) + ⟨wt − wt+1, ∇f(wt+1)⟩\\ng(wt+1) ≥ g(wt) + ⟨wt+1 − wt, ∇g(wt)⟩ .\\nAdding the two inequalities, rearranging, and using (3.135) shows thatJ(wt) =\\nf(wt) − g(wt) ≥ f(wt+1) − g(wt+1) = J(wt+1), as claimed.\\nLet w∗ be a stationary point of the iterates. Then ∇f(w∗) = ∇g(w∗),\\nwhich in turn implies that w∗ is a local minima of J because ∇J(w∗) = 0.\\nThere are a number of extensions to CCP. We mention only a few in the\\npassing. First, it can be shown that all instances of the EM algorithm (Sec-\\ntion ??) can be shown to be special cases of CCP. Second, the rate of con-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='50fc2d8a-e949-497a-b706-ce7794d53a52', embedding=None, metadata={'page_label': '139', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6 Some Practical Advice 139\\nvergence of CCP is related to the eigenvalues of the positive semi-deﬁnite\\nmatrix ∇2(f + g). Third, CCP can also be extended to solve constrained\\nproblems of the form:\\nmin\\nw\\nf0(w) − g0(w)\\ns.t. fi(w) − gi(w) ≤ ci for i = 1, . . . , n.\\nwhere, as before, fi and gi for i = 0, 1, . . . , n are assumed convex. At every\\niteration, we replace gi by its ﬁrst order Taylor approximation and solve the\\nfollowing constrained convex problem:\\nmin\\nw\\nf0(w) − g0(wt) + ⟨w − wt, ∇g0(wt)⟩\\ns.t. fi(w) − gi(wt) + ⟨w − wt, ∇gi(wt)⟩ ≤ ci for i = 1, . . . , n.\\n3.6 Some Practical Advice\\nThe range of optimization algorithms we presented in this chapter might be\\nsomewhat intimidating for the beginner. Some simple rules of thumb can\\nalleviate this anxiety\\nCode Reuse: Implementing an eﬃcient optimization algorithm correctly\\nis both time consuming and error prone. Therefore, as far as possible use\\nexisting libraries. A number of high class optimization libraries both com-\\nmercial and open source exist.\\nUnconstrained Problems: For unconstrained minimization of a smooth\\nconvex function LBFGS (Section 3.2.6.1 is the algorithm of choice. In many\\npractical situations the spectral gradient method (Section 3.2.6.2) is also\\nvery competitive. It also has the added advantage of being easy to imple-\\nment. If the function to be minimized is non-smooth then Bundle methods\\n(Section 3.2.7) are to be preferred. Amongst the diﬀerent formulations, the\\nBundle Trust algorithm tends to be quite robust.\\nConstrained Problems: For constrained problems it is very important\\nto understand the nature of the constraints. Simple equality ( Ax = b) and\\nbox ( l ≤ x ≤ u) constraints are easier to handle than general non-linear\\nconstraints. If the objective function is smooth, the constraint set Ω is simple,\\nand orthogonal projections PΩ are easy to compute, then spectral projected\\ngradient (Section 3.3.1) is the method of choice. If the optimization problem\\nis a QP or an LP then specialized solvers tend to be much faster than general\\npurpose solvers.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d3f6e95b-ce43-413d-9662-6825afe94018', embedding=None, metadata={'page_label': '140', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='140 3 Optimization\\nLarge Scale Problems: If your parameter vector is high dimensional then\\nconsider coordinate descent (Section 3.2.2) especially if the one dimensional\\nline search along a coordinate can be carried out eﬃciently. If the objective\\nfunction is made up of a summation of large number of terms, consider\\nstochastic gradient descent (Section 3.4.1). Although both these algorithms\\ndo not guarantee a very accurate solution, practical experience shows that\\nfor large scale machine learning problems this is rarely necessary.\\nDuality: Sometimes problems which are hard to optimize in the primal\\nmay become simpler in the dual. For instance, if the objective function is\\nstrongly convex but non-smooth, its Fenchel conjugate is smooth with a\\nLipschitz continuous gradient.\\nProblems\\nProblem 3.1 (Intersection of Convex Sets {1}) If C1 and C2 are con-\\nvex sets, then show that C1 ∩ C2 is also convex. Extend your result to show\\nthat ⋂n\\ni=1 Ci are convex if Ci are convex.\\nProblem 3.2 (Linear Transform of Convex Sets {1}) Given a set C ⊂\\nRn and a linear transform A ∈ Rm×n, deﬁne AC := {y = Ax : x ∈ C}. If\\nC is convex then show that AC is also convex.\\nProblem 3.3 (Convex Combinations {1}) Show that a subset of Rn is\\nconvex if and only if it contains all the convex combination of its elements.\\nProblem 3.4 (Convex Hull {2}) Show that the convex hull, conv(X) is\\nthe smallest convex set which contains X.\\nProblem 3.5 (Epigraph of a Convex Function {2}) Show that a func-\\ntion satisﬁes Deﬁnition 3.3 if, and only if, its epigraph is convex.\\nProblem 3.6 Prove the Jensen’s inequality (3.6).\\nProblem 3.7 (Strong convexity of the negative entropy {3}) Show that\\nthe negative entropy (3.15) is 1-strongly convex with respect to the ∥·∥1 norm\\non the simplex. Hint: First show that φ(t) := ( t − 1) logt − 2 (t−1)2\\nt+1 ≥ 0 for\\nall t ≥ 0. Next substitute t = xi/yi to show that\\n∑\\ni\\n(xi − yi) log xi\\nyi\\n≥ ∥x − y∥2\\n1 .', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7e2b04a0-d84e-4aa4-b2d6-ff345988d094', embedding=None, metadata={'page_label': '141', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.6 Some Practical Advice 141\\nProblem 3.8 (Strongly Convex Functions {2}) Prove 3.16, 3.17, 3.18\\nand 3.19.\\nProblem 3.9 (Convex Functions with Lipschitz Continuous Gradient {2})\\nProve 3.22, 3.23, 3.24 and 3.25.\\nProblem 3.10 (One Dimensional Projection {1}) If f : Rd → R is\\nconvex, then show that for an arbitrary x and p in Rd the one dimensional\\nfunction Φ(η) := f(x + ηp) is also convex.\\nProblem 3.11 (Quasi-Convex Functions {2}) In Section 3.1 we showed\\nthat the below-sets of a convex function Xc := {x | f(x) ≤ c} are convex. Give\\na counter-example to show that the converse is not true, that is, there exist\\nnon-convex functions whose below-sets are convex. This class of functions is\\ncalled Quasi-Convex.\\nProblem 3.12 (Gradient of the p-norm {1}) Show that the gradient of\\nthe p-norm (3.31) is given by (3.32).\\nProblem 3.13 Derive the Fenchel conjugate of the following functions\\nf(x) =\\n{\\n0 if x ∈ C\\n∞ otherwise.\\nwhere C is a convex set\\nf(x) = ax + b\\nf(x) = 1\\n2 x⊤Ax where A is a positive deﬁnite matrix\\nf(x) = − log(x)\\nf(x) = exp(x)\\nf(x) = x log(x)\\nProblem 3.14 (Convergence of gradient descent {2}) Suppose J has\\na Lipschitz continuous gradient with modulus L. Then show that Algorithm\\n3.2 with an inexact line search satisfying the Wolfe conditions (3.42) and\\n(3.43) will return a solution wt with ∥∇J(wt)∥ ≤ ϵ in at most O(1/ϵ2) iter-\\nations.\\nProblem 3.15 Show that\\n1 + ∑T\\nt=1\\n1\\nt∑T\\nt=1\\n1√\\nt\\n≤ 1√\\nT', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a63f704d-b7b4-4253-b8e7-ef7b3824c6fb', embedding=None, metadata={'page_label': '142', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='142 3 Optimization\\nProblem 3.16 (Coordinate Descent for Quadratic Programming {2})\\nDerive a projection based method which uses coordinate descent to generate\\ndirections of descent for solving the following box constrained QP:\\nmin\\nw∈Rn\\n1\\n2 w⊤Qw + c⊤w\\ns.t. l ≤ w ≤ u.\\nYou may assume that Q is positive deﬁnite and l and u are scalars.\\nProblem 3.17 (Dual of a LP {1}) Show that the dual of the LP (3.122)\\nis (3.115). In other words, we recover the primal by computing the dual of\\nthe dual.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8e79b474-b51d-4653-8534-52a23bdbce1c', embedding=None, metadata={'page_label': '143', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4\\nOnline Learning and Boosting\\nSo far the learning algorithms we considered assumed that all the training\\ndata is available before building a model for predicting labels on unseen data\\npoints. In many modern applications data is available only in a streaming\\nfashion, and one needs to predict labels on the ﬂy. To describe a concrete\\nexample, consider the task of spam ﬁltering. As emails arrive the learning\\nalgorithm needs to classify them as spam or ham. Tasks such as these are\\ntackled via online learning. Online learning proceeds in rounds. At each\\nround a training example is revealed to the learning algorithm, which uses\\nits current model to predict the label. The true label is then revealed to\\nthe learner which incurs a loss and updates its model based on the feedback\\nprovided. This protocol is summarized in Algorithm 4.1. The goal of online\\nlearning is to minimize the total loss incurred. By an appropriate choice\\nof labels and loss functions, this setting encompasses a large number of\\ntasks such as classiﬁcation, regression, and density estimation. In our spam\\ndetection example, if an email is misclassiﬁed the user can provide feedback\\nwhich is used to update the spam ﬁlter, and the goal is to minimize the\\nnumber of misclassiﬁed emails.\\n4.1 Halving Algorithm\\nThe halving algorithm is conceptually simple, yet it illustrates many of the\\nconcepts in online learning. Suppose we have access to a set of n experts,\\nthat is, functions fi which map from the input space X to the output space\\nY = {±1}. Furthermore, assume that one of the experts is consistent, that\\nis, there exists a j ∈ {1, . . . , n} such that fj(xt) = yt for t = 1, . . . , T. The\\nhalving algorithm maintains a set Ct of consistent experts at time t. Initially\\nC0 = {1, . . . , n}, and it is updated recursively as\\nCt+1 = {i ∈ Ct s.t. fi(xt+1) = yt+1} . (4.1)\\nThe prediction on a new data point is computed via a majority vote amongst\\nthe consistent experts: ˆyt = majority(Ct).\\nLemma 4.1 The Halving algorithm makes at most log2(n) mistakes.\\n143', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c01beb3b-007a-442a-948e-2237cc5d876c', embedding=None, metadata={'page_label': '144', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='144 4 Online Learning and Boosting\\nAlgorithm 4.1 Protocol of Online Learning\\n1: for t = 1, . . . , T do do\\n2: Get training instance xt\\n3: Predict label ˆyt\\n4: Get true label yt\\n5: Incur loss l(ˆyt, xt, yt)\\n6: Update model\\n7: end for\\nProof Let M denote the total number of mistakes. The halving algorithm\\nmakes a mistake at iterationt if at least half the consistent expertsCt predict\\nthe wrong label. This in turn implies that\\n|Ct+1| ≤ |Ct|\\n2 ≤ |C0|\\n2M = n\\n2M .\\nOn the other hand, since one of the experts is consistent it follows that\\n1 ≤ |Ct+1|. Therefore, 2 M ≤ n. Solving for M completes the proof.\\n4.2 Weighted Majority\\nWe now turn to the scenario where none of the experts is consistent. There-\\nfore, the aim here is not to minimize the number mistakes but to minimize\\nregret.\\nIn this chapter we will consider online methods for solving the following\\noptimization problem:\\nmin\\nw∈Ω\\nJ(w) where J(w) =\\nT∑\\nt=1\\nft(w). (4.2)\\nSuppose we have access to a function ψ which is continuously diﬀerentiable\\nand strongly convex with modulus of strong convexity σ > 0 (see Section\\n3.1.4 for deﬁnition of strong convexity), then we can deﬁne the Bregman\\ndivergence (3.29) corresponding to ψ as\\n∆ψ(w, w′) = ψ(w) − ψ(w′) −\\n⟨\\nw − w′, ∇ψ(w′)\\n⟩\\n.\\nWe can also generalize the orthogonal projection (3.104) by replacing the\\nsquare Euclidean norm with the above Bregman divergence:\\nPψ,Ω(w′) = argmin\\nw∈Ω\\n∆ψ(w, w′). (4.3)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d04afe03-5de5-4543-8d21-a4e6db9b61cb', embedding=None, metadata={'page_label': '145', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Weighted Majority 145\\nAlgorithm 4.2 Stochastic (sub)gradient Descent\\n1: Input: Initial point x1, maximum iterations T\\n2: for t = 1, . . . , T do\\n3: Compute ˆwt+1 = ∇ψ∗ (∇ψ(wt) − ηtgt) with gt = ∂wft(wt)\\n4: Set wt+1 = Pψ,Ω ( ˆwt+1)\\n5: end for\\n6: Return: wT +1\\nDenote w∗ = Pψ,Ω(w′). Just like the Euclidean distance is non-expansive, the\\nBregman projection can also be shown to be non-expansive in the following\\nsense:\\n∆ψ(w, w′) ≥ ∆ψ(w, w∗) + ∆ψ(w∗, w′) (4.4)\\nfor all w ∈ Ω. The diameter of Ω as measured by ∆ ψ is given by\\ndiamψ(Ω) = max\\nw,w′∈Ω\\n∆ψ(w, w′). (4.5)\\nFor the rest of this chapter we will make the following standard assumptions:\\n• Each ft is convex and revealed at time instance t.\\n• Ω is a closed convex subset of Rn with non-empty interior.\\n• The diameter diam ψ(Ω) of Ω is bounded by F < ∞.\\n• The set of optimal solutions of (4.2) denoted by Ω ∗ is non-empty.\\n• The subgradient ∂wft(w) can be computed for every t and w ∈ Ω.\\n• The Bregman projection (4.3) can be computed for every w′ ∈ Rn.\\n• The gradient ∇ψ, and its inverse ( ∇ψ)−1 = ∇ψ∗ can be computed.\\nThe method we employ to solve (4.2) is given in Algorithm 4.2. Before\\nanalyzing the performance of the algorithm we would like to discuss three\\nspecial cases. First, Euclidean distance squared which recovers projected\\nstochastic gradient descent, second Entropy which recovers Exponentiated\\ngradient descent, and third the p-norms for p > 2 which recovers the p-norm\\nPerceptron. BUGBUG TODO.\\nOur key result is Lemma 4.3 given below. It can be found in various guises\\nin diﬀerent places most notably Lemma 2.1 and 2.2 in [ ?], Theorem 4.1 and\\nEq. (4.21) and (4.15) in [?], in the proof of Theorem 1 of [?], as well as Lemma\\n3 of [ ?]. We prove a slightly general variant; we allow for projections with\\nan arbitrary Bregman divergence and also take into account a generalized\\nversion of strong convexity of ft. Both these modiﬁcations will allow us to\\ndeal with general settings within a uniﬁed framework.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4561a384-ddb1-4718-a43f-a2cfc6e47c7e', embedding=None, metadata={'page_label': '146', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='146 4 Online Learning and Boosting\\nDeﬁnition 4.2 We say that a convex function f is strongly convex with\\nrespect to another convex function ψ with modulus λ if\\nf(w) − f(w′) −\\n⟨\\nw − w′, µ\\n⟩\\n≥ λ∆ψ(w, w′) for all µ ∈ ∂f (w′). (4.6)\\nThe usual notion of strong convexity is recovered by setting ψ(·) = 1\\n2 ∥·∥2.\\nLemma 4.3 Let ft be strongly convex with respect to ψ with modulus λ ≥ 0\\nfor all t. For any w ∈ Ω the sequences generated by Algorithm 4.2 satisfy\\n∆ψ(w, wt+1) ≤ ∆ψ(w, wt) − ηt ⟨gt, wt − w⟩ + η2\\nt\\n2σ ∥gt∥2 (4.7)\\n≤ (1 − ηtλ)∆ψ(w, wt) − ηt(ft(wt) − ft(w)) + η2\\nt\\n2σ ∥gt∥2 . (4.8)\\nProof We prove the result in three steps. First we upper bound ∆ψ(w, wt+1)\\nby ∆ψ(w, ˆwt+1). This is a consequence of (4.4) and the non-negativity of the\\nBregman divergence which allows us to write\\n∆ψ(w, wt+1) ≤ ∆ψ(w, ˆwt+1). (4.9)\\nIn the next step we use Lemma 3.11 to write\\n∆ψ(w, wt) + ∆ψ(wt, ˆwt+1) − ∆ψ(w, ˆwt+1) = ⟨∇ψ( ˆwt+1) − ∇ψ(wt), w − wt⟩ .\\nSince ∇ψ∗ = (∇ψ)−1, the update in step 3 of Algorithm 4.2 can equivalently\\nbe written as ∇ψ( ˆwt+1) − ∇ ψ(wt) = −ηtgt. Plugging this in the above\\nequation and rearranging\\n∆ψ(w, ˆwt+1) = ∆ψ(w, wt) − ηt ⟨gt, wt − w⟩ + ∆ψ(wt, ˆwt+1). (4.10)\\nFinally we upper bound ∆ ψ(wt, ˆwt+1). For this we need two observations:\\nFirst, ⟨x, y⟩ ≤ 1\\n2σ ∥x∥2 + σ\\n2 ∥y∥2 for all x, y ∈ Rn and σ > 0. Second, the σ\\nstrong convexity of ψ allows us to bound ∆ ψ( ˆwt+1, wt) ≥ σ\\n2 ∥wt − ˆwt+1∥2.\\nUsing these two observations\\n∆ψ(wt, ˆwt+1) = ψ(wt) − ψ( ˆwt+1) − ⟨∇ψ( ˆwt+1), wt − ˆwt+1⟩\\n= −(ψ( ˆwt+1) − ψ(wt) − ⟨∇ψ(wt), ˆwt+1 − wt⟩) + ⟨ηtgt, wt − ˆwt+1⟩\\n= −∆ψ( ˆwt+1, wt) + ⟨ηtgt, wt − ˆwt+1⟩\\n≤ − σ\\n2 ∥wt − ˆwt+1∥2 + η2\\nt\\n2σ ∥gt∥2 + σ\\n2 ∥wt − ˆwt+1∥2\\n= η2\\nt\\n2σ ∥gt∥2 . (4.11)\\nInequality (4.7) follows by putting together (4.9), (4.10), and (4.11), while\\n(4.8) follows by using (4.6) with f = ft and w′ = wt and substituting into', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='447f9781-0bcc-4257-8ee6-5de42ac263b7', embedding=None, metadata={'page_label': '147', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.2 Weighted Majority 147\\n(4.7).\\nNow we are ready to prove regret bounds.\\nLemma 4.4 Let w∗ ∈ Ω∗ denote the best parameter chosen in hindsight,\\nand let ∥gt∥ ≤ L for all t. Then the regret of Algorithm 4.2 can be bounded\\nvia\\nT∑\\nt=1\\nft(wt) − ft(w∗) ≤ F\\n( 1\\nηT\\n− T λ\\n)\\n+ L2\\n2σ\\nT∑\\nt=1\\nηt. (4.12)\\nProof Set w = w∗ and rearrange (4.8) to obtain\\nft(wt) − ft(w∗) ≤ 1\\nηt\\n((1 − ληt)∆ψ(w∗, wt) − ∆ψ(w∗, wt+1)) + ηt\\n2σ ∥gt∥2 .\\nSumming over t\\nT∑\\nt=1\\nft(wt) − ft(w∗) ≤\\nT∑\\nt=1\\n1\\nηt\\n((1 − ηtλ)∆ψ(w∗, wt) − ∆ψ(w∗, wt+1))\\n\\ued19 \\ued18\\ued17 \\ued1a\\nT1\\n+\\nT∑\\nt=1\\nηt\\n2σ ∥gt∥2\\n\\ued19 \\ued18\\ued17 \\ued1a\\nT2\\n.\\nSince the diameter of Ω is bounded by F and ∆ψ is non-negative\\nT1 =\\n( 1\\nη1\\n− λ\\n)\\n∆ψ(w∗, w1) − 1\\nηT\\n∆ψ(w∗, wT +1) +\\nT∑\\nt=2\\n∆ψ(w∗, wt)\\n( 1\\nηt\\n− 1\\nηt−1\\n− λ\\n)\\n≤\\n( 1\\nη1\\n− λ\\n)\\n∆ψ(w∗, w1) +\\nT∑\\nt=2\\n∆ψ(w∗, wt)\\n( 1\\nηt\\n− 1\\nηt−1\\n− λ\\n)\\n≤\\n( 1\\nη1\\n− λ\\n)\\nF +\\nT∑\\nt=2\\nF\\n( 1\\nηt\\n− 1\\nηt−1\\n− λ\\n)\\n= F\\n( 1\\nηT\\n− T λ\\n)\\n.\\nOn the other hand, since the subgradients are Lipschitz continuous with\\nconstant L it follows that\\nT2 ≤ L2\\n2σ\\nT∑\\nt=1\\nηt.\\nPutting together the bounds for T1 and T2 yields (4.12).\\nCorollary 4.5 If λ > 0 and we set ηt = 1\\nλt then\\nT∑\\nt=1\\nft(xt) − ft(x∗) ≤ L2\\n2σλ (1 + log(T )),', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8f738a4b-0923-40ce-9323-e3e054965c0f', embedding=None, metadata={'page_label': '148', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='148 4 Online Learning and Boosting\\nOn the other hand, when λ = 0, if we set ηt = 1√\\nt then\\nT∑\\nt=1\\nft(xt) − ft(x∗) ≤\\n(\\nF + L2\\nσ\\n)√\\nT .\\nProof First consider λ > 0 with ηt = 1\\nλt. In this case 1\\nηT\\n= T λ, and\\nconsequently (4.12) specializes to\\nT∑\\nt=1\\nft(wt) − ft(w∗) ≤ L2\\n2σλ\\nT∑\\nt=1\\n1\\nt ≤ L2\\n2σλ (1 + log(T )).\\nWhen λ = 0, and we set ηt = 1√\\nt and use problem 4.2 to rewrite (4.12) as\\nT∑\\nt=1\\nft(wt) − ft(w∗) ≤ F\\n√\\nT + L2\\nσ\\nT∑\\nt=1\\n1\\n2\\n√\\nt ≤ F\\n√\\nT + L2\\nσ\\n√\\nT .\\nProblems\\nProblem 4.1 (Generalized Cauchy-Schwartz {1}) Show that ⟨x, y⟩ ≤\\n1\\n2σ ∥x∥2 + σ\\n2 ∥y∥2 for all x, y ∈ Rn and σ > 0.\\nProblem 4.2 (Bounding sum of a series {1}) Show that ∑b\\nt=a\\n1\\n2\\n√\\nt ≤√\\nb − a + 1. Hint: Upper bound the sum by an integral.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2425f42a-0f41-4157-975a-9ebfaa8016b1', embedding=None, metadata={'page_label': '149', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5\\nConditional Densities\\nA number of machine learning algorithms can be derived by using condi-\\ntional exponential families of distribution (Section 2.3). Assume that the\\ntraining set {(x1, y1), . . . ,(xm, ym)} was drawn iid from some underlying\\ndistribution. Using Bayes rule (1.15) one can write the likelihood\\np(θ|X, Y ) ∝ p(θ)p(Y |X, θ) = p(θ)\\nm∏\\ni=1\\np(yi|xi, θ), (5.1)\\nand hence the negative log-likelihood\\n− log p(θ|X, Y ) = −\\nm∑\\ni=1\\nlog p(yi|xi, θ) − log p(θ) + const. (5.2)\\nBecause we do not have any prior knowledge about the data, we choose a\\nzero mean unit variance isotropic normal distribution for p(θ). This yields\\n− log p(θ|X, Y ) = 1\\n2 ∥θ∥2 −\\nm∑\\ni=1\\nlog p(yi|xi, θ) + const. (5.3)\\nFinally, if we assume a conditional exponential family model for p(y|x, θ),\\nthat is,\\np(y|x, θ) = exp (⟨φ(x, y), θ⟩ − g(θ|x)) , (5.4)\\nthen\\n− log p(θ|X, Y ) = 1\\n2 ∥θ∥2 +\\nm∑\\ni=1\\ng(θ|xi) − ⟨φ(xi, yi), θ⟩ + const. (5.5)\\nwhere\\ng(θ|x) = log\\n∑\\ny∈Y\\nexp (⟨φ(x, y), θ⟩) , (5.6)\\nis the log-partition function. Clearly, (5.5) is a smooth convex objective\\nfunction, and algorithms for unconstrained minimization from Chapter 3\\n149', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='adaf0e50-8e87-48eb-afe6-523c20d4f18a', embedding=None, metadata={'page_label': '150', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='150 5 Conditional Densities\\ncan be used to obtain the maximum aposteriori (MAP) estimate forθ. Given\\nthe optimal θ, the class label at any given x can be predicted using\\ny∗ = argmax\\ny\\np(y|x, θ). (5.7)\\nIn this chapter we will discuss a number of these algorithms that can be\\nderived by specializing the above setup. Our discussion uniﬁes seemingly\\ndisparate algorithms, which are often discussed separately in literature.\\n5.1 Logistic Regression\\nWe begin with the simplest case namely binary classiﬁcation 1. The key ob-\\nservation here is that the labels y ∈ {±1} and hence\\ng(θ|x) = log (exp (⟨φ(x, +1), θ⟩) + exp (⟨φ(x, −1), θ⟩)) . (5.8)\\nDeﬁne ˆφ(x) := φ(x, +1) − φ(x, −1). Plugging (5.8) into (5.4), using the\\ndeﬁnition of ˆφ and rearranging\\np(y = +1|x, θ) = 1\\n1 + exp\\n(⣨\\n−ˆφ(x), θ\\n⟩) and\\np(y = −1|x, θ) = 1\\n1 + exp\\n(⣨\\nˆφ(x), θ\\n⟩),\\nor more compactly\\np(y|x, θ) = 1\\n1 + exp\\n(⣨\\n−y ˆφ(x), θ\\n⟩). (5.9)\\nSince p(y|x, θ) is a logistic function, hence the name logistic regression. The\\nclassiﬁcation rule (5.7) in this case specializes as follows: predict +1 when-\\never p(y = +1|x, θ) ≥ p(y = −1|x, θ) otherwise predict −1. However\\nlog p(y = +1|x, θ)\\np(y = −1|x, θ) =\\n⣨\\nˆφ(x), θ\\n⟩\\n,\\ntherefore one can equivalently use sign\\n(⣨\\nˆφ(x), θ\\n⟩)\\nas our prediction func-\\ntion. Using (5.9) we can write the objective function of logistic regression\\nas\\n1\\n2 ∥θ∥2 +\\nm∑\\ni=1\\nlog\\n(\\n1 + exp\\n(⣨\\n−yi ˆφ(xi), θ\\n⟩))\\n1 The name logistic regression is a misnomer!', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='02018506-d62c-4766-8879-457504a5825b', embedding=None, metadata={'page_label': '151', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.2 Regression 151\\nTo minimize the above objective function we ﬁrst compute the gradient.\\n∇J(θ) = θ +\\nm∑\\ni=1\\nexp\\n(⣨\\n−yi ˆφ(xi), θ\\n⟩)\\n1 + exp\\n(⣨\\n−yi ˆφ(xi), θ\\n⟩)(−yi ˆφ(xi))\\n= θ +\\nm∑\\ni=1\\n(p(yi|xi, θ) − 1)yi ˆφ(xi).\\nNotice that the second term of the gradient vanishes whenever p(yi|xi, θ) =\\n1. Therefore, one way to interpret logistic regression is to view it as a method\\nto maximize p(yi|xi, θ) for each point ( xi, yi) in the training set. Since the\\nobjective function of logistic regression is twice diﬀerentiable one can also\\ncompute its Hessian\\n∇2J(θ) = I −\\nm∑\\ni=1\\np(yi|xi, θ)(1 − p(yi|xi, θ))ˆφ(xi)ˆφ(xi)⊤,\\nwhere we used y2\\ni = 1. The Hessian can be used in the Newton method\\n(Section 3.2.6) to obtain the optimal parameter θ.\\n5.2 Regression\\n5.2.1 Conditionally Normal Models\\nﬁxed variance\\n5.2.2 Posterior Distribution\\nintegrating out vs. Laplace approximation, eﬃcient estimation (sparse greedy)\\n5.2.3 Heteroscedastic Estimation\\nexplain that we have two parameters. not too many details (do that as an\\nassignment).\\n5.3 Multiclass Classiﬁcation\\n5.3.1 Conditionally Multinomial Models\\njoint feature map', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='797d4ce5-3cc8-4d0a-8989-fb7ff7f65aaf', embedding=None, metadata={'page_label': '152', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='152 5 Conditional Densities\\n5.4 What is a CRF?\\n• Motivation with learning a digit example\\n• general deﬁnition\\n• Gaussian process + structure = CRF\\n5.4.1 Linear Chain CRFs\\n• Graphical model\\n• Applications\\n• Optimization problem\\n5.4.2 Higher Order CRFs\\n• 2-d CRFs and their applications in vision\\n• Skip chain CRFs\\n• Hierarchical CRFs (graph transducers, sutton et. al. JMLR etc)\\n5.4.3 Kernelized CRFs\\n• From feature maps to kernels\\n• The clique decomposition theorem\\n• The representer theorem\\n• Optimization strategies for kernelized CRFs\\n5.5 Optimization Strategies\\n5.5.1 Getting Started\\n• three things needed to optimize\\n– MAP estimate\\n– log-partition function\\n– gradient of log-partition function\\n• Worked out example (linear chain?)\\n5.5.2 Optimization Algorithms\\n- Optimization algorithms (LBFGS, SGD, EG (Globerson et. al))\\n5.5.3 Handling Higher order CRFs\\n- How things can be done for higher order CRFs (brieﬂy)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f690d08c-3de3-469a-a544-3a3defb554c8', embedding=None, metadata={'page_label': '153', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='5.6 Hidden Markov Models 153\\n5.6 Hidden Markov Models\\n• Deﬁnition\\n• Discuss that they are modeling joint distribution p(x, y)\\n• The way they predict is by marginalizing out x\\n• Why they are wasteful and why CRFs generally outperform them\\n5.7 Further Reading\\nWhat we did not talk about:\\n• Details of HMM optimization\\n• CRFs applied to predicting parse trees via matrix tree theorem (collins,\\nkoo et al)\\n• CRFs for graph matching problems\\n• CRFs with Gaussian distributions (yes they exist)\\n5.7.1 Optimization\\nissues in optimization (blows up with number of classes). structure is not\\nthere. can we do better?\\nProblems\\nProblem 5.1 Poisson models\\nProblem 5.2 Bayes Committee Machine\\nProblem 5.3 Newton / CG approach', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='de0cf97e-c9b2-46b2-8c8f-32a99b140e12', embedding=None, metadata={'page_label': '154', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6c5e7e36-c82d-4582-b972-959bb2204c61', embedding=None, metadata={'page_label': '155', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6\\nKernels and Function Spaces\\nKernels are measures of similarity. Broadly speaking, machine learning al-\\ngorithms which rely only on the dot product between instances can be “ker-\\nnelized” by replacing all instances of ⟨x, x′⟩ by a kernel function k(x, x′).\\nWe saw examples of such algorithms in Sections 1.3.3 and 1.3.4 and we will\\nsee many more examples in Chapter 7. Arguably, the design of a good ker-\\nnel underlies the success of machine learning in many applications. In this\\nchapter we will lay the ground for the theoretical properties of kernels and\\npresent a number of examples. Algorithms which use these kernels can be\\nfound in later chapters.\\n6.1 The Basics\\nLet X denote the space of inputs and k : X × X → R be a function which\\nsatisﬁes\\nk(x, x′) = ⟨Φ(x), Φ(x)⟩ (6.1)\\nwhere Φ is a feature map which maps X into some dot product space H. In\\nother words, kernels correspond to dot products in some dot product space.\\nThe main advantage of using a kernel as a similarity measure are threefold:\\nFirst, if the feature space is rich enough, then simple estimators such as\\nhyperplanes and half-spaces may be suﬃcient. For instance, to classify the\\npoints in Figure BUGBUG, we need a nonlinear decision boundary, but\\nonce we map the points to a 3 dimensional space a hyperplane suﬃces.\\nSecond, kernels allow us to construct machine learning algorithms in the\\ndot product space H without explicitly computing Φ(x). Third, we need not\\nmake any assumptions about the input space X other than for it to be a\\nset. As we will see later in this chapter, this allows us to compute similarity\\nbetween discrete objects such as strings, trees, and graphs. In the ﬁrst half\\nof this chapter we will present some examples of kernels, and discuss some\\ntheoretical properties of kernels in the second half.\\n155', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c8cfecfa-e778-4686-8039-bb39ebdcd180', embedding=None, metadata={'page_label': '156', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='156 6 Kernels and Function Spaces\\n6.1.1 Examples\\n6.1.1.1 Linear Kernel\\nLinear kernels are perhaps the simplest of all kernels. We assume thatx ∈ Rn\\nand deﬁne\\nk(x, x′) =\\n⟨\\nx, x′⟩\\n=\\n∑\\ni\\nxix′\\ni.\\nIf x and x′ are dense then computing the kernel takes O(n) time. On the\\nother hand, for sparse vectors this can be reduced to O(|nnz(x) ∩ nnz(x′)|),\\nwhere nnz(·) denotes the set of non-zero indices of a vector and | · | de-\\nnotes the size of a set. Linear kernels are a natural representation to use for\\nvectorial data. They are also widely used in text mining where documents\\nare represented by a vector containing the frequency of occurrence of words\\n(Recall that we encountered this so-called bag of words representation in\\nChapter 1). Instead of a simple bag of words, one can also map a text to the\\nset of pairs of words that co-occur in a sentence for a richer representation.\\n6.1.1.2 Polynomial Kernel\\nGiven x ∈ Rn, we can compute a feature map Φ by taking all the d-th\\norder products (also called the monomials) of the entries of x. To illustrate\\nwith a concrete example, let us consider x = (x1, x2) and d = 2, in which\\ncase Φ( x) =\\n(\\nx2\\n1, x2\\n2, x1x2, x2x1\\n)\\n. Although it is tedious to compute Φ( x)\\nand Φ(x′) explicitly in order to compute k(x, x), there is a shortcut as the\\nfollowing proposition shows.\\nProposition 6.1 Let Φ(x) (resp. Φ(x′)) denote the vector whose entries\\nare all possible d-th degree ordered products of the entries of x (resp. x′).\\nThen\\nk(x, x′) =\\n⟨\\nΦ(x), Φ(x′)\\n⟩\\n=\\n(⟨\\nx, x′⟩)d . (6.2)\\nProof By direct computation\\n⟨\\nΦ(x), Φ(x′)\\n⟩\\n=\\n∑\\nj1\\n. . .\\n∑\\njd\\nxj1 . . . xjd · x′\\nj1 . . . x′\\njd\\n=\\n∑\\nj1\\nxj1 · x′\\nj1 . . .\\n∑\\njd\\nxjd · x′\\njd =\\n\\uf8eb\\n\\uf8ed∑\\nj\\nxj · x′\\nj\\n\\uf8f6\\n\\uf8f8\\nd\\n=\\n(⟨\\nx, x′⟩)d', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='922a52d1-a988-4841-b1d5-94164afae636', embedding=None, metadata={'page_label': '157', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 The Basics 157\\nThe kernel (6.2) is called the polynomial kernel. An useful extension is the\\ninhomogeneous polynomial kernel\\nk(x, x′) =\\n(⟨\\nx, x′⟩\\n+ c\\n)d , (6.3)\\nwhich computes all monomials up to degree d (problem 6.2).\\n6.1.1.3 Radial Basis Function Kernels\\n6.1.1.4 Convolution Kernels\\nThe framework of convolution kernels is a general way to extend the notion\\nof kernels to structured objects such as strings, trees, and graphs. Let x ∈ X\\nbe a discrete object which can be decomposed into P parts xp ∈ Xp in many\\ndiﬀerent ways. As a concrete example consider the string x = abc which can\\nbe split into two sets of substrings of size two namely {a, bc} and {ab, c}.\\nWe denote the set of all such decompositions as R(x), and use it to build a\\nkernel on X as follows:\\n[k1 ⋆ . . . ⋆ kP ] (x, x′) =\\n∑\\n¯x∈R(x),¯x′∈R(x′)\\nP∏\\np=1\\nkp(¯xp, ¯x′\\np). (6.4)\\nHere, the sum is over all possible ways in which we can decompose x and\\nx′ into ¯x1, . . . ,¯xP and ¯x′\\n1, . . . ,¯x′\\nP respectively. If the cardinality of R(x) is\\nﬁnite, then it can be shown that (6.4) results in a valid kernel. Although\\nconvolution kernels provide the abstract framework, speciﬁc instantiations\\nof this idea lead to a rich set of kernels on discrete objects. We will now\\ndiscuss some of them in detail.\\n6.1.1.5 String Kernels\\nThe basic idea behind string kernels is simple: Compare the strings by\\nmeans of the subsequences they contain. More the number of common sub-\\nsequences, the more similar two strings are. The subsequences need not have\\nequal weights. For instance, the weight of a subsequence may be given by the\\ninverse frequency of its occurrence. Similarly, if the ﬁrst and last characters\\nof a subsequence are rather far apart, then its contribution to the kernel\\nmust be down-weighted.\\nFormally, a string x is composed of characters from a ﬁnite alphabet Σ\\nand |x| denotes its length. We say that s is a subsequence of x = x1x2 . . . x|x|\\nif s = xi1xi2 . . . xi|s| for some 1 ≤ i1 < i 2 < . . . < i|s| ≤ | x|. In particular, if\\nii+1 = ii +1 then s is a substring of x. For example, acb is not a subsequence\\nof adbc while abc is a subsequence and adc is a substring. Assume that there\\nexists a function #( x, s) which returns the number of times a subsequence', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='20ebb300-32b8-4ab6-8b78-80bbb89710bd', embedding=None, metadata={'page_label': '158', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='158 6 Kernels and Function Spaces\\ns occurs in x and a non-negative weighting function w(s) ≥ 0 which returns\\nthe weight associated with s. Then the basic string kernel can be written as\\nk(x, x′) =\\n∑\\ns\\n#(x, s) #(x′, s) w(s). (6.5)\\nDiﬀerent string kernels are derived by specializing the above equation:\\nAll substrings kernel: If we restrict the summation in (6.5) to sub-\\nstrings then [VS04] provide a suﬃx tree based algorithm which allows one\\nto compute for arbitrary w(s) the kernel k(x, x′) in O(|x| + |x′|) time and\\nmemory.\\nk-Spectrum kernel: The k-spectrum kernel is obtained by restricting\\nthe summation in (6.5) to substrings of length k. A slightly general variant\\nconsiders all substrings of length up to k. Here k is a tuning parameter\\nwhich is typically set to be a small number ( e.g., 5). A simple trie based\\nalgorithm can be used to compute the k-spectrum kernel in O((|x| + |x′|)k)\\ntime (problem 6.3).\\nInexact substring kernel: Sometimes the input strings might have\\nmeasurement errors and therefore it is desirable to take into account inexact\\nmatches. This is done by replacing #( x, s) in (6.5) by another function\\n#(x, s, ϵ) which reports the number of approximate matches of s in x. Here\\nϵ denotes the number of mismatches allowed, typically a small number (e.g.,\\n3). By trading oﬀ computational complexity with storage the kernel can be\\ncomputed eﬃciently. See [LK03] for details.\\nMismatch kernel: Instead of simply counting the number of occurrences\\nof a substring if we use a weighting scheme which down-weights the contribu-\\ntions of longer subsequences then this yields the so-called mismatch kernel.\\nGiven an index sequence I = (i1, . . . , ik) with 1 ≤ i1 < i 2 < . . . < i k ≤ | x|\\nwe can associate the subsequence x(I) = xi1xi2 . . . xik with I. Furthermore,\\ndeﬁne |I| = ik − i1 + 1. Clearly, |I| > k if I is not contiguous. Let λ ≤ 1 be\\na decay factor. Redeﬁne\\n#(x, s) =\\n∑\\ns=x(I)\\nλ|I|, (6.6)\\nthat is, we count all occurrences of s in x but now the weight associated with\\na subsequence depends on its length. To illustrate, consider the subsequence\\nabc which occurs in the string abcebc twice, namely, abcebc and abcebc. The\\nﬁrst occurrence is counted with weight λ3 while the second occurrence is\\ncounted with the weight λ6. As it turns out, this kernel can be computed\\nby a dynamic programming algorithm (problem BUGBUG) in O(|x| · | x′|)\\ntime.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b3ada696-5db5-4cb7-9d76-29b089cdfa5f', embedding=None, metadata={'page_label': '159', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.1 The Basics 159\\n6.1.1.6 Graph Kernels\\nThere are two diﬀerent notions of graph kernels. First, kernels on graphs\\nare used to compare nodes of a single graph. In contrast, kernels between\\ngraphs focus on comparing two graphs. A random walk (or its continuous\\ntime limit, diﬀusion) underlie both types of kernels. The basic intuition is\\nthat two nodes are similar if there are a number of paths which connect\\nthem while two graphs are similar if they share many common paths. To\\ndescribe these kernels formally we need to introduce some notation.\\nA graph G consists of an ordered set of n vertices V = {v1, v2, . . . , vn},\\nand a set of directed edges E ⊂ V ×V . A vertex vi is said to be a neighbor\\nof another vertex vj if they are connected by an edge, i.e., if ( vi, vj) ∈ E;\\nthis is also denoted vi ∼ vj. The adjacency matrix of a graph is the n × n\\nmatrix A with Aij = 1 if vi ∼ vj, and 0 otherwise. A walk of length k on G\\nis a sequence of indices i0, i1, . . . ik such that vir−1 ∼ vir for all 1 ≤ r ≤ k.\\nThe adjacency matrix has a normalized cousin, deﬁned ˜A := D−1A, which\\nhas the property that each of its rows sums to one, and it can therefore\\nserve as the transition matrix for a stochastic process. Here, D is a diag-\\nonal matrix of node degrees, i.e., Dii = di = ∑\\nj Aij. A random walk on\\nG is a process generating sequences of vertices vi1, vi2, vi3, . . . according to\\nP(ik+1|i1, . . . ik) = ˜Aik,ik+1. The tth power of ˜A thus describes t-length walks,\\ni.e., ( ˜At)ij is the probability of a transition from vertex vj to vertex vi via\\na walk of length t (problem BUGBUG). If p0 is an initial probability dis-\\ntribution over vertices, then the probability distribution pt describing the\\nlocation of our random walker at time t is pt = ˜Atp0. The jth component of\\npt denotes the probability of ﬁnishing a t-length walk at vertex vj. A random\\nwalk need not continue indeﬁnitely; to model this, we associate every node\\nvik in the graph with a stopping probability qik. The overall probability of\\nstopping after t steps is given by q⊤pt.\\nGiven two graphs G(V, E) and G′(V′, E′), their direct product G× is a\\ngraph with vertex set\\nV× = {(vi, v′\\nr) : vi ∈ V, v ′\\nr ∈ V′}, (6.7)\\nand edge set\\nE× = {((vi, v′\\nr), (vj, v′\\ns)) : ( vi, vj) ∈ E ∧ (v′\\nr, v′\\ns) ∈ E′}. (6.8)\\nIn other words, G× is a graph over pairs of vertices from G and G′, and\\ntwo vertices in G× are neighbors if and only if the corresponding vertices\\nin G and G′ are both neighbors; see Figure 6.1 for an illustration. If A and\\nA′ are the respective adjacency matrices of G and G′, then the adjacency', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='19247819-ec48-47b9-abf6-a852977c31a1', embedding=None, metadata={'page_label': '160', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='160 6 Kernels and Function Spaces\\nG1\\n1\\n2\\n3\\nG2\\n1’ 2’\\n3’4’\\nG×\\n11’ 21’ 31’\\n34’ 12’\\n24’ 22’\\n14’ 32’\\n33’ 23’ 13’\\nFig. 6.1. Two graphs ( G1 & G2) and their direct product ( G×). Each node of the\\ndirect product graph is labeled with a pair of nodes (6.7); an edge exists in the\\ndirect product if and only if the corresponding nodes are adjacent in both original\\ngraphs (6.8). For instance, nodes 11′ and 32′ are adjacent because there is an edge\\nbetween nodes 1 and 3 in the ﬁrst, and 1 ′ and 2′ in the second graph.\\nmatrix of G× is A× = A ⊗ A′. Similarly, ˜A× = ˜A ⊗ ˜A′. Performing a random\\nwalk on the direct product graph is equivalent to performing a simultaneous\\nrandom walk on G and G′. If p and p′ denote initial probability distributions\\nover the vertices of G and G′, then the corresponding initial probability\\ndistribution on the direct product graph is p× := p ⊗ p′. Likewise, if q and\\nq′ are stopping probabilities (that is, the probability that a random walk\\nends at a given vertex), then the stopping probability on the direct product\\ngraph is q× := q ⊗ q′.\\nTo deﬁne a kernel which computes the similarity between G and G′, one\\nnatural idea is to simply sum up q⊤\\n× ˜At\\n×p× for all values of t. However, this\\nsum might not converge, leaving the kernel value undeﬁned. To overcome\\nthis problem, we introduce appropriately chosen non-negative coeﬃcients\\nµ(t), and deﬁne the kernel between G and G′ as\\nk(G, G′) :=\\n∞∑\\nt=0\\nµ(t) q⊤\\n× ˜At\\n×p×. (6.9)\\nThis idea can be extended to graphs whose nodes are associated with labels\\nby replacing the matrix ˜A× with a matrix of label similarities. For appro-\\npriate choices of µ(t) the above sum converges and eﬃcient algorithms for\\ncomputing the kernel can be devised. See [ ?] for details.\\nAs it turns out, the simple idea of performing a random walk on the prod-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9b5e04dd-8698-4089-a28f-d55dbc6ca992', embedding=None, metadata={'page_label': '161', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.2 Kernels 161\\nuct graph can be extended to compute kernels on Auto Regressive Moving\\nAverage (ARMA) models [VSV07]. Similarly, it can also be used to deﬁne\\nkernels between transducers. Connections between the so-called rational ker-\\nnels on transducers and the graph kernels deﬁned via (6.9) are made explicit\\nin [?].\\n6.2 Kernels\\n6.2.1 Feature Maps\\ngive examples, linear classiﬁer, nonlinear ones with r2-r3 map\\n6.2.2 The Kernel Trick\\n6.2.3 Examples of Kernels\\ngaussian, polynomial, linear, texts, graphs\\n- stress the fact that there is a diﬀerence between structure in the input\\nspace and structure in the output space\\n6.3 Algorithms\\n6.3.1 Kernel Perceptron\\n6.3.2 Trivial Classiﬁer\\n6.3.3 Kernel Principal Component Analysis\\n6.4 Reproducing Kernel Hilbert Spaces\\nAs it turns out, this class of functions coincides with the class of positive\\nsemi-deﬁnite functions. Intuitively, the notion of a positive semi-deﬁnite\\nfunction is an extension of the familiar notion of a positive semi-deﬁnite\\nmatrix (also see Appendix BUGBUG):\\nDeﬁnition 6.2 A real n × n symmetric matrix K satisfying\\n∑\\ni,j\\nαiαjKi,j ≥ 0 (6.10)\\nfor all αi, αj ∈ R is called positive semi-deﬁnite. If equality in (6.10) occurs\\nonly when α1, . . . , αn = 0, then K is said to be positive deﬁnite.\\nDeﬁnition 6.3 Given a set of points x1, . . . , xn ∈ X and a function k, the\\nmatrix\\nKi,j = k(xi, xj) (6.11)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a3d39982-3e08-4df9-9707-efcf67c14774', embedding=None, metadata={'page_label': '162', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='162 6 Kernels and Function Spaces\\nis called the Gram matrix or the kernel matrix of k with respect to x1, . . . , xn.\\nDeﬁnition 6.4 Let X be a nonempty set, k : X × X → R be a function. If\\nk gives rise to a positive (semi-)deﬁnite Gram matrix for all x1, . . . , xn ∈ X\\nand n ∈ N then k is said to be positive (semi-)deﬁnite.\\nClearly, every kernel function k of the form (6.1) is positive semi-deﬁnite.\\nTo see this simply write\\n∑\\ni,j\\nαiαjk(xi, xj) =\\n∑\\ni,j\\nαiαj ⟨xi, xj⟩ =\\n⟨∑\\ni\\nαixi,\\n∑\\nj\\nαjxj\\n⟩\\n≥ 0.\\nWe now establish the converse, that is, we show that every positive semi-\\ndeﬁnite kernel function can be written as (6.1). Towards this end, deﬁne a\\nmap Φ from X into the space of functions mapping X to R (denoted RX) via\\nΦ(x) = k(·, x). In other words, Φ(x) : X → R is a function which assigns the\\nvalue k(x′, x) to x′ ∈ X. Next construct a vector space by taking all possible\\nlinear combinations of Φ( x)\\nf(·) =\\nn∑\\ni=1\\nαiΦ(xi) =\\nn∑\\ni=1\\nαik(·, xi), (6.12)\\nwhere i ∈ N, αi ∈ R, and xi ∈ X are arbitrary. This space can be endowed\\nwith a natural dot product\\n⟨f, g⟩ =\\nn∑\\ni=1\\nn′\\n∑\\nj=1\\nαiβjk(xi, x′\\nj). (6.13)\\nTo see that the above dot product is well deﬁned even though it contains\\nthe expansion coeﬃcients (which need not be unique), note that ⟨f, g⟩ =∑n′\\nj=1 βjf(x′\\nj), independent of αi. Similarly, forg, note that ⟨f, g⟩ = ∑n\\ni=1 αif(xi),\\nthis time independent of βj. This also shows that ⟨f, g⟩ is bilinear. Symme-\\ntry follows because ⟨f, g⟩ = ⟨g, f ⟩, while the positive semi-deﬁniteness of k\\nimplies that\\n⟨f, f ⟩ =\\n∑\\ni,j\\nαiαjk(xi, xj) ≥ 0. (6.14)\\nApplying (6.13) shows that for all functions (6.12) we have\\n⟨f, k(·, x)⟩ = f(x). (6.15)\\nIn particular\\n⟨\\nk(·, x), k(·, x′)\\n⟩\\n= k(x, x′). (6.16)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3f6c1680-45c2-45d9-8ab1-7192010dca2c', embedding=None, metadata={'page_label': '163', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='6.4 Reproducing Kernel Hilbert Spaces 163\\nIn view of these properties, k is called a reproducing kernel. By using (6.15)\\nand the following property of positive semi-deﬁnite functions (problem 6.1)\\nk(x, x′)2 ≤ k(x, x) · k(x′, x′) (6.17)\\nwe can now write\\n|f(x)|2 = | ⟨f, k(·, x)⟩ | ≤ k(x, x) · ⟨f, f ⟩ . (6.18)\\nFrom the above inequality, f = 0 whenever ⟨f, f ⟩ = 0, thus establishing\\n⟨·, ·⟩ as a valid dot product. In fact, one can complete the space of functions\\n(6.12) in the norm corresponding to the dot product (6.13), and thus get a\\nHilbert space H, called the reproducing kernel Hilbert Space (RKHS).\\nAn alternate way to deﬁne a RKHS is as a Hilbert space H on functions\\nfrom some input space X to R with the property that for any f ∈ H and\\nx ∈ X, the point evaluations f → f(x) are continuous (in particular, all\\npoints values f(x) are well deﬁned, which already distinguishes an RKHS\\nfrom many L2 Hilbert spaces). Given the point evaluation functional, one\\ncan then construct the reproducing kernel using the Riesz representation\\ntheorem. The Moore-Aronszajn theorem states that, for every positive semi-\\ndeﬁnite kernel on X × X, there exists a unique RKHS and vice versa.\\nWe ﬁnish this section by noting that ⟨·, ·⟩ is a positive semi-deﬁnite func-\\ntion in the vector space of functions (6.12). This follows directly from the\\nbilinearity of the dot product and (6.14) by which we can write for functions\\nf1, . . . , fp and coeﬃcients γ1, . . . , γp\\n∑\\ni\\n∑\\nj\\nγiγj ⟨fi, fj⟩ =\\n⟨∑\\ni\\nγifi,\\n∑\\nj\\nγjfj\\n⟩\\n≥ 0. (6.19)\\n6.4.1 Hilbert Spaces\\nevaluation functionals, inner products\\n6.4.2 Theoretical Properties\\nMercer’s theorem, positive semideﬁniteness\\n6.4.3 Regularization\\nRepresenter theorem, regularization', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='224b7763-a459-4c5c-9fd6-b97d76284e77', embedding=None, metadata={'page_label': '164', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='164 6 Kernels and Function Spaces\\n6.5 Banach Spaces\\n6.5.1 Properties\\n6.5.2 Norms and Convex Sets\\n- smoothest function (L2) - smallest coeﬃcients (L1) - structured priors\\n(CAP formalism)\\nProblems\\nProblem 6.1 Show that (6.17) holds for an arbitrary positive semi-deﬁnite\\nfunction k.\\nProblem 6.2 Show that the inhomogeneous polynomial kernel (6.3) is a\\nvalid kernel and that it computes all monomials of degree up to d.\\nProblem 6.3 ( k-spectrum kernel {2}) Given two strings x and x′ show\\nhow one can compute the k-spectrum kernel (section 6.1.1.5) in O((|x| +\\n|x′|)k) time. Hint: You need to use a trie.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7e845031-9e32-484b-a931-e19af46fb658', embedding=None, metadata={'page_label': '165', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7\\nLinear Models\\nA hyperplane in a space H endowed with a dot product ⟨·, ·⟩ is described by\\nthe set\\n{x ∈ H| ⟨w, x⟩ + b = 0} (7.1)\\nwhere w ∈ H and b ∈ R. Such a hyperplane naturally divides H into two\\nhalf-spaces: {x ∈ H| ⟨w, x⟩ + b ≥ 0} and {x ∈ H| ⟨w, x⟩ + b < 0}, and\\nhence can be used as the decision boundary of a binary classiﬁer. In this\\nchapter we will study a number of algorithms which employ such linear\\ndecision boundaries. Although such models look restrictive at ﬁrst glance,\\nwhen combined with kernels (Chapter 6) they yield a large class of useful\\nalgorithms.\\nAll the algorithms we will study in this chapter maximize the margin.\\nGiven a set X = {x1, . . . , xm}, the margin is the distance of the closest point\\nin X to the hyperplane (7.1). Elementary geometric arguments (Problem 7.1)\\nshow that the distance of a point xi to a hyperplane is given by | ⟨w, xi⟩ +\\nb |/ ∥w∥, and hence the margin is simply\\nmin\\ni=1,...,m\\n| ⟨w, xi⟩ + b |\\n∥w∥ . (7.2)\\nNote that the parameterization of the hyperplane (7.1) is not unique; if we\\nmultiply both w and b by the same non-zero constant, then we obtain the\\nsame hyperplane. One way to resolve this ambiguity is to set\\nmin\\ni=1,...m\\n| ⟨w, xi⟩ + b| = 1.\\nIn this case, the margin simply becomes 1 /∥w∥. We postpone justiﬁcation\\nof margin maximization for later and jump straight ahead to the description\\nof various algorithms.\\n7.1 Support Vector Classiﬁcation\\nConsider a binary classiﬁcation task, where we are given a training set\\n{(x1, y1), . . . ,(xm, ym)} with xi ∈ H and yi ∈ {± 1}. Our aim is to ﬁnd\\na linear decision boundary parameterized by ( w, b) such that ⟨w, xi⟩ + b ≥ 0\\n165', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9e455439-7bb5-4ed2-90a9-80d807b38a78', embedding=None, metadata={'page_label': '166', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='166 7 Linear Models\\nx1\\nw\\nx2\\nyi = −1\\nyi = +1\\n{x | ⟨ w, x⟩ + b = −1} {x | ⟨ w, x⟩ + b = 1}\\n{x | ⟨ w, x⟩ + b = 0}\\n⟨w, x1⟩ + b = +1\\n⟨w, x2⟩ + b = −1\\n⟨w, x1 − x2⟩ = 2⣨\\nw\\n∥w∥ , x1 − x2\\n⟩\\n= 2\\n∥w∥\\nFig. 7.1. A linearly separable toy binary classiﬁcation problem of separating the\\ndiamonds from the circles. We normalize (w, b) to ensure that mini=1,...m | ⟨w, xi⟩+\\nb | = 1. In this case, the margin is given by 1\\n∥w∥ as the calculation in the inset shows.\\nwhenever yi = +1 and ⟨w, xi⟩+b < 0 whenever yi = −1. Furthermore, as dis-\\ncussed above, we ﬁx the scaling ofw by requiring mini=1,...m | ⟨w, xi⟩+b | = 1.\\nA compact way to write our desiderata is to require yi(⟨w, xi⟩ + b) ≥ 1 for\\nall i (also see Figure 7.1). The problem of maximizing the margin therefore\\nreduces to\\nmax\\nw,b\\n1\\n∥w∥ (7.3a)\\ns.t. yi(⟨w, xi⟩ + b) ≥ 1 for all i, (7.3b)\\nor equivalently\\nmin\\nw,b\\n1\\n2 ∥w∥2 (7.4a)\\ns.t. yi(⟨w, xi⟩ + b) ≥ 1 for all i. (7.4b)\\nThis is a constrained convex optimization problem with a quadratic objec-\\ntive function and linear constraints (see Section 3.3). In deriving (7.4) we\\nimplicitly assumed that the data is linearly separable, that is, there is a\\nhyperplane which correctly classiﬁes the training data. Such a classiﬁer is\\ncalled a hard margin classiﬁer . If the data is not linearly separable, then\\n(7.4) does not have a solution. To deal with this situation we introduce', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fbbd7065-3f64-42af-b240-1c8b34c3b1e8', embedding=None, metadata={'page_label': '167', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Support Vector Classiﬁcation 167\\nnon-negative slack variables ξi to relax the constraints:\\nyi(⟨w, xi⟩ + b) ≥ 1 − ξi.\\nGiven any w and b the constraints can now be satisﬁed by making ξi large\\nenough. This renders the whole optimization problem useless. Therefore, one\\nhas to penalize large ξi. This is done via the following modiﬁed optimization\\nproblem:\\nmin\\nw,b,ξ\\n1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nξi (7.5a)\\ns.t. yi(⟨w, xi⟩ + b) ≥ 1 − ξi for all i (7.5b)\\nξi ≥ 0, (7.5c)\\nwhere C > 0 is a penalty parameter. The resultant classiﬁer is said to be a\\nsoft margin classiﬁer . By introducing non-negative Lagrange multipliers αi\\nand βi one can write the Lagrangian (see Section 3.3)\\nL(w, b, ξ, α, β) = 1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nξi +\\nm∑\\ni=1\\nαi(1 − ξi − yi(⟨w, xi⟩ + b)) −\\nm∑\\ni=1\\nβiξi.\\nNext take gradients with respect to w, b and ξ and set them to zero.\\n∇wL = w −\\nm∑\\ni=1\\nαiyixi = 0 (7.6a)\\n∇bL = −\\nm∑\\ni=1\\nαiyi = 0 (7.6b)\\n∇ξiL = C\\nm − αi − βi = 0. (7.6c)\\nSubstituting (7.6) into the Lagrangian and simplifying yields the dual ob-\\njective function:\\n−1\\n2\\n∑\\ni,j\\nyiyjαiαj ⟨xi, xj⟩ +\\nm∑\\ni=1\\nαi, (7.7)\\nwhich needs to be maximized with respect to α. For notational convenience\\nwe will minimize the negative of (7.7) below. Next we turn our attention\\nto the dual constraints. Recall that αi ≥ 0 and βi ≥ 0, which in conjunc-\\ntion with (7.6c) immediately yields 0 ≤ αi ≤ C\\nm. Furthermore, by (7.6b)∑m\\ni=1 αiyi = 0. Putting everything together, the dual optimization problem', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c1903e46-f902-4e48-9719-1ed8f532caf7', embedding=None, metadata={'page_label': '168', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='168 7 Linear Models\\nboils down to\\nmin\\nα\\n1\\n2\\n∑\\ni,j\\nyiyjαiαj ⟨xi, xj⟩ −\\nm∑\\ni=1\\nαi (7.8a)\\ns.t.\\nm∑\\ni=1\\nαiyi = 0 (7.8b)\\n0 ≤ αi ≤ C\\nm . (7.8c)\\nIf we let H be a m × m matrix with entries Hij = yiyj ⟨xi, xj⟩, while e, α,\\nand y be m-dimensional vectors whose i-th components are one, αi, and yi\\nrespectively, then the above dual can be compactly written as the following\\nQuadratic Program (QP) (Section 3.3.3):\\nmin\\nα\\n1\\n2 α⊤Hα − α⊤e (7.9a)\\ns.t. α⊤y = 0 (7.9b)\\n0 ≤ αi ≤ C\\nm . (7.9c)\\nBefore turning our attention to algorithms for solving (7.9), a number of\\nobservations are in order. First, note that computing H only requires com-\\nputing dot products between training examples. If we map the input data to\\na Reproducing Kernel Hilbert Space (RKHS) via a feature map φ, then we\\ncan still compute the entries of H and solve for the optimal α. In this case,\\nHij = yiyj ⟨φ(xi), φ(xj)⟩ = yiyjk(xi, xj), where k is the kernel associated\\nwith the RKHS. Given the optimal α, one can easily recover the decision\\nboundary. This is a direct consequence of (7.6a), which allows us to write w\\nas a linear combination of the training data:\\nw =\\nm∑\\ni=1\\nαiyiφ(xi),\\nand hence the decision boundary as\\n⟨w, x⟩ + b =\\nm∑\\ni=1\\nαiyik(xi, x) + b. (7.10)\\nBy the KKT conditions (Section 3.3) we have\\nαi(1 − ξi − yi(⟨w, xi⟩ + b)) = 0 and βiξi = 0.\\nWe now consider three cases for yi(⟨w, xi⟩ + b) and the implications of the\\nKKT conditions (see Figure 7.2).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a3b52ae2-945c-4d98-9018-133cf116e5e1', embedding=None, metadata={'page_label': '169', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Support Vector Classiﬁcation 169\\n{x | ⟨ w, x⟩ + b = −1} {x | ⟨ w, x⟩ + b = 1}\\nFig. 7.2. The picture depicts the well classiﬁed points ( yi(⟨w, xi⟩ + b) > 1 in black,\\nthe support vectors yi(⟨w, xi⟩+ b) = 1 in blue, and margin errors yi(⟨w, xi⟩+ b) < 1\\nin red.\\nyi(⟨w, xi⟩ + b) < 1: In this case, ξi > 0, and hence the KKT conditions\\nimply that βi = 0. Consequently, αi = C\\nm (see (7.6c)). Such points\\nare said to be margin errors.\\nyi(⟨w, xi⟩ + b) > 1: In this case, ξi = 0, (1−ξi −yi(⟨w, xi⟩+b)) < 0, and by\\nthe KKT conditions αi = 0. Such points are said to be well classiﬁed.\\nIt is easy to see that the decision boundary (7.10) does not change\\neven if these points are removed from the training set.\\nyi(⟨w, xi⟩ + b) = 1: In this case ξi = 0 and βi ≥ 0. Since αi is non-negative\\nand satisﬁes (7.6c) it follows that 0 ≤ αi ≤ C\\nm. Such points are said\\nto be on the margin. They are also sometimes called support vectors.\\nSince the support vectors satisfy yi(⟨w, xi⟩ + b) = 1 and yi ∈ {±1} it follows\\nthat b = yi − ⟨ w, xi⟩ for any support vector xi. However, in practice to\\nrecover b we average\\nb = yi −\\n∑\\ni\\n⟨w, xi⟩ . (7.11)\\nover all support vectors, that is, points xi for which 0 < α i < C\\nm. Because\\nit uses support vectors, the overall algorithm is called C-Support Vector\\nclassiﬁer or C-SV classiﬁer for short.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f1f1eb74-dbe2-4c9e-8d5b-3b35673c8058', embedding=None, metadata={'page_label': '170', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='170 7 Linear Models\\n7.1.1 A Regularized Risk Minimization Viewpoint\\nA closer examination of (7.5) reveals thatξi = 0 whenever yi(⟨w, xi⟩+b) > 1.\\nOn the other hand, ξi = 1 − yi(⟨w, xi⟩ + b) whenever yi(⟨w, xi⟩ + b) <\\n1. In short, ξi = max(0 , 1 − yi(⟨w, xi⟩ + b)). Using this observation one\\ncan eliminate ξi from (7.5), and write it as the following unconstrained\\noptimization problem:\\nmin\\nw,b\\n1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nmax(0, 1 − yi(⟨w, xi⟩ + b)). (7.12)\\nWriting (7.5) as (7.12) is particularly revealing because it shows that a\\nsupport vector classiﬁer is nothing but a regularized risk minimizer. Here\\nthe regularizer is the square norm of the decision hyperplane 1\\n2 ∥w∥2, and\\nthe loss function is the so-called binary hinge loss (Figure 7.3):\\nl(w, x, y) = max(0, 1 − y(⟨w, x⟩ + b)). (7.13)\\nIt is easy to verify that the binary hinge loss (7.13) is convex but non-\\ndiﬀerentiable (see Figure 7.3) which renders the overall objective function\\n(7.12) to be convex but non-smooth. There are two diﬀerent strategies to\\nminimize such an objective function. If minimizing (7.12) in the primal, one\\ncan employ non-smooth convex optimizers such as bundle methods (Section\\n3.2.7). This yields a d dimensional problem where d is the dimension of x.\\nOn the other hand, since (7.12) is strongly convex because of the presence\\nof the 1\\n2 ∥w∥2 term, its Fenchel dual has a Lipschitz continuous gradient\\n(see Lemma 3.10). The dual problem is m dimensional and contains linear\\nconstraints. This strategy is particularly attractive when the kernel trick is\\nused or whenever d ≫ m. In fact, the dual problem obtained via Fenchel\\nduality is very related to the Quadratic programming problem (7.9) obtained\\nvia Lagrange duality (problem 7.4).\\n7.1.2 An Exponential Family Interpretation\\nOur motivating arguments for deriving the SVM algorithm have largely\\nbeen geometric. We now show that an equally elegant probabilistic interpre-\\ntation also exists. Assuming that the training set {(x1, y1), . . . ,(xm, ym)}\\nwas drawn iid from some underlying distribution, and using the Bayes rule\\n(1.15) one can write the likelihood\\np(θ|X, Y ) ∝ p(θ)p(Y |X, θ) = p(θ)\\nm∏\\ni=1\\np(yi|xi, θ), (7.14)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e403d0db-e4a3-479b-9608-4daac12d8588', embedding=None, metadata={'page_label': '171', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Support Vector Classiﬁcation 171\\ny(⟨w, x⟩ + b)\\nloss\\nFig. 7.3. The binary hinge loss. Note that the loss is convex but non-diﬀerentiable\\nat the kink point. Furthermore, it increases linearly as the distance from the decision\\nhyperplane y(⟨w, x⟩ + b) decreases.\\nand hence the negative log-likelihood\\n− log p(θ|X, Y ) = −\\nm∑\\ni=1\\nlog p(yi|xi, θ) − log p(θ) + const. (7.15)\\nIn the absence of any prior knowledge about the data, we choose a zero\\nmean unit variance isotropic normal distribution for p(θ). This yields\\n− log p(θ|X, Y ) = 1\\n2 ∥θ∥2 −\\nm∑\\ni=1\\nlog p(yi|xi, θ) + const. (7.16)\\nThe maximum aposteriori (MAP) estimate for θ is obtained by minimizing\\n(7.16) with respect to θ. Given the optimal θ, we can predict the class label\\nat any given x via\\ny∗ = argmax\\ny\\np(y|x, θ). (7.17)\\nOf course, our aim is not just to maximize p(yi|xi, θ) but also to ensure\\nthat p(y|xi, θ) is small for all y ̸= yi. This, for instance, can be achieved by\\nrequiring\\np(yi|xi, θ)\\np(y|xi, θ) ≥ η, for all y ̸= yi and some η ≥ 1. (7.18)\\nAs we saw in Section 2.3 exponential families of distributions are rather ﬂex-\\nible modeling tools. We could, for instance, model p(yi|xi, θ) as a conditional\\nexponential family distribution. Recall the deﬁnition:\\np(y|x, θ) = exp (⟨φ(x, y), θ⟩ − g(θ|x)) . (7.19)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='96340323-5e8d-4be0-a133-205295c6685c', embedding=None, metadata={'page_label': '172', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='172 7 Linear Models\\nHere φ(x, y) is a joint feature map which depends on both the input data x\\nand the label y, while g(θ|x) is the log-partition function. Now (7.18) boils\\ndown to\\np(yi|xi, θ)\\nmaxy̸=yi p(y|xi, θ) = exp\\n(⟨\\nφ(xi, yi) − max\\ny̸=yi\\nφ(xi, y), θ\\n⟩)\\n≥ η. (7.20)\\nIf we choose η such that log η = 1, set φ(x, y) = y\\n2 φ(x), and observe that\\ny ∈ {±1} we can rewrite (7.20) as\\n⣨ yi\\n2 φ(xi) −\\n(\\n− yi\\n2\\n)\\nφ(xi), θ\\n⟩\\n= yi ⟨φ(xi), θ⟩ ≥ 1. (7.21)\\nBy replacing − log p(yi|xi, θ) in (7.16) with the condition (7.21) we obtain\\nthe following objective function:\\nmin\\nθ\\n1\\n2 ∥θ∥2 (7.22a)\\ns.t. yi ⟨φ(xi), θ⟩ ≥ 1 for all i, (7.22b)\\nwhich recovers (7.4), but without the bias b. The prediction function is\\nrecovered by noting that (7.17) specializes to\\ny∗ = argmax\\ny∈{±1}\\n⟨φ(x, y), θ⟩ = argmax\\ny∈{±1}\\ny\\n2 ⟨φ(x), θ⟩ = sign(⟨φ(x), θ⟩). (7.23)\\nAs before, we can replace (7.21) by a linear penalty for constraint viola-\\ntion in order to recover (7.5). The quantity log p(yi|xi,θ)\\nmaxy̸=yi p(y|xi,θ) is sometimes\\ncalled the log-odds ratio, and the above discussion shows that SVMs can\\nbe interpreted as maximizing the log-odds ratio in the exponential family.\\nThis interpretation will be developed further when we consider extensions of\\nSVMs to tackle multiclass, multilabel, and structured prediction problems.\\n7.1.3 Specialized Algorithms for Training SVMs\\nThe main task in training SVMs boils down to solving (7.9). The m × m\\nmatrix H is usually dense and cannot be stored in memory. Decomposition\\nmethods are designed to overcome these diﬃculties. The basic idea here\\nis to identify and update a small working set B by solving a small sub-\\nproblem at every iteration. Formally, let B ⊂ {1, . . . , m} be the working set\\nand αB be the corresponding sub-vector of α. Deﬁne ¯B = {1, . . . , m} \\\\ B\\nand α ¯B analogously. In order to update αB we need to solve the following', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5e08d6a4-d440-43fc-b7b5-ee089bac388f', embedding=None, metadata={'page_label': '173', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Support Vector Classiﬁcation 173\\nsub-problem of (7.9) obtained by freezing α ¯B:\\nmin\\nαB\\n1\\n2\\n[\\nα⊤\\nB α⊤¯B\\n] [ HBB HB ¯B\\nH ¯BB H ¯B ¯B\\n] [ αB\\nα ¯B\\n]\\n−\\n[\\nα⊤\\nB α⊤¯B\\n]\\ne (7.24a)\\ns.t.\\n[\\nα⊤\\nB α⊤¯B\\n]\\ny = 0 (7.24b)\\n0 ≤ αi ≤ C\\nm for all i ∈ B. (7.24c)\\nHere,\\n[ HBB HB ¯B\\nH ¯BB H ¯B ¯B\\n]\\nis a permutation of the matrix H. By eliminating\\nconstant terms and rearranging, one can simplify the above problem to\\nmin\\nαB\\n1\\n2 α⊤\\nBHBB αB + α⊤\\nB(H ¯BB α ¯B − e) (7.25a)\\ns.t. α⊤\\nByB = −α⊤¯By ¯B (7.25b)\\n0 ≤ αi ≤ C\\nm for all i ∈ B. (7.25c)\\nAn extreme case of a decomposition method is the Sequential Minimal Op-\\ntimization (SMO) algorithm of Platt [Pla99], which updates only two coef-\\nﬁcients per iteration. The advantage of this strategy as we will see below is\\nthat the resultant sub-problem can be solved analytically. Without loss of\\ngenerality let B = {i, j}, and deﬁne s = yi/yj,\\n[\\nci cj\\n]\\n= (H ¯BB α ¯B − e)⊤\\nand d = (−α⊤¯By ¯B/yj). Then (7.25) specializes to\\nmin\\nαi,αj\\n1\\n2(Hiiα2\\ni + Hjj α2\\nj + 2Hijαjαi) + ciαi + cjαj (7.26a)\\ns.t. sαi + αj = d (7.26b)\\n0 ≤ αi, αj ≤ C\\nm . (7.26c)\\nThis QP in two variables has an analytic solution.\\nLemma 7.1 (Analytic solution of 2 variable QP) Deﬁne bounds\\nL =\\n{\\nmax(0,\\nd−C\\nm\\ns ) if s > 0\\nmax(0, d\\ns) otherwise\\n(7.27)\\nH =\\n{\\nmin( C\\nm , d\\ns) if s > 0\\nmin( C\\nm ,\\nd−C\\nm\\ns ) otherwise,\\n(7.28)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e622c4ec-77f0-4bf8-926e-f47b69dd528c', embedding=None, metadata={'page_label': '174', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='174 7 Linear Models\\nand auxiliary variables\\nχ = (Hii + Hjj s2 − 2sHij) and (7.29)\\nρ = (cjs − ci − Hijd + Hjj ds). (7.30)\\nThe optimal value of (7.26) can be computed analytically as follows: If χ = 0\\nthen\\nαi =\\n{\\nL if ρ < 0\\nH otherwise.\\nIf χ > 0, then αi = max(L, min(H, ρ/χ)). In both cases, αj = (d − sαi).\\nProof Eliminate the equality constraint by setting αj = (d − sαi). Due to\\nthe constraint 0 ≤ αj ≤ C\\nm it follows that sαi = d − αj can be bounded\\nvia d − C\\nm ≤ sαi ≤ d. Combining this with 0 ≤ αi ≤ C\\nm one can write\\nL ≤ αi ≤ H where L and H are given by (7.27) and (7.28) respectively.\\nSubstituting αj = (d−sαi) into the objective function, dropping the terms\\nwhich do not depend on αi, and simplifying by substituting χ and ρ yields\\nthe following optimization problem in αi:\\nmin\\nαi\\n1\\n2 α2\\ni χ − αiρ\\ns.t. L ≤ αi ≤ H.\\nFirst consider the case when χ = 0. In this case, αi = L if ρ < 0 otherwise\\nαi = H. On other hand, if χ > 0 then the unconstrained optimum of the\\nabove optimization problem is given by ρ/χ. The constrained optimum is\\nobtained by clipping appropriately: max( L, min(H, ρ/χ)). This concludes\\nthe proof.\\nTo complete the description of SMO we need a valid stopping criterion as\\nwell as a scheme for selecting the working set at every iteration. In order\\nto derive a stopping criterion we will use the KKT gap, that is, the extent\\nto which the KKT conditions are violated. Towards this end introduce non-\\nnegative Lagrange multipliers b ∈ R, λ ∈ Rm and µ ∈ Rm and write the\\nLagrangian of (7.9).\\nL(α, b, λ, µ) = 1\\n2 α⊤Hα − α⊤e + bα⊤y − λ⊤α + µ⊤(α − C\\nm e). (7.31)\\nIf we let J(α) = 1\\n2 α⊤Hα − α⊤e be the objective function and ∇J(α) =\\nHα − e its gradient, then taking gradient of the Lagrangian with respect to\\nα and setting it to 0 shows that\\n∇J(α) + by = λ − µ. (7.32)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4474b928-c330-41d5-b47f-ce3617d780b7', embedding=None, metadata={'page_label': '175', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.1 Support Vector Classiﬁcation 175\\nFurthermore, by the KKT conditions we have\\nλiαi = 0 and µi( C\\nm − αi) = 0, (7.33)\\nwith λi ≥ 0 and µi ≥ 0. Equations (7.32) and (7.33) can be compactly\\nrewritten as\\n∇J(α)i + byi ≥ 0 if αi = 0 (7.34a)\\n∇J(α)i + byi ≤ 0 if αi = C\\nm (7.34b)\\n∇J(α)i + byi = 0 if 0 < α i < C\\nm . (7.34c)\\nSince yi ∈ {±1}, we can further rewrite (7.34) as\\n−yi∇J(α)i ≤ b for all i ∈ Iup\\n−yi∇J(α)i ≥ b for all i ∈ Idown,\\nwhere the index sets Iup and Idown are deﬁned as\\nIup = {i : αi < C\\nm , yi = 1 or αi > 0, yi = −1} (7.35a)\\nIdown = {i : αi < C\\nm , yi = −1 or αi > 0, yi = 1}. (7.35b)\\nIn summary, the KKT conditions imply that α is a solution of (7.9) if and\\nonly if\\nm(α) ≤ M(α)\\nwhere\\nm(α) = max\\ni∈Iup\\n−yi∇J(α)i and M(α) = min\\ni∈Idown\\n−yi∇J(α)i. (7.36)\\nTherefore, a natural stopping criterion is to stop when the KKT gap falls\\nbelow a desired tolerance ϵ, that is,\\nm(α) ≤ M(α) + ϵ. (7.37)\\nFinally, we turn our attention to the issue of working set selection. The\\nﬁrst order approximation to the objective function J(α) can be written as\\nJ(α + d) ≈ J(α) + ∇J(α)⊤d.\\nSince we are only interested in updating coeﬃcients in the working set B\\nwe set d⊤ =\\n[\\nd⊤\\nB 0\\n]\\n, in which case we can rewrite the above ﬁrst order', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f7115835-ebd7-4763-a80e-989e572187a9', embedding=None, metadata={'page_label': '176', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='176 7 Linear Models\\napproximation as\\n∇J(α)⊤\\nBdB ≈ J(α + d) − J(α).\\nFrom among all possible directionsdB we wish to choose one which decreases\\nthe objective function the most while maintaining feasibility. This is best\\nexpressed as the following optimization problem:\\nmin\\ndB\\n∇J(α)⊤\\nBdB (7.38a)\\ns.t. y⊤\\nB dB = 0 (7.38b)\\ndi ≥ 0 if αi = 0 and i ∈ B (7.38c)\\ndi ≤ 0 if αi = C\\nm and i ∈ B (7.38d)\\n− 1 ≤ di ≤ 1. (7.38e)\\nHere (7.38b) comes from y⊤(α + d) = 0 and y⊤α = 0, while (7.38c) and\\n(7.38d) comes from 0 ≤ αi ≤ C\\nm. Finally, (7.38e) prevents the objective\\nfunction from diverging to −∞. If we specialize (7.38) to SMO, we obtain\\nmin\\ni,j\\n∇J(α)idi + ∇J(α)jdj (7.39a)\\ns.t. yidi + yjdj = 0 (7.39b)\\ndk ≥ 0 if αk = 0 and k ∈ {i, j} (7.39c)\\ndk ≤ 0 if αk = C\\nm and k ∈ {i, j} (7.39d)\\n− 1 ≤ dk ≤ 1 for k ∈ {i, j}. (7.39e)\\nAt ﬁrst glance, it seems that choosing the optimal i and j from the set\\n{1, . . . , m}×{ 1, . . . m} requires O(m2) eﬀort. We now show thatO(m) eﬀort\\nsuﬃces.\\nDeﬁne new variables ˆdk = ykdk for k ∈ { i, j}, and use the observation\\nyk ∈ {±1} to rewrite the objective function as\\n(−yi∇J(α)i + yj∇J(α)j) ˆdj.\\nConsider the case −∇J(α)iyi ≥ −∇ J(α)jyj. Because of the constraints\\n(7.39c) and (7.39d) if we choose i ∈ Iup and j ∈ Idown, then ˆdj = −1 and\\nˆdi = 1 is feasible and the objective function attains a negative value. For\\nall other choices of i and j (i, j ∈ Iup; i, j ∈ Idown; i ∈ Idown and j ∈ Iup)\\nthe objective function value of 0 is attained by setting ˆdi = ˆdj = 0. The\\ncase −∇J(α)jyj ≥ −∇ J(α)iyi is analogous. In summary, the optimization', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='20f551c4-3b49-42d2-a283-33f1ac50dccb', embedding=None, metadata={'page_label': '177', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2 Extensions 177\\nproblem (7.39) boils down to\\nmin\\ni∈Iup,j∈Idown\\nyi∇J(α)i − yj∇J(α)j = min\\ni∈Iup\\nyi∇J(α)i − max\\nj∈Idown\\nyj∇J(α)j,\\nwhich clearly can be solved in O(m) time. Comparison with (7.36) shows\\nthat at every iteration of SMO we choose to update coeﬃcients αi and αj\\nwhich maximally violate the KKT conditions.\\n7.2 Extensions\\n7.2.1 The ν trick\\nIn the soft margin formulation the parameter C is a trade-oﬀ between two\\nconﬂicting requirements namely maximizing the margin and minimizing the\\ntraining error. Unfortunately, this parameter is rather unintuitive and hence\\ndiﬃcult to tune. The ν-SVM was proposed to address this issue. As Theorem\\n7.3 below shows, ν controls the number of support vectors and margin errors.\\nThe primal problem for the ν-SVM can be written as\\nmin\\nw,b,ξ,ρ\\n1\\n2 ∥w∥2 − ρ + 1\\nνm\\nm∑\\ni=1\\nξi (7.40a)\\ns.t. yi(⟨w, xi⟩ + b) ≥ ρ − ξi for all i (7.40b)\\nξi ≥ 0, and ρ ≥ 0. (7.40c)\\nAs before, if we write the Lagrangian by introducing non-negative Lagrange\\nmultipliers, take gradients with respect to the primal variables and set them\\nto zero, and substitute the result back into the Lagrangian we obtain the\\nfollowing dual:\\nmin\\nα\\n1\\n2\\n∑\\ni,j\\nyiyjαiαj ⟨xi, xj⟩ (7.41a)\\ns.t.\\nm∑\\ni=1\\nαiyi = 0 (7.41b)\\nm∑\\ni=1\\nαi ≥ 1 (7.41c)\\n0 ≤ αi ≤ 1\\nνm . (7.41d)\\nIt turns out that the dual can be further simpliﬁed via the following lemma.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2ded25f0-6aa1-470b-bd18-5309d577a61d', embedding=None, metadata={'page_label': '178', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='178 7 Linear Models\\nLemma 7.2 Let ν ∈ [0, 1] and (7.41) be feasible. Then there is at least one\\nsolution α which satisﬁes ∑\\ni αi = 1. Furthermore, if the ﬁnal objective value\\nof (7.41) is non-zero then all solutions satisfy ∑\\ni αi = 1.\\nProof The feasible region of (7.41) is bounded, therefore if it is feasible\\nthen there exists an optimal solution. Let α denote this solution and assume\\nthat ∑\\ni αi > 1. In this case we can deﬁne\\n¯α = 1∑\\nj αj\\nα,\\nand easily check that ¯α is also feasible. As before, let H denote a m × m\\nmatrix with Hij = yiyj ⟨xi, xj⟩. Since α is the optimal solution of (7.41) it\\nfollows that\\n1\\n2 α⊤Hα ≤ 1\\n2 ¯α⊤H ¯α =\\n(\\n1∑\\nj αj\\n)2\\n1\\n2 α⊤Hα ≤ 1\\n2 α⊤Hα.\\nThis implies that either 1\\n2 α⊤Hα = 0, in which case ¯α is an optimal solution\\nwith the desired property or 1\\n2 α⊤Hα ̸= 0, in which case all optimal solutions\\nsatisfy ∑\\ni αi = 1.\\nIn view of the above theorem one can equivalently replace (7.41) by the\\nfollowing simpliﬁed optimization problem with two equality constraints\\nmin\\nα\\n1\\n2\\n∑\\ni,j\\nyiyjαiαj ⟨xi, xj⟩ (7.42a)\\ns.t.\\nm∑\\ni=1\\nαiyi = 0 (7.42b)\\nm∑\\ni=1\\nαi = 1 (7.42c)\\n0 ≤ αi ≤ 1\\nνm . (7.42d)\\nThe following theorems, which we state without proof, explain the signif-\\nicance of ν and the connection between ν-SVM and the soft margin formu-\\nlation.\\nTheorem 7.3 Suppose we run ν-SVM with kernel k on some data and\\nobtain ρ > 0. Then\\n(i) ν is an upper bound on the fraction of margin errors, that is points\\nfor which yi (⟨w, xi⟩ + bi) < ρ.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='97f5dac6-1cf0-4963-b5bd-86dcf91b2531', embedding=None, metadata={'page_label': '179', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.2 Extensions 179\\n(ii) ν is a lower bound on the fraction of support vectors, that is points\\nfor which yi (⟨w, xi⟩ + bi) = ρ.\\n(iii) Suppose the data (X, Y ) were generated iid from a distribution p(x, y)\\nsuch that neither p(x, y = +1) or p(x, y = −1) contain any discrete\\ncomponents. Moreover, assume that the kernel k is analytic and non-\\nconstant. With probability 1, asympotically, ν equals both the fraction\\nof support vectors and fraction of margin errors.\\nTheorem 7.4 If (7.40) leads to a decision function with ρ > 0, then (7.5)\\nwith C = 1\\nρ leads to the same decision function.\\n7.2.2 Squared Hinge Loss\\nIn binary classiﬁcation, the actual loss which one would like to minimize is\\nthe so-called 0-1 loss\\nl(w, x, y) =\\n{\\n0 if y(⟨w, x⟩ + b) ≥ 1\\n1 otherwise .\\n(7.43)\\nThis loss is diﬃcult to work with because it is non-convex (see Figure 7.4). In\\ny(⟨w, x⟩ + b)\\nloss\\nFig. 7.4. The 0-1 loss which is non-convex and intractable is depicted in red. The\\nhinge loss is a convex upper bound to the 0-1 loss and shown in blue. The square\\nhinge loss is a diﬀerentiable convex upper bound to the 0-1 loss and is depicted in\\ngreen.\\nfact, it has been shown that ﬁnding the optimal ( w, b) pair which minimizes\\nthe 0-1 loss on a training dataset of m labeled points is NP hard [BDEL03].\\nTherefore various proxy functions such as the binary hinge loss (7.13) which\\nwe discussed in Section 7.1.1 are used. Another popular proxy is the square', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9f64d403-ed05-429e-8890-cbfd81f4c9c2', embedding=None, metadata={'page_label': '180', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='180 7 Linear Models\\nhinge loss:\\nl(w, x, y) = max(0, 1 − y(⟨w, x⟩ + b))2. (7.44)\\nBesides being a proxy for the 0-1 loss, the squared hinge loss, unlike the\\nhinge loss, is also diﬀerentiable everywhere. This sometimes makes the opti-\\nmization in the primal easier. Just like in the case of the hinge loss one can\\nderive the dual of the regularized risk minimization problem and show that\\nit is a quadratic programming problem (problem 7.5).\\n7.2.3 Ramp Loss\\nThe ramp loss\\nl(w, x, y) = min(1 − s, max(0, 1 − y(⟨w, x⟩ + b))) (7.45)\\nparameterized by s ≤ 0 is another proxy for the 0-1 loss (see Figure 7.5).\\nAlthough not convex, it can be expressed as the diﬀerence of two convex\\nfunctions\\nlconc(w, x, y) = max(0, 1 − y(⟨w, x⟩ + b)) and\\nlcave(w, x, y) = max(0, s − y(⟨w, x⟩ + b)).\\nTherefore the Convex-Concave procedure (CCP) we discussed in Section\\nFig. 7.5. The ramp loss depicted here with s = −0.3 can be viewed as the sum\\nof a convex function namely the binary hinge loss (left) and a concave function\\nmin(0, 1 − y(⟨w, x⟩ + b)) (right). Viewed alternatively, the ramp loss can be written\\nas the diﬀerence of two convex functions.\\n3.5.1 can be used to solve the resulting regularized risk minimization problem\\nwith the ramp loss. Towards this end write\\nJ(w) = 1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nlconc(w, xi, yi)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nJconc(w)\\n− C\\nm\\nm∑\\ni=1\\nlcave(w, xi, yi)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nJcave(w)\\n. (7.46)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='59834bb7-bf66-47f4-8868-f77a6e61826d', embedding=None, metadata={'page_label': '181', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Support Vector Regression 181\\nRecall that at every iteration of the CCP we replace Jcave(w) by its ﬁrst\\norder Taylor approximation, computing which requires\\n∂wJ(w) = C\\nm\\nm∑\\ni=1\\n∂wlcave(w, xi, yi). (7.47)\\nThis in turn can be computed as\\n∂wlcave(w, xi, yi) = δiyixi with δi =\\n{\\n−1 if s > y (⟨w, x⟩ + b)\\n0 otherwise .\\n(7.48)\\nIgnoring constant terms, each iteration of the CCP algorithm involves solv-\\ning the following minimization problem (also see (3.134))\\nJ(w) = 1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nlconc(w, xi, yi) −\\n(\\nC\\nm\\nm∑\\ni=1\\nδiyixi\\n)\\nw. (7.49)\\nLet δ denote a vector in Rm with components δi. Using the same notation\\nas in (7.9) we can write the following dual optimization problem which is\\nvery closely related to the standard SVM dual (7.9) (see problem 7.6)\\nmin\\nα\\n1\\n2 α⊤Hα − α⊤e (7.50a)\\ns.t. α⊤y = 0 (7.50b)\\n− C\\nm δ ≤ αi ≤ C\\nm(e − δ). (7.50c)\\nIn fact, this problem can be solved by a SMO solver with minor modiﬁca-\\ntions. Putting everything together yields Algorithm 7.1.\\nAlgorithm 7.1 CCP for Ramp Loss\\n1: Initialize δ0 and α0\\n2: repeat\\n3: Solve (7.50) to ﬁnd αt+1\\n4: Compute δt+1 using (7.48)\\n5: until δt+1 = δt\\n7.3 Support Vector Regression\\nAs opposed to classiﬁcation where the labels yi are binary valued, in re-\\ngression they are real valued. Given a tolerance ϵ, our aim here is to ﬁnd a', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='179c26e6-3000-4df7-bd82-fc7878c505f0', embedding=None, metadata={'page_label': '182', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='182 7 Linear Models\\ny − (⟨w, x⟩ + b)\\nloss\\nϵ\\nFig. 7.6. The ϵ insensitive loss. All points which lie within the ϵ tube shaded in\\ngray incur zero loss while points outside incur a linear loss.\\nhyperplane parameterized by ( w, b) such that\\n|yi − (⟨w, xi⟩ + b)| ≤ ϵ. (7.51)\\nIn other words, we want to ﬁnd a hyperplane such that all the training data\\nlies within an ϵ tube around the hyperplane. We may not always be able to\\nﬁnd such a hyperplane, hence we relax the above condition by introducing\\nslack variables ξ+\\ni and ξ−\\ni and write the corresponding primal problem as\\nmin\\nw,b,ξ+,ξ−\\n1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\n(ξ+\\ni + ξ−\\ni ) (7.52a)\\ns.t. yi − (⟨w, xi⟩ + b) ≤ ϵ + ξ+\\ni for all i (7.52b)\\n(⟨w, xi⟩ + b) − yi ≤ ϵ + ξ−\\ni for all i (7.52c)\\nξ+\\ni ≥ 0, and ξ−\\ni ≥ 0. (7.52d)\\nThe Lagrangian can be written by introducing non-negative Lagrange mul-\\ntipliers α+\\ni , α−\\ni , β+\\ni and β−\\ni :\\nL(w, b, ξ+, ξ−, α+, α−, β+, β−) =1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\n(ξ+\\ni + ξ−\\ni ) −\\nm∑\\ni=1\\n(β+\\ni ξ+\\ni + β−\\ni ξ−\\ni )\\n+\\nm∑\\ni=1\\nα+\\ni (yi − (⟨w, xi⟩ + b) − ϵ − ξ+)\\n+\\nm∑\\ni=1\\nα−\\ni ((⟨w, xi⟩ + b) − yi − ϵ − ξ−).', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7e4895d4-8f87-4e65-9d42-4534b93956b7', embedding=None, metadata={'page_label': '183', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Support Vector Regression 183\\nTaking gradients with respect to the primal variables and setting them to\\n0, we obtain the following conditions:\\nw =\\nm∑\\ni=1\\n(α+\\ni − α−\\ni )xi (7.53)\\nm∑\\ni=1\\nα+\\ni =\\nm∑\\ni=1\\nα−\\ni (7.54)\\nα+\\ni + β+\\ni = C\\nm (7.55)\\nα−\\ni + β−\\ni = C\\nm . (7.56)\\nNoting that α{+,−}\\ni , β{+,−}\\ni ≥ 0 and substituting the above conditions into\\nthe Lagrangian yields the dual\\nmin\\nα+,α−\\n1\\n2\\n∑\\ni,j\\n(α+\\ni − α−\\ni )(α+\\nj − α−\\nj ) ⟨xi, xj⟩ (7.57a)\\n+ ϵ\\nm∑\\ni=1\\n(α+\\ni + α−\\ni ) −\\nm∑\\ni=1\\nyi(α+\\ni − α−\\ni )\\ns.t.\\nm∑\\ni=1\\nα+\\ni =\\nm∑\\ni=1\\nα−\\ni (7.57b)\\n0 ≤ α+\\ni ≤ C\\nm (7.57c)\\n0 ≤ α−\\ni ≤ C\\nm . (7.57d)\\nThis is a quadratic programming problem with one equality constraint, and\\nhence a SMO like decomposition method can be derived for ﬁnding the\\noptimal coeﬃcients α+ and α− (Problem 7.7).\\nAs a consequence of (7.53), analogous to the classiﬁcation case, one can\\nmap the data via a feature map φ into an RKHS with kernel k and recover\\nthe decision boundary f(x) = ⟨w, φ(x)⟩ + b via\\nf(x) =\\nm∑\\ni=1\\n(α+\\ni − α−\\ni ) ⟨φ(x)i, φ(x)⟩ + b =\\nm∑\\ni=1\\n(α+\\ni − α−\\ni )k(xi, x) + b. (7.58)\\nFinally, the KKT conditions\\n( C\\nm − α+\\ni\\n)\\nξ+\\ni = 0\\n( C\\nm − α−\\ni\\n)\\nξ−\\ni = 0 and\\nα−\\ni ((⟨w, xi⟩ + b) − yi − ϵ − ξ−) = 0 α+\\ni (yi − (⟨w, xi⟩ + b) − ϵ − ξ+) = 0,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ebd56521-42cc-4add-a01a-91554d08f200', embedding=None, metadata={'page_label': '184', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='184 7 Linear Models\\nallow us to draw many useful conclusions:\\n• Whenever |yi − (⟨w, xi⟩ + b)| < ϵ , this implies that ξ+\\ni = ξ−\\ni = α+\\ni =\\nα−\\ni = 0. In other words, points which lie inside the ϵ tube around the\\nhyperplane ⟨w, x⟩ + b do not contribute to the solution thus leading to\\nsparse expansions in terms of α.\\n• If (⟨w, xi⟩+b)−yi > ϵ we have ξ−\\ni > 0 and therefore α−\\ni = C\\nm. On the other\\nhand, ξ+ = 0 and α+\\ni = 0. The case yi − (⟨w, xi⟩ + b) > ϵ is symmetric\\nand yields ξ− = 0, ξ+\\ni > 0, α+\\ni = C\\nm, and α−\\ni = 0.\\n• Finally, if (⟨w, xi⟩ + b) − yi = ϵ we have ξ−\\ni = 0 and 0 ≤ α−\\ni ≤ C\\nm, while\\nξ+ = 0 and α+\\ni = 0. Similarly, when yi − (⟨w, xi⟩ + b) = ϵ we obtain\\nξ+\\ni = 0, 0 ≤ α+\\ni ≤ C\\nm, ξ− = 0 and α−\\ni = 0.\\nNote that α+\\ni and α−\\ni are never simultaneously non-zero.\\n7.3.1 Incorporating General Loss Functions\\nUsing the same reasoning as in Section 7.1.1 we can deduce from (7.52) that\\nthe loss function of support vector regression is given by\\nl(w, x, y) = max(0, |y − ⟨w, x⟩ | − ϵ). (7.59)\\nIt turns out that the support vector regression framework can be easily\\nextended to handle other, more general, convex loss functions such as the\\nones found in Table 7.1. Diﬀerent losses have diﬀerent properties and hence\\nlead to diﬀerent estimators. For instance, the square loss leads to penalized\\nleast squares (LS) regression, while the Laplace loss leads to the penalized\\nleast absolute deviations (LAD) estimator. Huber’s loss on the other hand is\\na combination of the penalized LS and LAD estimators, and the pinball loss\\nwith parameter τ ∈ [0, 1] is used to estimate τ-quantiles. Setting τ = 0 .5\\nin the pinball loss leads to a scaled version of the Laplace loss. If we deﬁne\\nξ = y − ⟨w, x⟩, then it is easily veriﬁed that all these losses can all be written\\nas\\nl(w, x, y) =\\n\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\nl+(ξ − ϵ) if ξ > ϵ\\nl−(−ξ − ϵ) if ξ < ϵ\\n0 if ξ ∈ [−ϵ, ϵ].\\n(7.60)\\nFor all these diﬀerent loss functions, the support vector regression formu-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a274850f-10bf-4b8b-aa9c-e9e1185dca38', embedding=None, metadata={'page_label': '185', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.3 Support Vector Regression 185\\nlation can be written in a uniﬁed fashion as follows\\nmin\\nw,b,ξ+,ξ−\\n1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nl+(ξ+\\ni ) + l−(ξ−\\ni ) (7.61a)\\ns.t. yi − (⟨w, xi⟩ + b) ≤ ϵ + ξ+\\ni for all i (7.61b)\\n(⟨w, xi⟩ + b) − yi ≤ ϵ + ξ−\\ni for all i (7.61c)\\nξ+\\ni ≥ 0, and ξ−\\ni ≥ 0. (7.61d)\\nThe dual in this case is given by\\nmin\\nα+,α−\\n1\\n2\\n∑\\ni,j\\n(α+\\ni − α−\\ni )(α+\\nj − α−\\nj ) ⟨xi, xj⟩ (7.62a)\\n− C\\nm\\nm∑\\ni=1\\nT +(ξ+) + T−(ξ−) + ϵ\\nm∑\\ni=1\\n(α+\\ni + α−\\ni ) −\\nm∑\\ni=1\\nyi(α+\\ni − α−\\ni )\\ns.t.\\nm∑\\ni=1\\nα+\\ni =\\nm∑\\ni=1\\nα−\\ni (7.62b)\\n0 ≤ α{+,−}\\ni ≤ C\\nm ∂ξl{+,−}(ξ{+,−}\\ni ) (7.62c)\\n0 ≤ ξ{+,−}\\ni (7.62d)\\nξ{+,−}\\ni = inf\\n{\\nξ{+,−} | C\\nm ∂ξl{+,−} ≥ α{+,−}\\ni\\n}\\n. (7.62e)\\nHere T +(ξ) = l+(ξ) − ξ∂ξl+(ξ) and T−(ξ) = l−(ξ) − ξ∂ξl−(ξ). We now show\\nhow (7.62) can be specialized to the pinball loss. Clearly, l+(ξ) = τ ξ while\\nl−(−ξ) = (τ −1)ξ, and hence l−(ξ) = (1−τ)ξ. Therefore, T +(ξ) = (τ −1)ξ −\\nξ(τ − 1) = 0. Similarly T−(ξ) = 0. Since ∂ξl+(ξ) = τ and ∂ξl−(ξ) = (1 − τ)\\nfor all ξ ≥ 0, it follows that the bounds on α{+,−} can be computed as\\n0 ≤ α+\\ni ≤ C\\nm τ and 0 ≤ α−\\ni ≤ C\\nm(1 − τ). If we denote α = α+ − α− and\\nTable 7.1. Various loss functions which can be used in support vector\\nregression. For brevity we denote y − ⟨w, x⟩ as ξ and write the loss\\nl(w, x, y) in terms of ξ.\\nϵ-insensitive loss max(0 , |ξ| − ϵ)\\nLaplace loss |ξ|\\nSquare loss 1\\n2 |ξ|2\\nHuber’s robust loss\\n{ 1\\n2σ ξ2 if |ξ| ≤ σ\\n|ξ| − σ\\n2 otherwise\\nPinball loss\\n{τ ξ if ξ ≥ 0\\n(τ − 1)ξ otherwise.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d52de071-dde5-4104-87ad-6e71d2c2fbd0', embedding=None, metadata={'page_label': '186', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='186 7 Linear Models\\nobserve that ϵ = 0 for the pinball loss then (7.62) specializes as follows:\\nmin\\nα\\n1\\n2\\n∑\\ni,j\\nαiαj ⟨xi, xj⟩ −\\nm∑\\ni=1\\nyiαi (7.63a)\\ns.t.\\nm∑\\ni=1\\nαi = 0 (7.63b)\\nC\\nm(τ − 1) ≤ αi ≤ C\\nm τ. (7.63c)\\nSimilar specializations of (7.62) for other loss functions in Table 7.1 can be\\nderived.\\n7.3.2 Incorporating the ν Trick\\nOne can also incorporate the ν trick into support vector regression. The\\nprimal problem obtained after incorporating the ν trick can be written as\\nmin\\nw,b,ξ+,ξ−,ϵ\\n1\\n2 ∥w∥2 +\\n(\\nϵ + 1\\nνm\\nm∑\\ni=1\\n(ξ+\\ni + ξ−\\ni )\\n)\\n(7.64a)\\ns.t. ( ⟨w, xi⟩ + b) − yi ≤ ϵ + ξ+\\ni for all i (7.64b)\\nyi − (⟨w, xi⟩ + b) ≤ ϵ + ξ−\\ni for all i (7.64c)\\nξ+\\ni ≥ 0, ξ−\\ni ≥ 0, and ϵ ≥ 0. (7.64d)\\nProceeding as before we obtain the following simpliﬁed dual\\nmin\\nα+,α−\\n1\\n2\\n∑\\ni,j\\n(α−\\ni − α+\\ni )(α−\\nj − α+\\nj ) ⟨xi, xj⟩ −\\nm∑\\ni=1\\nyi(α−\\ni − α+\\ni ) (7.65a)\\ns.t.\\nm∑\\ni=1\\n(α−\\ni − α+\\ni ) = 0 (7.65b)\\nm∑\\ni=1\\n(α−\\ni + α+\\ni ) = 1 (7.65c)\\n0 ≤ α+\\ni ≤ 1\\nνm (7.65d)\\n0 ≤ α−\\ni ≤ 1\\nνm . (7.65e)\\n7.4 Novelty Detection\\nThe large margin approach can also be adapted to perform novelty detection\\nor quantile estimation. Novelty detection is an unsupervised task where one', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8e1203cd-d6d2-48ab-93d4-7f541f4b186b', embedding=None, metadata={'page_label': '187', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.4 Novelty Detection 187\\nis interested in ﬂagging a small fraction of the input X = {x1, . . . , xm} as\\natypical or novel. It can be viewed as a special case of the quantile estimation\\ntask, where we are interested in estimating a simple set C such that P r(x ∈\\nC) ≥ µ for some µ ∈ [0, 1]. One way to measure simplicity is to use the\\nvolume of the set. Formally, if |C| denotes the volume of a set, then the\\nquantile estimation task is to estimate\\narginf{|C| s.t. P r(x ∈ C) ≥ µ}. (7.66)\\nGiven the input data X one can compute the empirical density\\nˆp(x) =\\n{\\n1\\nm if x ∈ X\\n0 otherwise ,\\nand estimate its (not necessarily unique) µ-quantiles. Unfortunately, such\\nestimates are very brittle and do not generalize well to unseen data. One\\npossible way to address this issue is to restrict C to be simple subsets such\\nas spheres or half spaces. In other words, we estimate simple sets which\\ncontain µ fraction of the dataset. For our purposes, we speciﬁcally work\\nwith half-spaces deﬁned by hyperplanes. While half-spaces may seem rather\\nrestrictive remember that the kernel trick can be used to map data into\\na high-dimensional space; half-spaces in the mapped space correspond to\\nnon-linear decision boundaries in the input space. Furthermore, instead of\\nexplicitly identifying C we will learn an indicator function for C, that is, a\\nfunction f which takes on values −1 inside C and −1 elsewhere.\\nWith 1\\n2 ∥w∥2 as a regularizer, the problem of estimating a hyperplane such\\nthat a large fraction of the points in the input data X lie on one of its sides\\ncan be written as:\\nmin\\nw,ξ,ρ\\n1\\n2 ∥w∥2 + 1\\nνm\\nm∑\\ni=1\\nξi − ρ (7.67a)\\ns.t. ⟨w, xi⟩ ≥ ρ − ξi for all i (7.67b)\\nξi ≥ 0. (7.67c)\\nClearly, we want ρ to be as large as possible so that the volume of the half-\\nspace ⟨w, x⟩ ≥ ρ is minimized. Furthermore, ν ∈ [0, 1] is a parameter which\\nis analogous to ν we introduced for the ν-SVM earlier. Roughly speaking,\\nit denotes the fraction of input data for which ⟨w, xi⟩ ≤ ρ. An alternative\\ninterpretation of (7.67) is to assume that we are separating the data set X\\nfrom the origin (See Figure 7.7 for an illustration). Therefore, this method\\nis also widely known as the one-class SVM.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='90b41ac7-7860-4341-b099-7612f2ee8925', embedding=None, metadata={'page_label': '188', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='188 7 Linear Models\\nFig. 7.7. The novelty detection problem can be viewed as ﬁnding a large margin\\nhyperplane which separates ν fraction of the data points away from the origin.\\nThe Lagrangian of (7.67) can be written by introducing non-negative\\nLagrange multipliers αi, and βi:\\nL(w, ξ, ρ, α, β) = 1\\n2 ∥w∥2 + 1\\nνm\\nm∑\\ni=1\\nξi − ρ +\\nm∑\\ni=1\\nαi(ρ − ξi − ⟨w, xi⟩) −\\nm∑\\ni=1\\nβiξi.\\nBy taking gradients with respect to the primal variables and setting them\\nto 0 we obtain\\nw =\\nm∑\\ni=1\\nαixi (7.68)\\nαi = 1\\nνm − βi ≤ 1\\nνm (7.69)\\nm∑\\ni=1\\nαi = 1. (7.70)\\nNoting that αi, βi ≥ 0 and substituting the above conditions into the La-\\ngrangian yields the dual\\nmin\\nα\\n1\\n2\\n∑\\ni,j\\nαiαj ⟨xi, xj⟩ (7.71a)\\ns.t. 0 ≤ αi ≤ 1\\nνm (7.71b)\\nm∑\\ni=1\\nαi = 1. (7.71c)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='947400ff-db18-4c3e-b48f-8735363b07cd', embedding=None, metadata={'page_label': '189', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.5 Margins and Probability 189\\nThis can easily be solved by a straightforward modiﬁcation of the SMO\\nalgorithm (see Section 7.1.3 and Problem 7.7). Like in the previous sections,\\nan analysis of the KKT conditions shows that 0< α if and only if ⟨w, xi⟩ ≤ ρ;\\nsuch points are called support vectors. As before, we can replace ⟨xi, xj⟩ by\\na kernel k(xi, xj) to transform half-spaces in the feature space to non-linear\\nshapes in the input space. The following theorem explains the signiﬁcance\\nof the parameter ν.\\nTheorem 7.5 Assume that the solution of (7.71) satisﬁes ρ ̸= 0, then the\\nfollowing statements hold:\\n(i) ν is an upper bound on the fraction of support vectors, that is points\\nfor which ⟨w, xi⟩ ≤ ρ.\\n(ii) Suppose the data X were generated independently from a distribution\\np(x) which does not contain discrete components. Moreover, assume\\nthat the kernel k is analytic and non-constant. With probability 1,\\nasympotically, ν equals the fraction of support vectors.\\n7.5 Margins and Probability\\ndiscuss the connection between probabilistic models and linear classiﬁers.\\nissues of consistency, optimization, eﬃciency, etc.\\n7.6 Beyond Binary Classiﬁcation\\nIn contrast to binary classiﬁcation where there are only two possible ways\\nto label a training sample, in some of the extensions we discuss below each\\ntraining sample may be associated with one or more of k possible labels.\\nTherefore, we will use the decision function\\ny∗ = argmax\\ny∈{1,...,k}\\nf(x, y) where f(x, y) = ⟨φ(x, y), w⟩ . (7.72)\\nRecall that the joint feature map φ(x, y) was introduced in section 7.1.2.\\nOne way to interpret the above equation is to viewf(x, y) as a compatibility\\nscore between instance x and label y; we assign the label with the highest\\ncompatibility score to x. There are a number of extensions of the binary\\nhinge loss (7.13) which can be used to estimate this score function. In all\\nthese cases the objective function is written as\\nmin\\nw\\nJ(w) := λ\\n2 ∥w∥2 + 1\\nm\\nm∑\\ni=1\\nl(w, xi, yi). (7.73)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cd97d78a-3e82-4974-8d47-561fe69e40d0', embedding=None, metadata={'page_label': '190', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='190 7 Linear Models\\nHere λ is a scalar which trades oﬀ the regularizer 1\\n2 ∥w∥2 with the empirical\\nrisk 1\\nm\\n∑m\\ni=1 l(w, xi, yi). Plugging in diﬀerent loss functions yields classiﬁers\\nfor diﬀerent settings. Two strategies exist for ﬁnding the optimal w. Just\\nlike in the binary SVM case, one can compute and maximize the dual of\\n(7.73). However, the number of dual variables becomes m|Y|, where m is the\\nnumber of training points and|Y| denotes the size of the label set. The second\\nstrategy is to optimize (7.73) directly. However, the loss functions we discuss\\nbelow are non-smooth, therefore non-smooth optimization algorithms such\\nas bundle methods (section 3.2.7) need to be used.\\n7.6.1 Multiclass Classiﬁcation\\nIn multiclass classiﬁcation a training example is labeled with one of k pos-\\nsible labels, that is, Y = {1, . . . , k}. We discuss two diﬀerent extensions of\\nthe binary hinge loss to the multiclass setting. It can easily be veriﬁed that\\nsetting Y = {±1} and φ(x, y) = y\\n2 φ(x) recovers the binary hinge loss in both\\ncases.\\n7.6.1.1 Additive Multiclass Hinge Loss\\nA natural generalization of the binary hinge loss is to penalize all labels\\nwhich have been misclassiﬁed. The loss can now be written as\\nl(w, x, y) =\\n∑\\ny′̸=y\\nmax\\n(\\n0, 1 − (\\n⟨\\nφ(x, y) − φ(x, y′), w\\n⟩\\n)\\n)\\n. (7.74)\\n7.6.1.2 Maximum Multiclass Hinge Loss\\nAnother variant of (7.13) penalizes only the maximally violating label:\\nl(w, x, y) := max\\n(\\n0, max\\ny′̸=y\\n(1 −\\n⟨\\nφ(x, y) − φ(x, y′), w\\n⟩\\n)\\n)\\n. (7.75)\\nNote that both (7.74) and (7.75) are zero whenever\\nf(x, y) = ⟨φ(x, y), w⟩ ≥ 1 + max\\ny′̸=y\\n⟨\\nφ(x, y′), w\\n⟩\\n= 1 + max\\ny′̸=y\\nf(x, y′). (7.76)\\nIn other words, they both ensure an adequate margin of separation, in this\\ncase 1, between the score of the true label f(x, y) and every other label\\nf(x, y′). However, they diﬀer in the way they penalize violators, that is, la-\\nbels y′ ̸= y for which f(x, y) ≤ 1 + f(x, y′). In one case we linearly penalize\\nthe violators and sum up their contributions while in the other case we lin-\\nearly penalize only the maximum violator. In fact, (7.75) can be interpreted', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e67c2fd2-b419-446e-8719-5c5c651fcd48', embedding=None, metadata={'page_label': '191', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.6 Beyond Binary Classiﬁcation 191\\nas the log odds ratio in the exponential family. Towards this end choose η\\nsuch that log η = 1 and rewrite (7.20):\\nlog p(y|x, w)\\nmaxy′̸=y p(y′|x, w) =\\n⟨\\nφ(x, y) − max\\ny′̸=y\\nφ(x, y′), w\\n⟩\\n≥ 1.\\nRearranging yields (7.76).\\n7.6.2 Multilabel Classiﬁcation\\nIn multilabel classiﬁcation one or more of k possible labels are assigned to\\na training example. Just like in the multiclass case two diﬀerent losses can\\nbe deﬁned.\\n7.6.2.1 Additive Multilabel Hinge Loss\\nIf we let Yx ⊆ Y denote the labels assigned to x, and generalize the hinge\\nloss to penalize all labels y′ /∈ Yx which have been assigned higher score than\\nsome y ∈ Yx, then the loss can be written as\\nl(w, x, y) =\\n∑\\ny∈Yx and y′ /∈Yx\\nmax\\n(\\n0, 1 − (\\n⟨\\nφ(x, y) − φ(x, y′), w\\n⟩\\n)\\n)\\n. (7.77)\\n7.6.2.2 Maximum Multilabel Hinge Loss\\nAnother variant only penalizes the maximum violating pair. In this case the\\nloss can be written as\\nl(w, x, y) = max\\n(\\n0, max\\ny∈Yx,y′ /∈Yx\\n[\\n1 −\\n(⟨\\nφ(x, y) − φ(x, y′), w\\n⟩)])\\n. (7.78)\\nOne can immediately verify that specializing the above losses to the mul-\\nticlass case recovers (7.74) and (7.75) respectively, while the binary case\\nrecovers (7.13). The above losses are zero only when\\nmin\\ny∈Yx\\nf(x, y) = min\\ny∈Yx\\n⟨φ(x, y), w⟩ ≥ 1 + max\\ny′ /∈Yx\\n⟨\\nφ(x, y′), w\\n⟩\\n= 1 + max\\ny′ /∈Yx\\nf(x, y′).\\nThis can be interpreted as follows: The losses ensure that all the labels\\nassigned to x have larger scores compared to labels not assigned to x with\\nthe margin of separation of at least 1.\\nAlthough the above loss functions are compatible with multiple labels,\\nthe prediction function argmax y f(x, y) only takes into account the label\\nwith the highest score. This is a signiﬁcant drawback of such models, which\\ncan be overcome by using a multiclass approach instead. Let |Y| be the\\nsize of the label set and z ∈ R|Y| denote a vector with ±1 entries. We set', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='aafb609d-9af9-49a3-9306-6050fbed0233', embedding=None, metadata={'page_label': '192', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='192 7 Linear Models\\nzy = +1 if the y ∈ Yx and zy = −1 otherwise, and use the multiclass loss\\n(7.75) on z. To predict we compute z∗ = argmax z f(x, z) and assign to x\\nthe labels corresponding to components of z∗ which are +1. Since z can\\ntake on 2|Y| possible values, this approach is not feasible if |Y| is large. To\\ntackle such problems, and to further reduce the computational complexity\\nwe assume that the labels correlations are captured via a |Y| × |Y| positive\\nsemi-deﬁnite matrix P , and φ(x, y) can be written as φ(x) ⊗ P y. Here ⊗\\ndenotes the Kronecker product. Furthermore, we express the vector w as\\na n × |Y| matrix W , where n denotes the dimension of φ(x). With these\\nassumptions ⟨φ(x) ⊗ P (z − z′), w⟩ can be rewritten as\\n⣨\\nφ(x)⊤W P,(z − z′)\\n⟩\\n=\\n∑\\ni\\n[\\nφ(x)⊤W P\\n]\\ni\\n(zi − z′\\ni),\\nand (7.78) specializes to\\nl(w, x, z) := max\\n(\\n0,\\n(\\n1 −\\n∑\\ni\\nmin\\nz′\\ni̸=zi\\n[\\nφ(x)⊤W P\\n]\\ni\\n(zi − z′\\ni)\\n))\\n. (7.79)\\nA analogous specialization of (7.77) can also be derived wherein the mini-\\nmum is replaced by a summation. Since the minimum (or summation as the\\ncase may be) is over |Y| possible labels, computing the loss is tractable even\\nif the set of labels Y is large.\\n7.6.3 Ordinal Regression and Ranking\\nWe can generalize our above discussion to consider slightly more general\\nranking problems. Denote by Y the set of all directed acyclic graphs on N\\nnodes. The presence of an edge ( i, j) in y ∈ Y indicates that i is preferred\\nto j. The goal is to ﬁnd a function f(x, i) which imposes a total order on\\n{1, . . . , N} which is in close agreement with y. Speciﬁcally, if the estimation\\nerror is given by the number of subgraphs of y which are in disagreement\\nwith the total order imposed by f, then the additive version of the loss can\\nbe written as\\nl(w, x, y) =\\n∑\\nG∈A(y)\\nmax\\n(i,j)∈G\\n(0, 1 − (f(x, i) − f(x, j))) , (7.80)\\nwhere A(y) denotes the set of all possible subgraphs of y. The maximum\\nmargin version, on the other hand, is given by\\nl(w, x, y) = max\\nG∈A(y)\\nmax\\n(i,j)∈G\\n(0, 1 − (f(x, i) − f(x, j))) . (7.81)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6a5213e7-391e-40e7-8fe3-538e86dfc9bb', embedding=None, metadata={'page_label': '193', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.7 Large Margin Classiﬁers with Structure 193\\nIn other words, we test for each subgraphG of y if the ranking imposed by G\\nis satisﬁed by f. Selecting speciﬁc types of directed acyclic graphs recovers\\nthe multiclass and multilabel settings (problem 7.9).\\n7.7 Large Margin Classiﬁers with Structure\\n7.7.1 Margin\\ndeﬁne margin pictures\\n7.7.2 Penalized Margin\\ndiﬀerent types of loss, rescaling\\n7.7.3 Nonconvex Losses\\nthe max - max loss\\n7.8 Applications\\n7.8.1 Sequence Annotation\\n7.8.2 Matching\\n7.8.3 Ranking\\n7.8.4 Shortest Path Planning\\n7.8.5 Image Annotation\\n7.8.6 Contingency Table Loss\\n7.9 Optimization\\n7.9.1 Column Generation\\nsubdiﬀerentials\\n7.9.2 Bundle Methods\\n7.9.3 Overrelaxation in the Dual\\nwhen we cannot do things exactly', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b4f490bb-b0e7-4f35-911a-514134cc8737', embedding=None, metadata={'page_label': '194', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='194 7 Linear Models\\n7.10 CRFs vs Structured Large Margin Models\\n7.10.1 Loss Function\\n7.10.2 Dual Connections\\n7.10.3 Optimization\\nProblems\\nProblem 7.1 (Deriving the Margin {1}) Show that the distance of a\\npoint xi to a hyperplane H = {x| ⟨w, x⟩ + b = 0} is given by | ⟨w, xi⟩ +\\nb |/ ∥w∥.\\nProblem 7.2 (SVM without Bias {1}) A homogeneous hyperplane is one\\nwhich passes through the origin, that is,\\nH = {x| ⟨w, x⟩ = 0}. (7.82)\\nIf we devise a soft margin classiﬁer which uses the homogeneous hyperplane\\nas a decision boundary, then the corresponding primal optimization problem\\ncan be written as follows:\\nmin\\nw,ξ\\n1\\n2 ∥w∥2 + C\\nm∑\\ni=1\\nξi (7.83a)\\ns.t. yi ⟨w, xi⟩ ≥ 1 − ξi for all i (7.83b)\\nξi ≥ 0, (7.83c)\\nDerive the dual of (7.83) and contrast it with (7.9). What changes to the\\nSMO algorithm would you make to solve this dual?\\nProblem 7.3 (Deriving the simpliﬁed ν-SVM dual {2}) In Lemma 7.2\\nwe used (7.41) to show that the constraint ∑\\ni αi ≥ 1 can be replaced by∑\\ni αi = 1. Show that an equivalent way to arrive at the same conclusion is\\nby arguing that the constraint ρ ≥ 0 is redundant in the primal (7.40). Hint:\\nObserve that whenever ρ < 0 the objective function is always non-negative.\\nOn the other hand, setting w = ξ = b = ρ = 0 yields an objective function\\nvalue of 0.\\nProblem 7.4 (Fenchel and Lagrange Duals {2}) We derived the La-\\ngrange dual of (7.12) in Section 7.1 and showed that it is (7.9). Derive the\\nFenchel dual of (7.12) and relate it to (7.9). Hint: See theorem 3.3.5 of\\n[BL00].', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d353edb6-9ba4-4a25-b440-bcb3585827b2', embedding=None, metadata={'page_label': '195', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7.10 CRFs vs Structured Large Margin Models 195\\nProblem 7.5 (Dual of the square hinge loss {1}) The analog of (7.5)\\nwhen working with the square hinge loss is the following\\nmin\\nw,b,ξ\\n1\\n2 ∥w∥2 + C\\nm\\nm∑\\ni=1\\nξ2\\ni (7.84a)\\ns.t. yi(⟨w, xi⟩ + b) ≥ 1 − ξi for all i (7.84b)\\nξi ≥ 0, (7.84c)\\nDerive the Lagrange dual of the above optimization problem and show that\\nit a Quadratic Programming problem.\\nProblem 7.6 (Dual of the ramp loss {1}) Derive the Lagrange dual of\\n(7.49) and show that it the Quadratic Programming problem (7.50).\\nProblem 7.7 (SMO for various SVM formulations {2}) Derive an SMO\\nlike decomposition algorithm for solving the dual of the following problems:\\n• ν-SVM (7.41).\\n• SV regression (7.57).\\n• SV novelty detection (7.71).\\nProblem 7.8 (Novelty detection with Balls {2}) In Section 7.4 we as-\\nsumed that we wanted to estimate a halfspace which contains a major frac-\\ntion of the input data. An alternative approach is to use balls, that is, we\\nestimate a ball of small radius in feature space which encloses a majority of\\nthe input data. Write the corresponding optimization problem and its dual.\\nShow that if the kernel is translation invariant, that is, k(x, x′) depends only\\non ∥x − x′∥ then the optimization problem with balls is equivalent to (7.71).\\nExplain why this happens geometrically.\\nProblem 7.9 (Multiclass and Multilabel loss from Ranking Loss {1})\\nShow how the multiclass (resp. multilabel) losses (7.74) and (7.75) (resp.\\n(7.77) and (7.79)) can be derived as special cases of (7.80) and (7.81) re-\\nspectively.\\nProblem 7.10 Invariances (basic loss)\\nProblem 7.11 Polynomial transformations - SDP constraints', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f23dc73b-2b34-4113-919c-a211c7046ba1', embedding=None, metadata={'page_label': '196', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5da5a8ce-47e8-4f34-96c4-637e6591c643', embedding=None, metadata={'page_label': '197', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix 1\\nLinear Algebra and Functional Analysis\\nA1.1 Johnson Lindenstrauss Lemma\\nLemma 1.1 (Johnson Lindenstrauss) Let X be a set of n points in Rd\\nrepresented as a n × d matrix A. Given ϵ, β > 0 let\\nk ≥ 4 + 2β\\nϵ2/2 − ϵ3/3 log n (1.1)\\nbe a positive integer. Construct a d × k random matrix R with independent\\nstandard normal random variables, that is, Rij ∼ N(0, 1), and let\\nE = 1√\\nk\\nAR. (1.2)\\nDeﬁne f : Rd → Rk as the function which maps the rows of A to the rows\\nof E. With probability at least 1 − n−β, for all u, v ∈ X we have\\n(1 − ϵ) ∥u − v∥2 ≤ ∥f(u) − f(v)∥2 ≤ (1 + ϵ) ∥u − v∥2 . (1.3)\\nOur proof presentation by and large follows [ ?]. We ﬁrst show that\\nLemma 1.2 For any arbitrary vector α ∈ Rd let qi denote the i-th compo-\\nnent of f(α). Then qi ∼ N(0, ∥α∥2 /k) and hence\\nE\\n[\\n∥f(α)∥2\\n]\\n=\\nk∑\\ni=1\\nE\\n[\\nq2\\ni\\n]\\n= ∥α∥2 . (1.4)\\nIn other words, the expected length of vectors are preserved even after em-\\nbedding them in a k dimensional space. Next we show that the lengths of\\nthe embedded vectors are tightly concentrated around their mean.\\nLemma 1.3 For any ϵ > 0 and any unit vector α ∈ Rd we have\\nP r\\n(\\n∥f(α)∥2 > 1 + ϵ\\n)\\n< exp\\n(\\n− k\\n2\\n(\\nϵ2/2 − ϵ3/3\\n))\\n(1.5)\\nP r\\n(\\n∥f(α)∥2 < 1 − ϵ\\n)\\n< exp\\n(\\n− k\\n2\\n(\\nϵ2/2 − ϵ3/3\\n))\\n. (1.6)\\n197', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ee079066-c674-40a8-9b88-a04925d6e6d1', embedding=None, metadata={'page_label': '198', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='198 1 Linear Algebra and Functional Analysis\\nCorollary 1.4 If we choose k as in (1.1) then for any α ∈ Rd we have\\nP r\\n(\\n(1 − ϵ) ∥α∥2 ≤ ∥f(α)∥2 ≤ (1 + ϵ) ∥α∥2\\n)\\n≥ 1 − 2\\nn2+β . (1.7)\\nProof Follows immediately from Lemma 1.3 by setting\\n2 exp\\n(\\n− k\\n2\\n(\\nϵ2/2 − ϵ3/3\\n))\\n≤ 2\\nn2+β ,\\nand solving for k.\\nThere are\\n(n\\n2\\n)\\npairs of vectors u, v in X, and their corresponding distances\\n∥u − v∥ are preserved within 1 ± ϵ factor as shown by the above lemma.\\nTherefore, the probability of not satisfying (1.3) is bounded by\\n(n\\n2\\n)\\n· 2\\nn2+β <\\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\\nto prove Lemma 1.2 and 1.3.\\nProof (Lemma 1.2). Since qi = 1√\\nk\\n∑\\nj Rijαj is a linear combination of stan-\\ndard normal random variables Rij it follows that qi is normally distributed.\\nTo compute the mean note that\\nE [qi] = 1√\\nk\\n∑\\nj\\nαj E [Rij] = 0.\\nSince Rij are independent zero mean unit variance random variables,E [RijRil] =\\n1 if j = l and 0 otherwise. Using this\\nE\\n[\\nq2\\ni\\n]\\n= 1\\nk E\\n\\uf8eb\\n\\uf8ed\\nd∑\\nj=1\\nRijαj\\n\\uf8f6\\n\\uf8f8\\n2\\n= 1\\nk\\nd∑\\nj=1\\nd∑\\nl=1\\nαjαl E [RijRil] = 1\\nk\\nd∑\\nj=1\\nα2\\nj = 1\\nk ∥α∥2 .\\nProof (Lemma 1.3). Clearly, for all λ\\nP r\\n[\\n∥f(α)∥2 > 1 + ϵ\\n]\\n= P r\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)\\n> exp(λ(1 + ϵ))\\n]\\n.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e99eaae6-1b86-455f-8dbb-150510f1c4e1', embedding=None, metadata={'page_label': '199', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A1.1 Johnson Lindenstrauss Lemma 199\\nUsing Markov’s inequality (P r[X ≥ a] ≤ E[X]/a) we obtain\\nP r\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)\\n> exp(λ(1 + ϵ))\\n]\\n≤\\nE\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)]\\nexp(λ(1 + ϵ))\\n=\\nE\\n[\\nexp\\n(\\nλ ∑k\\ni=1 q2\\ni\\n)]\\nexp(λ(1 + ϵ))\\n=\\nE\\n[∏k\\ni=1 exp\\n(\\nλq2\\ni\\n)]\\nexp(λ(1 + ϵ))\\n=\\n(\\nE\\n[\\nexp\\n(\\nλq2\\ni\\n)]\\nexp\\n(λ\\nk (1 + ϵ)\\n)\\n)k\\n. (1.8)\\nThe last equality is because the qi’s are i.i.d. Since α is a unit vector, from\\nthe previous lemma qi ∼ N(0, 1/k). Therefore, kq2\\ni is a χ2 random variable\\nwith moment generating function\\nE\\n[\\nexp\\n(\\nλq2\\ni\\n)]\\n= E\\n[\\nexp\\n(λ\\nk kq2\\ni\\n)]\\n= 1√\\n1 − 2λ\\nk\\n.\\nPlugging this into (1.8)\\nP r\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)\\n> exp (λ(1 + ϵ))\\n]\\n≤\\n\\uf8eb\\n\\uf8edexp\\n(\\n− λ\\nk (1 + ϵ)\\n)\\n√\\n1 − 2λ\\nk\\n\\uf8f6\\n\\uf8f8\\nk\\n.\\nSetting λ = kϵ\\n2(1+ϵ) in the above inequality and simplifying\\nP r\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)\\n> exp(λ(1 + ϵ))\\n]\\n≤ (exp(−ϵ)(1 + ϵ))k/2 .\\nUsing the inequality\\nlog(1 + ϵ) < ϵ − ϵ2/2 + ϵ3/3\\nwe can write\\nP r\\n[\\nexp\\n(\\nλ ∥f(α)∥2\\n)\\n> exp(λ(1 + ϵ))\\n]\\n≤ exp\\n(\\n− k\\n2\\n(\\nϵ2/2 − ϵ3/3\\n))\\n.\\nThis proves (1.5). To prove (1.6) we need to repeat the above steps and use\\nthe inequality\\nlog(1 − ϵ) < −ϵ − ϵ2/2.\\nThis is left as an exercise to the reader.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5f8379ca-7173-4447-ba0a-428292742c2a', embedding=None, metadata={'page_label': '200', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='200 1 Linear Algebra and Functional Analysis\\nA1.2 Spectral Properties of Matrices\\nA1.2.1 Basics\\nA1.2.2 Special Matrices\\nunitary, hermitean, positive semideﬁnite\\nA1.2.3 Normal Forms\\nJacobi\\nA1.3 Functional Analysis\\nA1.3.1 Norms and Metrics\\nvector space, norm, triangle inequality\\nA1.3.2 Banach Spaces\\nnormed vector space, evaluation functionals, examples, dual space\\nA1.3.3 Hilbert Spaces\\nsymmetric inner product\\nA1.3.4 Operators\\nspectrum, norm, bounded, unbounded operators\\nA1.4 Fourier Analysis\\nA1.4.1 Basics\\nA1.4.2 Operators', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='735a94a9-3f9d-4e46-97d0-c01d330936fa', embedding=None, metadata={'page_label': '201', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix 2\\nConjugate Distributions\\n201', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='46c481da-0f0b-4e53-8cf6-b95a4fd87dcf', embedding=None, metadata={'page_label': '202', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='202 2 Conjugate Distributions\\nBinomial — Beta\\nφ(x) = x\\neh(nν,n) = Γ(nν + 1)Γ(n(1 − ν) + 1)\\nΓ(n + 2) = B(nν + 1, n(1 − ν) + 1)\\nIn traditional notation one represents the conjugate as\\np(z; α, β) = Γ(α + β)\\nΓ(α)Γ(β) zα−1(1 − z)β−1\\nwhere α = nν + 1 and β = n(1 − bν) + 1.\\nMultinomial — Dirichlet\\nφ(x) = ex\\neh(nν,n) =\\n∏d\\ni=1 Γ(nνi + 1)\\nΓ(n + d)\\nIn traditional notation one represents the conjugate as\\np(z; α) = Γ(∑d\\ni=1 αi)∏d\\ni=1 Γ(αi)\\nd∏\\ni=1\\nzαi−1\\ni\\nwhere αi = nνi + 1\\nPoisson — Gamma\\nφ(x) = x\\neh(nν,n) = n−nνΓ(nν)\\nIn traditional notation one represents the conjugate as\\np(z; α) = β−αΓ(α)zα−1e−βx\\nwhere α = nν and β = n.\\n• Multinomial / Binomial\\n• Gaussian\\n• Laplace\\n• Poisson\\n• Dirichlet\\n• Wishart\\n• Student-t\\n• Beta\\n• Gamma', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='97025909-1531-4593-b921-4598d24d7c5e', embedding=None, metadata={'page_label': '203', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix 3\\nLoss Functions\\nA3.1 Loss Functions\\nA multitude of loss functions are commonly used to derive seemingly diﬀer-\\nent algorithms. This often blurs the similarities as well as subtle diﬀerences\\nbetween them, often for historic reasons: Each new loss is typically accompa-\\nnied by at least one publication dedicated to it. In many cases, the loss is not\\nspelled out explicitly either but instead, it is only given by means of a con-\\nstrained optimization problem. A case in point are the papers introducing\\n(binary) hinge loss [BM92, CV95] and structured loss [TGK04, TJHA05].\\nLikewise, a geometric description obscures the underlying loss function, as\\nin novelty detection [SPST+01].\\nIn this section we give an expository yet unifying presentation of many\\nof those loss functions. Many of them are well known, while others, such\\nas multivariate ranking, hazard regression, or Poisson regression are not\\ncommonly used in machine learning. Tables A3.1 and A3.1 contain a choice\\nsubset of simple scalar and vectorial losses. Our aim is to put the multitude\\nof loss functions in an uniﬁed framework, and to show how these losses\\nand their (sub)gradients can be computed eﬃciently for use in our solver\\nframework.\\nNote that not all losses, while convex, are continuously diﬀerentiable. In\\nthis situation we give a subgradient. While this may not be optimal, the\\nconvergence rates of our algorithm do not depend on which element of the\\nsubdiﬀerential we provide: in all cases the ﬁrst order Taylor approximation\\nis a lower bound which is tight at the point of expansion.\\nIn this setion, with little abuse of notation, vi is understood as the i-th\\ncomponent of vector v when v is clearly not an element of a sequence or a\\nset.\\nA3.1.1 Scalar Loss Functions\\nIt is well known [Wah97] that the convex optimization problem\\nmin\\nξ\\nξ subject to y ⟨w, x⟩ ≥ 1 − ξ and ξ ≥ 0 (3.1)\\n203', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='213f3a00-ed9d-4f5d-9a3d-a92e3d22953f', embedding=None, metadata={'page_label': '204', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='204 3 Loss Functions\\nScalar loss functions and their derivatives, depending onf:=⟨w, x⟩, andy.\\nLoss¯l(f, y) Derivative¯l′(f, y)\\nHinge [BM92] max(0,1−yf) 0 ifyf≥1 and−yotherwise\\nSquared Hinge [KD05]1\\n2max(0,1−yf)20 ifyf≥1 andf−yotherwise\\nExponential [CDLS99] exp(−yf)−yexp(−yf)\\nLogistic [CSS00] log(1 + exp(−yf))−y/(1 + exp(−yf))\\nNovelty [SPST+01] max(0, ρ−f) 0 iff≥ρand−1 otherwise\\nLeast mean squares [Wil98]1\\n2(f−y)2f−y\\nLeast absolute deviation|f−y|sign(f−y)\\nQuantile regression [Koe05] max(τ(f−y),(1−τ)(y−f))τiff > yandτ−1 otherwise\\nϵ-insensitive [VGS97] max(0,|f−y| −ϵ) 0 if|f−y| ≤ϵ, else sign(f−y)\\nHuber’s robust loss [MSR+97]1\\n2(f−y)2if|f−y| ≤1, else|f−y| −1\\n2f−yif|f−y| ≤1, else sign(f−y)\\nPoisson regression [Cre93] exp(f)−yfexp(f)−y\\nVectorial loss functions and their derivatives, depending on the vectorf:=W xand ony.\\nLoss Derivative\\nSoft-Margin Multiclass [TGK04] maxy′(fy′−fy+ ∆(y, y′))ey∗−ey\\n[CS03] wherey∗is the argmax of the loss\\nScaled Soft-Margin Multiclass maxy′Γ(y, y′)(fy′−fy+ ∆(y, y′)) Γ(y, y′)(ey∗−ey)\\n[TJHA05] wherey∗is the argmax of the loss\\nSoftmax Multiclass [CDLS99] log∑\\ny′exp(fy′)−fy\\n[∑\\ny′ey′exp(f′\\ny)\\n]\\n/∑\\ny′exp(f′\\ny)−ey\\nMultivariate Regression1\\n2(f−y)⊤M(f−y) whereM⪰0M(f−y)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e6baa763-5597-4a9b-a4e8-994b7b793e2a', embedding=None, metadata={'page_label': '205', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 205\\ntakes on the value max(0 , 1 − y ⟨w, x⟩). The latter is a convex function in\\nw and x. Likewise, we may rewrite the ϵ-insensitive loss, Huber’s robust\\nloss, the quantile regression loss, and the novelty detection loss in terms of\\nloss functions rather than a constrained optimization problem. In all cases,\\n⟨w, x⟩ will play a key role insofar as the loss is convex in terms of the scalar\\nquantity ⟨w, x⟩. A large number of loss functions fall into this category,\\nas described in Table A3.1. Note that not all functions of this type are\\ncontinuously diﬀerentiable. In this case we adopt the convention that\\n∂x max(f(x), g(x)) =\\n{\\n∂xf(x) if f(x) ≥ g(x)\\n∂xg(x) otherwise .\\n(3.2)\\nSince we are only interested in obtaining an arbitrary element of the subd-\\niﬀerential this convention is consistent with our requirements.\\nLet us discuss the issue of eﬃcient computation. For all scalar losses we\\nmay write l(x, y, w) = ¯l(⟨w, x⟩ , y), as described in Table A3.1. In this case a\\nsimple application of the chain rule yields that ∂wl(x, y, w) = ¯l′(⟨w, x⟩ , y)·x.\\nFor instance, for squared loss we have\\n¯l(⟨w, x⟩ , y) = 1\\n2(⟨w, x⟩ − y)2 and ¯l′(⟨w, x⟩ , y) = ⟨w, x⟩ − y.\\nConsequently, the derivative of the empirical risk term is given by\\n∂wRemp(w) = 1\\nm\\nm∑\\ni=1\\n¯l′(⟨w, xi⟩ , yi) · xi. (3.3)\\nThis means that if we want to compute l and ∂wl on a large number of\\nobservations xi, represented as matrix X, we can make use of fast linear\\nalgebra routines to pre-compute the vectors\\nf = Xw and g⊤X where gi = ¯l′(fi, yi). (3.4)\\nThis is possible for any of the loss functions listed in Table A3.1, and many\\nother similar losses. The advantage of this uniﬁed representation is that im-\\nplementation of each individual loss can be done in very little time. The\\ncomputational infrastructure for computing Xw and g⊤X is shared. Eval-\\nuating ¯l(fi, yi) and ¯l′(fi, yi) for all i can be done in O(m) time and it is\\nnot time-critical in comparison to the remaining operations. Algorithm 3.1\\ndescribes the details.\\nAn important but often neglected issue is worth mentioning. Computingf\\nrequires us to right multiply the matrix X with the vector w while computing\\ng requires the left multiplication of X with the vector g⊤. If X is stored in a\\nrow major format then Xw can be computed rather eﬃciently while g⊤X is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fd40795f-1396-48f3-be5f-b8b14af7552a', embedding=None, metadata={'page_label': '206', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='206 3 Loss Functions\\nAlgorithm 3.1 ScalarLoss(w, X, y)\\n1: input: Weight vector w, feature matrix X, and labels y\\n2: Compute f = Xw\\n3: Compute r = ∑\\ni ¯l(fi, yi) and g = ¯l′(f, y)\\n4: g ← g⊤X\\n5: return Risk r and gradient g\\nexpensive. This is particularly true ifX cannot ﬁt in main memory. Converse\\nis the case when X is stored in column major format. Similar problems are\\nencountered when X is a sparse matrix and stored in either compressed row\\nformat or in compressed column format.\\nA3.1.2 Structured Loss\\nIn recent years structured estimation has gained substantial popularity in\\nmachine learning [TJHA05, TGK04, BHS +07]. At its core it relies on two\\ntypes of convex loss functions: logistic loss:\\nl(x, y, w) = log\\n∑\\ny′∈Y\\nexp\\n(⟨\\nw, φ(x, y′)\\n⟩)\\n− ⟨w, φ(x, y)⟩ , (3.5)\\nand soft-margin loss:\\nl(x, y, w) = max\\ny′∈Y\\nΓ(y, y′)\\n⟨\\nw, φ(x, y′) − φ(x, y)\\n⟩\\n+ ∆(y, y′). (3.6)\\nHere φ(x, y) is a joint feature map, ∆( y, y′) ≥ 0 describes the cost of mis-\\nclassifying y by y′, and Γ(y, y′) ≥ 0 is a scaling term which indicates by how\\nmuch the large margin property should be enforced. For instance, [TGK04]\\nchoose Γ(y, y′) = 1. On the other hand [TJHA05] suggest Γ(y, y′) = ∆(y, y′),\\nwhich reportedly yields better performance. Finally, [McA07] recently sug-\\ngested generic functions Γ( y, y′).\\nThe logistic loss can also be interpreted as the negative log-likelihood of\\na conditional exponential family model:\\np(y|x; w) := exp(⟨w, φ(x, y)⟩ − g(w|x)), (3.7)\\nwhere the normalizing constant g(w|x), often called the log-partition func-\\ntion, reads\\ng(w|x) := log\\n∑\\ny′∈Y\\nexp\\n(⟨\\nw, φ(x, y′)\\n⟩)\\n. (3.8)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='adefa20e-49d0-4dc0-810d-86b4698d1297', embedding=None, metadata={'page_label': '207', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 207\\nAs a consequence of the Hammersley-Cliﬀord theorem [Jor08] every expo-\\nnential family distribution corresponds to a undirected graphical model. In\\nour case this implies that the labels y factorize according to an undirected\\ngraphical model. A large number of problems have been addressed by this\\nsetting, amongst them named entity tagging [LMP01], sequence alignment\\n[TJHA05], segmentation [RSS+07] and path planning [RBZ06]. It is clearly\\nimpossible to give examples of all settings in this section, nor would a brief\\nsummary do this ﬁeld any justice. We therefore refer the reader to the edited\\nvolume [BHS+07] and the references therein.\\nIf the underlying graphical model is tractable then eﬃcient inference al-\\ngorithms based on dynamic programming can be used to compute (3.5) and\\n(3.6). We discuss intractable graphical models in Section A3.1.2.1, and now\\nturn our attention to the derivatives of the above structured losses.\\nWhen it comes to computing derivatives of the logistic loss, (3.5), we have\\n∂wl(x, y, w) =\\n∑\\ny′ φ(x, y′) exp⟨w, φ(x, y′)⟩∑\\ny′ exp ⟨w, φ(x, y′)⟩ − φ(x, y) (3.9)\\n= Ey′∼p(y′|x)\\n[\\nφ(x, y′)\\n]\\n− φ(x, y). (3.10)\\nwhere p(y|x) is the exponential family model (3.7). In the case of (3.6) we\\ndenote by ¯y(x) the argmax of the RHS, that is\\n¯y(x) := argmax\\ny′\\nΓ(y, y′)\\n⟨\\nw, φ(x, y′) − φ(x, y)\\n⟩\\n+ ∆(y, y′). (3.11)\\nThis allows us to compute the derivative of l(x, y, w) as\\n∂wl(x, y, w) = Γ(y, ¯y(x)) [φ(x, ¯y(x)) − φ(x, y)] . (3.12)\\nIn the case where the loss is maximized for more than one distinct value ¯y(x)\\nwe may average over the individual values, since any convex combination of\\nsuch terms lies in the subdiﬀerential.\\nNote that (3.6) majorizes ∆( y, y∗), where y∗ := argmax y′ ⟨w, φ(x, y′)⟩\\n[TJHA05]. This can be seen via the following series of inequalities:\\n∆(y, y∗) ≤ Γ(y, y∗) ⟨w, φ(x, y∗) − φ(x, y)⟩ + ∆(y, y∗) ≤ l(x, y, w).\\nThe ﬁrst inequality follows because Γ(y, y∗) ≥ 0 and y∗ maximizes ⟨w, φ(x, y′)⟩\\nthus implying that Γ( y, y∗) ⟨w, φ(x, y∗) − φ(x, y)⟩ ≥ 0. The second inequal-\\nity follows by deﬁnition of the loss.\\nWe conclude this section with a simple lemma which is at the heart of\\nseveral derivations of [Joa05]. While the proof in the original paper is far\\nfrom trivial, it is straightforward in our setting:', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4944baa6-3e9b-4fdb-8c76-3d7cb2859383', embedding=None, metadata={'page_label': '208', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='208 3 Loss Functions\\nLemma 3.1 Denote by δ(y, y′) a loss and let φ(xi, yi) be a feature map for\\nobservations (xi, yi) with 1 ≤ i ≤ m. Moreover, denote by X, Y the set of\\nall m patterns and labels respectively. Finally let\\nΦ(X, Y ) :=\\nm∑\\ni=1\\nφ(xi, yi) and ∆(Y, Y′) :=\\nm∑\\ni=1\\nδ(yi, y′\\ni). (3.13)\\nThen the following two losses are equivalent:\\nm∑\\ni=1\\nmax\\ny′\\n⟨\\nw, φ(xi, y′) − φ(xi, yi)\\n⟩\\n+ δ(yi, y′) and max\\nY′\\n⟨\\nw, Φ(X, Y′) − Φ(X, Y )\\n⟩\\n+ ∆(Y, Y′).\\nThis is immediately obvious, since both feature map and loss decompose,\\nwhich allows us to perform maximization over Y′ by maximizing each of its\\nm components. In doing so, we showed that aggregating all data and labels\\ninto a single feature map and loss yields results identical to minimizing\\nthe sum over all individual losses. This holds, in particular, for the sample\\nerror loss of [Joa05]. Also note that this equivalence does not hold whenever\\nΓ(y, y′) is not constant.\\nA3.1.2.1 Intractable Models\\nWe now discuss cases where computing l(x, y, w) itself is too expensive. For\\ninstance, for intractable graphical models, the computation of∑\\ny exp ⟨w, φ(x, y)⟩\\ncannot be computed eﬃciently. [WJ03] propose the use of a convex majoriza-\\ntion of the log-partition function in those cases. In our setting this means\\nthat instead of dealing with\\nl(x, y, w) = g(w|x) − ⟨w, φ(x, y)⟩ where g(w|x) := log\\n∑\\ny\\nexp ⟨w, φ(x, y)⟩\\n(3.14)\\none uses a more easily computable convex upper bound on g via\\nsup\\nµ∈MARG(x)\\n⟨w, µ⟩ + HGauss(µ|x). (3.15)\\nHere MARG( x) is an outer bound on the conditional marginal polytope\\nassociated with the map φ(x, y). Moreover, HGauss(µ|x) is an upper bound\\non the entropy by using a Gaussian with identical variance. More reﬁned\\ntree decompositions exist, too. The key beneﬁt of our approach is that the\\nsolution µ of the optimization problem (3.15) can immediately be used as a\\ngradient of the upper bound. This is computationally rather eﬃcient.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7abc382b-272b-4014-8791-d6f125265b9f', embedding=None, metadata={'page_label': '209', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 209\\nLikewise note that [TGK04] use relaxations when solving structured esti-\\nmation problems of the form\\nl(x, y, w) = max\\ny′\\nΓ(y, y′)\\n⟨\\nw, φ(x, y′) − φ(x, y)\\n⟩\\n+ ∆(y, y′), (3.16)\\nby enlarging the domain of maximization with respect to y′. For instance,\\ninstead of an integer programming problem we might relax the setting to\\na linear program which is much cheaper to solve. This, again, provides an\\nupper bound on the original loss function.\\nIn summary, we have demonstrated that convex relaxation strategies are\\nwell applicable for bundle methods. In fact, the results of the corresponding\\noptimization procedures can be used directly for further optimization steps.\\nA3.1.3 Scalar Multivariate Performance Scores\\nWe now discuss a series of structured loss functions and how they can be\\nimplemented eﬃciently. For the sake of completeness, we give a concise rep-\\nresentation of previous work on multivariate performance scores and ranking\\nmethods. All these loss functions rely on having access to ⟨w, x⟩, which can\\nbe computed eﬃciently by using the same operations as in Section A3.1.1.\\nA3.1.3.1 ROC Score\\nDenote by f = Xw the vector of function values on the training set. It is\\nwell known that the area under the ROC curve is given by\\nAUC(x, y, w) = 1\\nm+m−\\n∑\\nyi<yj\\nI(⟨w, xi⟩ < ⟨w, xj⟩), (3.17)\\nwhere m+ and m− are the numbers of positive and negative observations\\nrespectively, and I(·) is indicator function. Directly optimizing the cost 1 −\\nAUC(x, y, w) is diﬃcult as it is not continuous in w. By using max(0 , 1 +\\n⟨w, xi − xj⟩) as the surrogate loss function for all pairs (i, j) for which yi < y j\\nwe have the following convex multivariate empirical risk\\nRemp(w) = 1\\nm+m−\\n∑\\nyi<yj\\nmax(0, 1 + ⟨w, xi − xj⟩) = 1\\nm+m−\\n∑\\nyi<yj\\nmax(0, 1 + fi − fj).\\n(3.18)\\nObviously, we could compute Remp(w) and its derivative by an O(m2) op-\\neration. However [Joa05] showed that both can be computed in O(m log m)\\ntime using a sorting operation, which we now describe.\\nDenote by c = f − 1\\n2 y an auxiliary variable and let i and j be indices such', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='368d1206-cdea-4615-8b69-02bf3093b378', embedding=None, metadata={'page_label': '210', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='210 3 Loss Functions\\nAlgorithm 3.2 ROCScore(X, y, w)\\n1: input: Feature matrix X, labels y, and weight vector w\\n2: initialization: s− = m− and s+ = 0 and l = 0m and c = Xw − 1\\n2 y\\n3: π ← {1, . . . , m} sorted in ascending order of c\\n4: for i = 1 to m do\\n5: if yπi = −1 then\\n6: lπi ← s+ and s− ← s− − 1\\n7: else\\n8: lπi ← −s− and s+ ← s+ + 1\\n9: end if\\n10: end for\\n11: Rescale l ← l/(m+m−) and compute r = ⟨l, c⟩ and g = l⊤X.\\n12: return Risk r and subgradient g\\nthat yi = −1 and yj = 1. It follows that ci − cj = 1 + fi − fj. The eﬃcient\\nalgorithm is due to the observation that there are at most m distinct terms\\nck, k = 1, . . . , m, each with diﬀerent frequency lk and sign, appear in (3.18).\\nThese frequencies lk can be determined by ﬁrst sorting c in ascending order\\nthen scanning through the labels according to the sorted order of c and\\nkeeping running statistics such as the number s− of negative labels yet to\\nencounter, and the number s+ of positive labels encountered. When visiting\\nyk, we know ck should appears s+ (or s−) times with positive (or negative)\\nsign in (3.18) if yk = −1 (or yk = 1). Algorithm 3.2 spells out explicitly how\\nto compute Remp(w) and its subgradient.\\nA3.1.3.2 Ordinal Regression\\nEssentially the same preference relationships need to hold for ordinal re-\\ngression. The only diﬀerence is that yi need not take on binary values any\\nmore. Instead, we may have an arbitrary number of diﬀerent values yi (e.g.,\\n1 corresponding to ’strong reject’ up to 10 corresponding to ’strong accept’,\\nwhen it comes to ranking papers for a conference). That is, we now have\\nyi ∈ {1, . . . , n} rather than yi ∈ {±1}. Our goal is to ﬁnd some w such that\\n⟨w, xi − xj⟩ < 0 whenever yi < y j. Whenever this relationship is not satis-\\nﬁed, we incur a cost C(yi, yj) for preferring xi to xj. For examples, C(yi, yj)\\ncould be constant i.e., C(yi, yj) = 1 [Joa06] or linear i.e., C(yi, yj) = yj − yi.\\nDenote by mi the number of xj for which yj = i. In this case, there are\\n¯M = m2 − ∑n\\ni=1 m2\\ni pairs (yi, yj) for which yi ̸= yj; this implies that there\\nare M = ¯M /2 pairs ( yi, yj) such that yi < y j. Normalizing by the total', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a9ffb64a-7cb4-4b2d-8edf-1b1edb4c4d61', embedding=None, metadata={'page_label': '211', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 211\\nnumber of comparisons we may write the overall cost of the estimator as\\n1\\nM\\n∑\\nyi<yj\\nC(yi, yj)I(⟨w, xi⟩ > ⟨w, xj⟩) where M = 1\\n2\\n[\\nm2 −\\nn∑\\ni\\nm2\\ni\\n]\\n. (3.19)\\nUsing the same convex majorization as above when we were maximizing the\\nROC score, we obtain an empirical risk of the form\\nRemp(w) = 1\\nM\\n∑\\nyi<yj\\nC(yi, yj) max(0, 1 + ⟨w, xi − xj⟩) (3.20)\\nNow the goal is to ﬁnd an eﬃcient algorithm for obtaining the number of\\ntimes when the individual losses are nonzero such as to compute both the\\nvalue and the gradient of Remp(w). The complication arises from the fact\\nthat observations xi with label yi may appear in either side of the inequality\\ndepending on whether yj < y i or yj > y i. This problem can be solved as\\nfollows: sort f = Xw in ascending order and traverse it while keeping track\\nof how many items with a lower value yj are no more than 1 apart in terms\\nof their value of fi. This way we may compute the count statistics eﬃciently.\\nAlgorithm 3.3 describes the details, generalizing the results of [Joa06]. Again,\\nits runtime is O(m log m), thus allowing for eﬃcient computation.\\nA3.1.3.3 Preference Relations\\nIn general, our loss may be described by means of a set of preference relations\\nj ⪰ i for arbitrary pairs ( i, j) ∈ { 1, . . . m}2 associated with a cost C(i, j)\\nwhich is incurred whenever i is ranked above j. This set of preferences may\\nor may not form a partial or a total order on the domain of all observations.\\nIn these cases eﬃcient computations along the lines of Algorithm 3.3 exist.\\nIn general, this is not the case and we need to rely on the fact that the set\\nP containing all preferences is suﬃciently small that it can be enumerated\\neﬃciently. The risk is then given by\\n1\\n|P |\\n∑\\n(i,j)∈P\\nC(i, j)I(⟨w, xi⟩ > ⟨w, xj⟩) (3.21)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='f6d6ffcb-bb67-40f6-b792-58548bdf349a', embedding=None, metadata={'page_label': '212', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='212 3 Loss Functions\\nAlgorithm 3.3 OrdinalRegression(X, y, w, C)\\n1: input: Feature matrix X, labels y, weight vector w, and score matrix C\\n2: initialization: l = 0n and ui = mi ∀i ∈ [n] and r = 0 and g = 0m\\n3: Compute f = Xw and set c = [ f − 1\\n2 , f + 1\\n2] ∈ R2m (concatenate the\\nvectors)\\n4: Compute M = (m2 − ∑n\\ni=1 m2\\ni )/2\\n5: Rescale C ← C/M\\n6: π ← {1, . . . ,2m} sorted in ascending order of c\\n7: for i = 1 to 2m do\\n8: j = πi mod m\\n9: if πi ≤ m then\\n10: for k = 1 to yj − 1 do\\n11: r ← r − C(k, yj)ukcj\\n12: gj ← gj − C(k, yj)uk\\n13: end for\\n14: lyj ← lyj + 1\\n15: else\\n16: for k = yj + 1 to n do\\n17: r ← r + C(yj, k)lkcj+m\\n18: gj ← gj + C(yj, k)lk\\n19: end for\\n20: uyj ← uyj − 1\\n21: end if\\n22: end for\\n23: g ← g⊤X\\n24: return: Risk r and subgradient g\\nAgain, the same majorization argument as before allows us to write a convex\\nupper bound\\nRemp(w) = 1\\n|P |\\n∑\\n(i,j)∈P\\nC(i, j) max (0, 1 + ⟨w, xi⟩ − ⟨w, xj⟩) (3.22)\\nwhere ∂wRemp(w) = 1\\n|P |\\n∑\\n(i,j)∈P\\nC(i, j)\\n{\\n0 if ⟨w, xj − xi⟩ ≥ 1\\nxi − xj otherwise\\n(3.23)\\nThe implementation is straightforward, as given in Algorithm 3.4.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='cdd09e54-aedc-4f69-822b-5a391dfa052d', embedding=None, metadata={'page_label': '213', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 213\\nAlgorithm 3.4 Preference(X, w, C, P)\\n1: input: Feature matrix X, weight vector w, score matrix C, and prefer-\\nence set P\\n2: initialization: r = 0 and g = 0m\\n3: Compute f = Xw\\n4: while (i, j) ∈ P do\\n5: if fj − fi < 1 then\\n6: r ← r + C(i, j)(1 + fi − fj)\\n7: gi ← gi + C(i, j) and gj ← gj − C(i, j)\\n8: end if\\n9: end while\\n10: g ← g⊤X\\n11: return Risk r and subgradient g\\nA3.1.3.4 Ranking\\nIn webpage and document ranking we are often in a situation similar to that\\ndescribed in Section A3.1.3.2, however with the diﬀerence that we do not\\nonly care about objects xi being ranked according to scores yi but moreover\\nthat diﬀerent degrees of importance are placed on diﬀerent documents.\\nThe information retrieval literature is full with a large number of diﬀer-\\nent scoring functions. Examples are criteria such as Normalized Discounted\\nCumulative Gain (NDCG) , Mean Reciprocal Rank (MRR), Precision@n, or\\nExpected Rank Utility (ERU). They are used to address the issue of evaluat-\\ning rankers, search engines or recommender sytems [Voo01, JK02, BHK98,\\nBH04]. For instance, in webpage ranking only the ﬁrst k retrieved docu-\\nments that matter, since users are unlikely to look beyond the ﬁrst k, say\\n10, retrieved webpages in an internet search. [LS07] show that these scores\\ncan be optimized directly by minimizing the following loss:\\nl(X, y, w) = max\\nπ\\n∑\\ni\\nci\\n⟨\\nw, xπ(i) − xi\\n⟩\\n+ ⟨a − a(π), b(y)⟩ . (3.24)\\nHere ci is a monotonically decreasing sequence, the documents are assumed\\nto be arranged in order of decreasing relevance, π is a permutation, the\\nvectors a and b(y) depend on the choice of a particular ranking measure, and\\na(π) denotes the permutation of a according to π. Pre-computing f = Xw\\nwe may rewrite (3.24) as\\nl(f, y) = max\\nπ\\n[\\nc⊤f(π) − a(π)⊤b(y)\\n]\\n− c⊤f + a⊤b(y) (3.25)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='170a9e0f-7e03-47d4-90d4-6ae5f4660f2d', embedding=None, metadata={'page_label': '214', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='214 3 Loss Functions\\nAlgorithm 3.5 Ranking(X, y, w)\\n1: input: Feature matrix X, relevances y, and weight vector w\\n2: Compute vectors a and b(y) according to some ranking measure\\n3: Compute f = Xw\\n4: Compute elements of matrix Cij = cifj − biaj\\n5: π = LinearAssignment(C)\\n6: r = c⊤(f(π) − f) + (a − a(π))⊤b\\n7: g = c(π−1) − c and g ← g⊤X\\n8: return Risk r and subgradient g\\nand consequently the derivative of l(X, y, w) with respect to w is given by\\n∂wl(X, y, w) = (c(¯π−1) − c)⊤X where ¯π = argmax\\nπ\\nc⊤f(π) − a(π)⊤b(y).\\n(3.26)\\nHere π−1 denotes the inverse permutation, such thatπ◦π−1 = 1. Finding the\\npermutation maximizing c⊤f(π) − a(π)⊤b(y) is a linear assignment problem\\nwhich can be easily solved by the Hungarian Marriage algorithm, that is,\\nthe Kuhn-Munkres algorithm.\\nThe original papers by [Kuh55] and [Mun57] implied an algorithm with\\nO(m3) cost in the number of terms. Later, [Kar80] suggested an algorithm\\nwith expected quadratic time in the size of the assignment problem (ignor-\\ning log-factors). Finally, [OL93] propose a linear time algorithm for large\\nproblems. Since in our case the number of pages is fairly small (in the order\\nof 50 to 200 per query) the scaling behavior per query is not too important.\\nWe used an existing implementation due to [JV87].\\nNote also that training sets consist of a collection of ranking problems,\\nthat is, we have several ranking problems of size 50 to 200. By means of\\nparallelization we are able to distribute the work onto a cluster of worksta-\\ntions, which is able to overcome the issue of the rather costly computation\\nper collection of queries. Algorithm 3.5 spells out the steps in detail.\\nA3.1.3.5 Contingency Table Scores\\n[Joa05] observed that Fβ scores and related quantities dependent on a con-\\ntingency table can also be computed eﬃciently by means of structured es-\\ntimation. Such scores depend in general on the number of true and false\\npositives and negatives alike. Algorithm 3.6 shows how a corresponding em-\\npirical risk and subgradient can be computed eﬃciently. As with the pre-\\nvious losses, here again we use convex majorization to obtain a tractable\\noptimization problem.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ebbd4be7-2152-4762-8d74-9f8acd84705f', embedding=None, metadata={'page_label': '215', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 215\\nGiven a set of labels y and an estimate y′, the numbers of true positives\\n(T+), true negatives ( T−), false positives ( F+), and false negatives ( F−) are\\ndetermined according to a contingency table as follows:\\ny > 0 y < 0\\ny′ > 0 T+ F+\\ny′ < 0 F− T−\\nIn the sequel, we denote by m+ = T+ + F− and m− = T− + F+ the numbers\\nof positives and negative labels in y, respectively. We note that Fβ score can\\nbe computed based on the contingency table [Joa05] as\\nFβ(T+, T−) = (1 + β2)T+\\nT+ + m− − T− + β2m+\\n. (3.27)\\nIf we want to use⟨w, xi⟩ to estimate the label of observationxi, we may use\\nthe following structured loss to “directly” optimize w.r.t. Fβ score [Joa05]:\\nl(X, y, w) = max\\ny′\\n[\\n(y′ − y)⊤f + ∆(T+, T−)\\n]\\n, (3.28)\\nwhere f = Xw, ∆( T+, T−) := 1 − Fβ(T+, T−), and ( T+, T−) is determined\\nby using y and y′. Since ∆ does not depend on the speciﬁc choice of ( y, y′)\\nbut rather just on which sets they disagree, l can be maximized as follows:\\nEnumerating all possible m+m− contingency tables in a way such that given\\na conﬁguration ( T+, T−), T+ (T−) positive (negative) observations xi with\\nlargest (lowest) value of ⟨w, xi⟩ are labeled as positive (negative). This is\\neﬀectively implemented as a nested loop hence run inO(m2) time. Algorithm\\n3.6 describes the procedure in details.\\nA3.1.4 Vector Loss Functions\\nNext we discuss “vector” loss functions, i.e., functions where w is best de-\\nscribed as a matrix (denoted by W ) and the loss depends on W x. Here, we\\nhave feature vector x ∈ Rd, label y ∈ Rk, and weight matrix W ∈ Rd×k. We\\nalso denote feature matrix X ∈ Rm×d as a matrix of m feature vectors xi,\\nand stack up the columns Wi of W as a vector w.\\nSome of the most relevant cases are multiclass classiﬁcation using both\\nthe exponential families model and structured estimation, hierarchical mod-\\nels, i.e., ontologies, and multivariate regression. Many of those cases are\\nsummarized in Table A3.1.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0b1f3891-0575-4304-863e-31c5b84e2cfb', embedding=None, metadata={'page_label': '216', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='216 3 Loss Functions\\nAlgorithm 3.6 Fβ(X, y, w)\\n1: input: Feature matrix X, labels y, and weight vector w\\n2: Compute f = Xw\\n3: π+ ← {i : yi = 1} sorted in descending order of f\\n4: π− ← {i : yi = −1} sorted in ascending order of f\\n5: Let p0 = 0 and pi = 2 ∑m+\\nk=i fπ+\\nk\\n, i = 1, . . . , m+\\n6: Let n0 = 0 and ni = 2 ∑m−\\nk=i fπ−\\nk\\n, i = 1, . . . , m−\\n7: y′ ← −y and r ← −∞\\n8: for i = 0 to m+ do\\n9: for j = 0 to m− do\\n10: rtmp = ∆(i, j) − pi + nj\\n11: if rtmp > r then\\n12: r ← rtmp\\n13: T+ ← i and T− ← j\\n14: end if\\n15: end for\\n16: end for\\n17: y′\\nπ+\\ni\\n← 1, i = 1, . . . , T+\\n18: y′\\nπ−\\ni\\n← −1, i = 1, . . . , T−\\n19: g ← (y′ − y)⊤X\\n20: return Risk r and subgradient g\\nA3.1.4.1 Unstructured Setting\\nThe simplest loss is multivariate regression, wherel(x, y, W) = 1\\n2(y−x⊤W )⊤M(y−\\nx⊤W ). In this case it is clear that by pre-computing XW subsequent calcu-\\nlations of the loss and its gradient are signiﬁcantly accelerated.\\nA second class of important losses is given by plain multiclass classiﬁcation\\nproblems, e.g., recognizing digits of a postal code or categorizing high-level\\ndocument categories. In this case, φ(x, y) is best represented by ey ⊗x (using\\na linear model). Clearly we may view ⟨w, φ(x, y)⟩ as an operation which\\nchooses a column indexed by y from xW , since all labels y correspond to\\na diﬀerent weight vector Wy. Formally we set ⟨w, φ(x, y)⟩ = [xW ]y. In this\\ncase, structured estimation losses can be rewritten as\\nl(x, y, W) = max\\ny′\\nΓ(y, y′)\\n⟨\\nWy′ − Wy, x\\n⟩\\n+ ∆(y, y′) (3.29)\\nand ∂W l(x, y, W) = Γ(y, y∗)(ey∗ − ey) ⊗ x. (3.30)\\nHere Γ and ∆ are deﬁned as in Section A3.1.2 and y∗ denotes the value of y′', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7b8f841a-1a82-4a05-80f7-b1f6e08a27b1', embedding=None, metadata={'page_label': '217', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 217\\nfor which the RHS of (3.29) is maximized. This means that for unstructured\\nmulticlass settings we may simply compute xW . Since this needs to be per-\\nformed for all observations xi we may take advantage of fast linear algebra\\nroutines and compute f = XW for eﬃciency. Likewise note that comput-\\ning the gradient over m observations is now a matrix-matrix multiplication,\\ntoo: denote by G the matrix of rows of gradients Γ( yi, y∗\\ni )(ey∗\\ni − eyi). Then\\n∂W Remp(X, y, W) = G⊤X. Note that G is very sparse with at most two\\nnonzero entries per row, which makes the computation of G⊤X essentially\\nas expensive as two matrix vector multiplications. Whenever we have many\\nclasses, this may yield signiﬁcant computational gains.\\nLog-likelihood scores of exponential families share similar expansions. We\\nhave\\nl(x, y, W) = log\\n∑\\ny′\\nexp\\n⟨\\nw, φ(x, y′)\\n⟩\\n− ⟨w, φ(x, y)⟩ = log\\n∑\\ny′\\nexp\\n⟨\\nWy′, x\\n⟩\\n− ⟨Wy, x⟩\\n(3.31)\\n∂W l(x, y, W) =\\n∑\\ny′(ey′ ⊗ x) exp\\n⟨\\nWy′, x\\n⟩\\n∑\\ny′ exp\\n⟨\\nWy′, x\\n⟩ − ey ⊗ x. (3.32)\\nThe main diﬀerence to the soft-margin setting is that the gradients are\\nnot sparse in the number of classes. This means that the computation of\\ngradients is slightly more costly.\\nA3.1.4.2 Ontologies\\nFig. A3.1. Two ontologies. Left: a binary hierarchy with internal nodes {1, . . . ,7}\\nand labels {8, . . .15}. Right: a generic directed acyclic graph with internal nodes\\n{1, . . . ,6, 12} and labels {7, . . . ,11, 13, . . . ,15}. Note that node 5 has two parents,\\nnamely nodes 2 and 3. Moreover, the labels need not be found at the same level of\\nthe tree: nodes 14 and 15 are one level lower than the rest of the nodes.\\nAssume that the labels we want to estimate can be found to belong to\\na directed acyclic graph. For instance, this may be a gene-ontology graph', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='03def7cc-b061-484b-8806-8d76014f0c95', embedding=None, metadata={'page_label': '218', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='218 3 Loss Functions\\n[ABB+00] a patent hierarchy [CH04], or a genealogy. In these cases we have a\\nhierarchy of categories to which an element x may belong. Figure A3.1 gives\\ntwo examples of such directed acyclic graphs (DAG). The ﬁrst example is\\na binary tree, while the second contains nodes with diﬀerent numbers of\\nchildren (e.g., node 4 and 12), nodes at diﬀerent levels having children (e.g.,\\nnodes 5 and 12), and nodes which have more than one parent (e.g., node 5).\\nIt is a well known fundamental property of trees that they have at most as\\nmany internal nodes as they have leaf nodes.\\nIt is now our goal to build a classiﬁer which is able to categorize observa-\\ntions according to which leaf node they belong to (each leaf node is assigned\\na label y). Denote by k + 1 the number of nodes in the DAG including the\\nroot node. In this case we may design a feature map φ(y) ∈ Rk [CH04] by\\nassociating with every label y the vector describing the path from the root\\nnode to y, ignoring the root node itself. For instance, for the ﬁrst DAG in\\nFigure A3.1 we have\\nφ(8) = (1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0) and φ(13) = (0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0)\\nWhenever several paths are admissible, as in the right DAG of Figure A3.1\\nwe average over all possible paths. For example, we have\\nφ(10) = (0.5, 0.5, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0) and φ(15) = (0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1).\\nAlso note that the lengths of the paths need not be the same ( e.g., to\\nreach 15 it takes a longer path than to reach 13). Likewise, it is natural to\\nassume that ∆(y, y′), i.e., the cost for mislabeling y as y′ will depend on the\\nsimilarity of the path. In other words, it is likely that the cost for placing\\nx into the wrong sub-sub-category is less than getting the main category of\\nthe object wrong.\\nTo complete the setting, note that for φ(x, y) = φ(y) ⊗ x the cost of\\ncomputing all labels is k inner products, since the value of ⟨w, φ(x, y)⟩ for a\\nparticular y can be obtained by the sum of the contributions for the segments\\nof the path. This means that the values for all terms can be computed by\\na simple breadth ﬁrst traversal through the graph. As before, we may make\\nuse of vectorization in our approach, since we may compute xW ∈ Rk to\\nobtain the contributions on all segments of the DAG before performing the\\ngraph traversal. Since we have m patterns xi we may vectorize matters by\\npre-computing XW .\\nAlso note that φ(y)−φ(y′) is nonzero only for those edges where the paths\\nfor y and y′ diﬀer. Hence we only change weights on those parts of the graph\\nwhere the categorization diﬀers. Algorithm 3.7 describes the subgradient and\\nloss computation for the soft-margin type of loss function.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c8cc4470-9edc-4b9f-ab53-793e4ef58030', embedding=None, metadata={'page_label': '219', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A3.1 Loss Functions 219\\nAlgorithm 3.7 Ontology(X, y, W)\\n1: input: Feature matrix X ∈ Rm×d, labels y, and weight matrix W ∈\\nRd×k\\n2: initialization: G = 0 ∈ Rm×k and r = 0\\n3: Compute f = XW and let fi = xiW\\n4: for i = 1 to m do\\n5: Let Di be the DAG with edges annotated with the values of fi\\n6: Traverse Di to ﬁnd node y∗ that maximize sum of fi values on the\\npath plus ∆( yi, y′)\\n7: Gi = φ(y∗) − φ(yi)\\n8: r ← r + zy∗ − zyi\\n9: end for\\n10: g = G⊤X\\n11: return Risk r and subgradient g\\nThe same reasoning applies to estimation when using an exponential fam-\\nilies model. The only diﬀerence is that we need to compute a soft-max\\nover paths rather than exclusively choosing the best path over the ontol-\\nogy. Again, a breadth-ﬁrst recursion suﬃces: each of the leaves y of the\\nDAG is associated with a probability p(y|x). To obtain Ey∼p(y|x) [φ(y)] all\\nwe need to do is perform a bottom-up traversal of the DAG summing over\\nall probability weights on the path. Wherever a node has more than one\\nparent, we distribute the probability weight equally over its parents.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2b6089cf-8f66-4a2e-80af-410f8e02ad1c', embedding=None, metadata={'page_label': '220', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=None, image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b972e864-6757-499d-97e5-56ee26978a76', embedding=None, metadata={'page_label': '221', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bibliography\\n[ABB+00] M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. M.\\nCherry, A. P. Davis, K. Dolinski, S. S. Dwight, J. T. Eppig, M. A. Harris,\\nD. P. Hill, L. Issel-Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E. Richard-\\nson, M. Ringwald, G. M. Rubin, and G. Sherlock, Gene ontology: tool for the\\nuniﬁcation of biology. the gene ontology consortium , Nat Genet 25 (2000), 25–\\n29.\\n[AGML90] S. F. Altschul, W. Gish, E. W. Myers, and D. J. Lipman, Basic local\\nalignment search tool , Journal of Molecular Biology 215 (1990), no. 3, 403–\\n410.\\n[BBL05] O. Bousquet, S. Boucheron, and G. Lugosi, Theory of classiﬁcation: a sur-\\nvey of recent advances, ESAIM: Probab. Stat. 9 (2005), 323– 375.\\n[BCR84] C. Berg, J. P. R. Christensen, and P. Ressel, Harmonic analysis on semi-\\ngroups, Springer, New York, 1984.\\n[BDEL03] S. Ben-David, N. Eiron, and P.M. Long, On the diﬃculty of approximately\\nmaximizing agreements, J. Comput. System Sci. 66 (2003), no. 3, 496–514.\\n[Bel61] R. E. Bellman, Adaptive control processes , Princeton University Press,\\nPrinceton, NJ, 1961.\\n[Bel05] Alexandre Belloni, Introduction to bundle methods , Tech. report, Operation\\nResearch Center, M.I.T., 2005.\\n[Ber85] J. O. Berger, Statistical decision theory and Bayesian analysis , Springer,\\nNew York, 1985.\\n[BH04] J. Basilico and T. Hofmann, Unifying collaborative and content-based ﬁlter-\\ning, Proc. Intl. Conf. Machine Learning (New York, NY), ACM Press, 2004,\\npp. 65–72.\\n[BHK98] J. S. Breese, D. Heckerman, and C. Kardie, Empirical analysis of predictive\\nalgorithms for collaborative ﬁltering , Proceedings of the 14th Conference on\\nUncertainty in Artiﬁcial Intelligence, 1998, pp. 43–52.\\n[BHS+07] G. Bakir, T. Hofmann, B. Sch¨ olkopf, A. Smola, B. Taskar, and S. V. N.\\nVishwanathan, Predicting structured data , MIT Press, Cambridge, Mas-\\nsachusetts, 2007.\\n[Bil68] Patrick Billingsley, Convergence of probability measures , John Wiley and\\nSons, 1968.\\n[Bis95] C. M. Bishop, Neural networks for pattern recognition , Clarendon Press,\\nOxford, 1995.\\n[BK07] R. M. Bell and Y. Koren, Lessons from the netﬂix prize challenge , SIGKDD\\nExplorations 9 (2007), no. 2, 75–79.\\n[BKL06] A. Beygelzimer, S. Kakade, and J. Langford, Cover trees for nearest neigh-\\nbor, International Conference on Machine Learning, 2006.\\n[BL00] J. M. Borwein and A. S. Lewis, Convex analysis and nonlinear optimization:\\nTheory and examples , CMS books in Mathematics, Canadian Mathematical\\nSociety, 2000.\\n221', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d7aabf67-69f4-4086-ac13-80385207a4e9', embedding=None, metadata={'page_label': '222', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='222 3 Bibliography\\n[BM92] K. P. Bennett and O. L. Mangasarian, Robust linear programming discrimi-\\nnation of two linearly inseparable sets, Optim. Methods Softw. 1 (1992), 23–34.\\n[BNJ03] D. Blei, A. Ng, and M. Jordan, Latent Dirichlet allocation, Journal of Ma-\\nchine Learning Research 3 (2003), 993–1022.\\n[BT03] D.P. Bertsekas and J.N. Tsitsiklis, Introduction to probability, Athena Sci-\\nentiﬁc, 2003.\\n[BV04] S. Boyd and L. Vandenberghe, Convex optimization, Cambridge University\\nPress, Cambridge, England, 2004.\\n[CDLS99] R. Cowell, A. Dawid, S. Lauritzen, and D. Spiegelhalter, Probabilistic\\nnetworks and expert sytems , Springer, New York, 1999.\\n[CH04] Lijuan Cai and T. Hofmann, Hierarchical document categorization with sup-\\nport vector machines, Proceedings of the Thirteenth ACM conference on Infor-\\nmation and knowledge management (New York, NY, USA), ACM Press, 2004,\\npp. 78–87.\\n[Cra46] H. Cram´ er,Mathematical methods of statistics , Princeton University Press,\\n1946.\\n[Cre93] N. A. C. Cressie, Statistics for spatial data, John Wiley and Sons, New York,\\n1993.\\n[CS03] K. Crammer and Y. Singer, Ultraconservative online algorithms for multi-\\nclass problems, Journal of Machine Learning Research 3 (2003), 951–991.\\n[CSS00] M. Collins, R. E. Schapire, and Y. Singer, Logistic regression, AdaBoost\\nand Bregman distances , Proc. 13th Annu. Conference on Comput. Learning\\nTheory, Morgan Kaufmann, San Francisco, 2000, pp. 158–169.\\n[CV95] Corinna Cortes and V. Vapnik, Support vector networks, Machine Learning\\n20 (1995), no. 3, 273–297.\\n[DG03] S. Dasgupta and A. Gupta, An elementary proof of a theorem of johnson\\nand lindenstrauss, Random Struct. Algorithms 22 (2003), no. 1, 60–65.\\n[DG08] J. Dean and S. Ghemawat, MapReduce: simpliﬁed data processing on large\\nclusters, CACM 51 (2008), no. 1, 107–113.\\n[DGL96] L. Devroye, L. Gy¨ orﬁ, and G. Lugosi, A probabilistic theory of pattern\\nrecognition, Applications of mathematics, vol. 31, Springer, New York, 1996.\\n[Fel71] W. Feller, An introduction to probability theory and its applications , 2 ed.,\\nJohn Wiley and Sons, New York, 1971.\\n[FJ95] A. Frieze and M. Jerrum, An analysis of a monte carlo algorithm for esti-\\nmating the permanent , Combinatorica 15 (1995), no. 1, 67–83.\\n[FS99] Y. Freund and R. E. Schapire, Large margin classiﬁcation using the percep-\\ntron algorithm, Machine Learning 37 (1999), no. 3, 277–296.\\n[FT94] L. Fahrmeir and G. Tutz, Multivariate statistical modelling based on gener-\\nalized linear models, Springer, 1994.\\n[GIM99] A. Gionis, P. Indyk, and R. Motwani, Similarity search in high dimensions\\nvia hashing, Proceedings of the 25th VLDB Conference (Edinburgh, Scotland)\\n(M. P. Atkinson, M. E. Orlowska, P. Valduriez, S. B. Zdonik, and M. L. Brodie,\\neds.), Morgan Kaufmann, 1999, pp. 518–529.\\n[GS04] T.L. Griﬃths and M. Steyvers, Finding scientiﬁc topics , Proceedings of the\\nNational Academy of Sciences 101 (2004), 5228–5235.\\n[GW92] P. Groeneboom and J. A. Wellner, Information bounds and nonparametric\\nmaximum likelihood estimation, DMV, vol. 19, Springer, 1992.\\n[Hal92] P. Hall, The bootstrap and edgeworth expansions, Springer, New York, 1992.\\n[Hay98] S. Haykin, Neural networks : A comprehensive foundation , Macmillan, New\\nYork, 1998, 2nd edition.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e9df8aa3-aaee-4c5f-9813-5de3addb2d86', embedding=None, metadata={'page_label': '223', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bibliography 223\\n[Heb49] D. O. Hebb, The organization of behavior, John Wiley and Sons, New York,\\n1949.\\n[Hoe63] W. Hoeﬀding, Probability inequalities for sums of bounded random variables,\\nJournal of the American Statistical Association 58 (1963), 13–30.\\n[HUL93] J.B. Hiriart-Urruty and C. Lemar´ echal,Convex analysis and minimization\\nalgorithms, I and II , vol. 305 and 306, Springer-Verlag, 1993.\\n[IM98] P. Indyk and R. Motawani, Approximate nearest neighbors: Towards remov-\\ning the curse of dimensionality , Proceedings of the 30th Symposium on Theory\\nof Computing, 1998, pp. 604–613.\\n[JK02] K. Jarvelin and J. Kekalainen, IR evaluation methods for retrieving highly\\nrelevant documents, ACM Special Interest Group in Information Retrieval (SI-\\nGIR), New York: ACM, 2002, pp. 41–48.\\n[Joa05] T. Joachims, A support vector method for multivariate performance mea-\\nsures, Proc. Intl. Conf. Machine Learning (San Francisco, California), Morgan\\nKaufmann Publishers, 2005, pp. 377–384.\\n[Joa06] , Training linear SVMs in linear time , Proc. ACM Conf. Knowledge\\nDiscovery and Data Mining (KDD), ACM, 2006.\\n[Jor08] M. I. Jordan, An introduction to probabilistic graphical models , MIT Press,\\n2008, To Appear.\\n[JV87] R. Jonker and A. Volgenant, A shortest augmenting path algorithm for dense\\nand sparse linear assignment problems , Computing 38 (1987), 325–340.\\n[Kar80] R.M. Karp, An algorithm to solve the m × n assignment problem in expected\\ntime O(mn log n), Networks 10 (1980), no. 2, 143–152.\\n[KD05] S. S. Keerthi and D. DeCoste, A modiﬁed ﬁnite Newton method for fast\\nsolution of large scale linear SVMs , J. Mach. Learn. Res. 6 (2005), 341–361.\\n[Kel60] J. E. Kelly, The cutting-plane method for solving convex programs , Journal\\nof the Society for Industrial and Applied Mathematics8 (1960), no. 4, 703–712.\\n[Kiw90] Krzysztof C. Kiwiel, Proximity control in bundle methods for convex non-\\ndiﬀerentiable minimization, Mathematical Programming 46 (1990), 105–122.\\n[KM00] Paul Komarek and Andrew Moore, A dynamic adaptation of AD-trees for\\neﬃcient machine learning on large data sets , Proc. Intl. Conf. Machine Learn-\\ning, Morgan Kaufmann, San Francisco, CA, 2000, pp. 495–502.\\n[Koe05] R. Koenker, Quantile regression, Cambridge University Press, 2005.\\n[Kuh55] H.W. Kuhn, The Hungarian method for the assignment problem , Naval Re-\\nsearch Logistics Quarterly 2 (1955), 83–97.\\n[Lew98] D. D. Lewis, Naive (Bayes) at forty: The independence assumption in in-\\nformation retrieval, Proceedings of ECML-98, 10th European Conference on\\nMachine Learning (Chemnitz, DE) (C. N´ edellec and C. Rouveirol, eds.), no.\\n1398, Springer Verlag, Heidelberg, DE, 1998, pp. 4–15.\\n[LK03] C. Leslie and R. Kuang, Fast kernels for inexact string matching , Proc.\\nAnnual Conf. Computational Learning Theory, 2003.\\n[LMP01] J. D. Laﬀerty, A. McCallum, and F. Pereira, Conditional random ﬁelds:\\nProbabilistic modeling for segmenting and labeling sequence data , Proceedings\\nof International Conference on Machine Learning (San Francisco, CA), vol. 18,\\nMorgan Kaufmann, 2001, pp. 282–289.\\n[LNN95] Claude Lemar´ echal, Arkadii Nemirovskii, and Yurii Nesterov,New variants\\nof bundle methods , Mathematical Programming 69 (1995), 111–147.\\n[LS07] Q. Le and A.J. Smola, Direct optimization of ranking measures , J. Mach.\\nLearn. Res. (2007), submitted.\\n[LT92] Z. Q. Luo and P. Tseng, On the convergence of coordinate descent method', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='fe870c73-e700-4bc0-9c2e-8c89a57c806d', embedding=None, metadata={'page_label': '224', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='224 3 Bibliography\\nfor convex diﬀerentiable minimization , Journal of Optimization Theory and\\nApplications 72 (1992), no. 1, 7–35.\\n[Lue84] D. G. Luenberger, Linear and nonlinear programming, second ed., Addison-\\nWesley, Reading, May 1984.\\n[Mar61] M.E. Maron, Automatic indexing: An experimental inquiry , Journal of the\\nAssociation for Computing Machinery 8 (1961), 404–417.\\n[McA07] David McAllester, Generalization bounds and consistency for structured\\nlabeling, Predicting Structured Data (Cambridge, Massachusetts), MIT Press,\\n2007.\\n[McD89] C. McDiarmid, On the method of bounded diﬀerences , Survey in Combina-\\ntorics, Cambridge University Press, 1989, pp. 148–188.\\n[Mit97] T. M. Mitchell, Machine learning, McGraw-Hill, New York, 1997.\\n[MN83] P. McCullagh and J. A. Nelder, Generalized linear models, Chapman and\\nHall, London, 1983.\\n[MSR+97] K.-R. M¨ uller, A. J. Smola, G. R¨ atsch, B. Sch¨ olkopf, J. Kohlmorgen, and\\nV. Vapnik, Predicting time series with support vector machines, Artiﬁcial Neu-\\nral Networks ICANN’97 (Berlin) (W. Gerstner, A. Germond, M. Hasler, and\\nJ.-D. Nicoud, eds.), Lecture Notes in Comput. Sci., vol. 1327, Springer-Verlag,\\n1997, pp. 999–1004.\\n[Mun57] J. Munkres, Algorithms for the assignment and transportation problems ,\\nJournal of SIAM 5 (1957), no. 1, 32–38.\\n[MYA94] N. Murata, S. Yoshizawa, and S. Amari, Network information criterion —\\ndetermining the number of hidden units for artiﬁcial neural network models ,\\nIEEE Transactions on Neural Networks 5 (1994), 865–872.\\n[Nad65] E. A. Nadaraya, On nonparametric estimates of density functions and re-\\ngression curves, Theory of Probability and its Applications10 (1965), 186–190.\\n[NW99] J. Nocedal and S. J. Wright, Numerical optimization , Springer Series in\\nOperations Research, Springer, 1999.\\n[OL93] J.B. Orlin and Y. Lee, Quickmatch: A very fast algorithm for the assignment\\nproblem, Working Paper 3547-93, Sloan School of Management, Massachusetts\\nInstitute of Technology, Cambridge, MA, March 1993.\\n[Pap62] A. Papoulis, The fourier integral and its applications , McGraw-Hill, New\\nYork, 1962.\\n[Pla99] J. Platt, Fast training of support vector machines using sequential minimal\\noptimization, Advances in Kernel Methods — Support Vector Learning (Cam-\\nbridge, MA) (B. Sch¨ olkopf, C. J. C. Burges, and A. J. Smola, eds.), MIT Press,\\n1999, pp. 185–208.\\n[PTVF94] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery,\\nNumerical recipes in c. the art of scientiﬁc computation, Cambridge University\\nPress, Cambridge, UK, 1994.\\n[Rao73] C. R. Rao, Linear statistical inference and its applications , John Wiley and\\nSons, New York, 1973.\\n[RBZ06] N. Ratliﬀ, J. Bagnell, and M. Zinkevich, Maximum margin planning, Inter-\\nnational Conference on Machine Learning, July 2006.\\n[Ros58] F. Rosenblatt, The perceptron: A probabilistic model for information storage\\nand organization in the brain , Psychological Review 65 (1958), no. 6, 386–408.\\n[RPB06] M. Richardson, A. Prakash, and E. Brill, Beyond pagerank: machine learn-\\ning for static ranking , Proceedings of the 15th international conference on\\nWorld Wide Web, WWW (L. Carr, D. De Roure, A. Iyengar, C.A. Goble,\\nand M. Dahlin, eds.), ACM, 2006, pp. 707–715.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='be18eca4-26dc-40cf-8b40-8b41e2a6359b', embedding=None, metadata={'page_label': '225', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Bibliography 225\\n[RSS+07] G. R¨ atsch, S. Sonnenburg, J. Srinivasan, H. Witte, K.-R. M¨ uller, R. J.\\nSommer, and B. Sch¨ olkopf,Improving the Caenorhabditis elegans genome an-\\nnotation using machine learning, PLoS Computational Biology 3 (2007), no. 2,\\ne20 doi:10.1371/journal.pcbi.0030020.\\n[Rud73] W. Rudin, Functional analysis, McGraw-Hill, New York, 1973.\\n[Sil86] B. W. Silverman, Density estimation for statistical and data analysis , Mono-\\ngraphs on statistics and applied probability, Chapman and Hall, London, 1986.\\n[SPST+01] B. Sch¨ olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C.\\nWilliamson, Estimating the support of a high-dimensional distribution , Neu-\\nral Comput. 13 (2001), no. 7, 1443–1471.\\n[SS02] B. Sch¨ olkopf and A. Smola, Learning with kernels , MIT Press, Cambridge,\\nMA, 2002.\\n[SW86] G.R. Shorack and J.A. Wellner, Empirical processes with applications to\\nstatistics, Wiley, New York, 1986.\\n[SZ92] Helga Schramm and Jochem Zowe, A version of the bundle idea for minimiz-\\ning a nonsmooth function: Conceptual idea, convergence analysis, numerical\\nresults, SIAM J. Optimization 2 (1992), 121–152.\\n[TGK04] B. Taskar, C. Guestrin, and D. Koller, Max-margin Markov networks ,\\nAdvances in Neural Information Processing Systems 16 (Cambridge, MA)\\n(S. Thrun, L. Saul, and B. Sch¨ olkopf, eds.), MIT Press, 2004, pp. 25–32.\\n[TJHA05] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, Large margin\\nmethods for structured and interdependent output variables , J. Mach. Learn.\\nRes. 6 (2005), 1453–1484.\\n[Vap82] V. Vapnik, Estimation of dependences based on empirical data , Springer,\\nBerlin, 1982.\\n[Vap95] , The nature of statistical learning theory, Springer, New York, 1995.\\n[Vap98] , Statistical learning theory, John Wiley and Sons, New York, 1998.\\n[vdG00] S. van de Geer, Empirical processes in M-estimation, Cambridge University\\nPress, 2000.\\n[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical\\nprocesses, Springer, 1996.\\n[VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func-\\ntion approximation, regression estimation, and signal processing , Advances in\\nNeural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer,\\nM. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281–287.\\n[Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track ,\\nTREC, 2001.\\n[VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and\\ntree matching, Kernel Methods in Computational Biology (Cambridge, MA)\\n(B. Sch¨ olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113–130.\\n[VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels\\non dynamical systems and its application to the analysis of dynamic scenes ,\\nInternational Journal of Computer Vision 73 (2007), no. 1, 95–119.\\n[Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and\\nthe randomized GACV, Tech. Report 984, Department of Statistics, University\\nof Wisconsin, Madison, 1997.\\n[Wat64] G. S. Watson, Smooth regression analysis, Sankhya A 26 (1964), 359–372.\\n[Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression\\nto linear prediction and beyond , Learning and Inference in Graphical Models\\n(M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599–621.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8228dd7e-a990-4557-82c4-a9b1ae5bebd0', embedding=None, metadata={'page_label': '226', 'file_name': 'ML_BOOK.pdf', 'file_path': '/content/data/ML_BOOK.pdf', 'file_type': 'application/pdf', 'file_size': 10811677, 'creation_date': '2026-01-11', 'last_modified_date': '2026-01-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='226 3 Bibliography\\n[WJ03] M. J. Wainwright and M. I. Jordan, Graphical models, exponential fami-\\nlies, and variational inference , Tech. Report 649, UC Berkeley, Department of\\nStatistics, September 2003.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "settingup prompt"
      ],
      "metadata": {
        "id": "ZpHVa3SYIead"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "#specified to StableLM\n",
        "system_prompt=\"\"\"<|SYSTEM|># You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "# this will wrap the default prompts that are internal to llama-index\n",
        "\n",
        "query_wrapper_prompt=PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "PsbJOGBPIPPm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm=HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522,
          "referenced_widgets": [
            "6043ffa6f2ab48a5a3ad300068105335",
            "d18a3ae0b97047e38f2863d274daca0b",
            "a7256eeeb67045d2922786c603d45c8b",
            "2618025450084e509d50862e59e72a19",
            "a18f832d261d41f1885c8a218f8ad770",
            "6d6507c379c640ac98e75c250cda13c8",
            "c2e7c9ac43a042958ee56f1de3f21385",
            "9ba2ba1047bd44688a22667b81a014fc",
            "d55369c1412f4fa1801b37527db501a5",
            "23307169928a449784a016eaf431c415",
            "4c7992507637491a8ab5d25ac07e4ca2",
            "8a8e5618eecd492c808172c6c1c640a1",
            "7cd1eb3e1bc84b56a411d102aefa6ec6",
            "883a8185c6f94d209437eac8fbbfe4ab",
            "595d8eaa84934e8e9e3bad3052d033bf",
            "d9a9f1481ede4f4bbe5030fc0cdf4211",
            "bdc9e746098a4436adff87a5ea616b9e",
            "d0e6b5b191c94e96848bed3feebb5a99",
            "567209646e0d42b7951ce4c6dd836510",
            "aaceec3c36014c6dbba82ba34ff08c9b",
            "a94970801e4b45448a15f4248d65d933",
            "da33d451ce5c44f7a54a65d2d4144908",
            "f2459babf83b4c84aa7d3c90d627e8c8",
            "e9e60e37fad548cbb9ec7eaa6e5d98f8",
            "2a2ce18dc13145c2a5c6e64553685cab",
            "f88735c3408248a0a6fe7c5e6dc5a25e",
            "57974709fcea45e48601a6ddd0d1c4a2",
            "29f18a52e5204e1b802f8ad1a51fcd16",
            "1a55ea9e611543afbe9e98032bef97fc",
            "224176adcd834b9b9845876be6ef555a",
            "e0af84fcfd4545a19a875a01325a365b",
            "1630dc722e1c4d0da1f4fd05877eaff4",
            "b59dcc2bb3284301826f8c4ad998c55f",
            "639e3877b74d42f59d0b4df122a979bf",
            "5c80824fd0b346249169a006a1bb2cee",
            "e9024ace610a4c7ab15fae19b2a1ab44",
            "aa735e30b818407eaea6e2bf6230403b",
            "6ba9e8e14fcc42f5b0e66edfb0ed6a84",
            "676cfc531f1947029bd6c92f2efbedc5",
            "f4cb87f786964e9988db38b37db8d8ec",
            "9b210cfcd23941f8a3c709c6de6e6ca0",
            "f9339162d01d48aca659ec5cdcb2ffa4",
            "4ae8cac6d3c44fd2b23b12483c8edf74",
            "78c35b36bbf84ad3bdd9cacb2c03d241",
            "d5acf88c93a748219ac421085c13f913",
            "7e1937a443994d50833ee50ceb0d4358",
            "ef0a4658c3634e52bc415ead83ce94e2",
            "38ac0c8e3c3943709f2dbc5959338d6b",
            "6a128243b98949b48612784b2d560a53",
            "b16f1b6e88284adcb0913d425ef6ce18",
            "15c05ae077c9410dbd9955713cf45df8",
            "850a804e1fc64213bf41abb78258115d",
            "60fdc94e59db480ebf339d7dce0a5a3f",
            "c9da93537920449bbe1e49cf3b5af756",
            "dcea51bf756d4b54bb0fac398d19141d",
            "8c122a3b87d3431dad1c6f43efc30322",
            "9657d916144d48aa9b17a507eea9b610",
            "b19b133a21134bb5a9c1a6ec247119d6",
            "18671052a3f4421eafac92a6b4319350",
            "f7a357c5dffe4d8cb54e38729483f0db",
            "e683e6a06065445583630cfa6b4cf952",
            "719183f142b146589ce96aac5033c3ee",
            "311f9567f81e44119a429e9a4cfe25f1",
            "16296035f7b0493b927ca6836ea6a82d",
            "9e5e5e4dc58a43919e46091c429918e8",
            "a81454f0306e4b8f9876d10af5768edd",
            "65340f070f784a128101dd47ad3ae9c7",
            "168ab778127948cdaf7cfe718db69f39",
            "7a1592477df341e886aca61d86c99deb",
            "0e244e864864430ea2447b812619019f",
            "ba1e63005a664d78b71b22c1033c75d9",
            "de36d6f95fa54f06a6cb39c5029de8dd",
            "506d9085b76c4b5a8bf7da8c0fdeed65",
            "2e84362f0bc440dcb51137a0148e89fe",
            "d9f05872ad0642f2bd6998e453a93a99",
            "5a59b613e2d04ad5a7a066002f8a9c64",
            "1924b2578d3d47749360271052438df6",
            "88f19c79637a443d8a6b790974958a4f",
            "62ed5ea610bb43cb9fc88891f51373e1",
            "9a3ae63f05084815a48e40692a3369f1",
            "11421bb6a3384f2d961222829f30fc30",
            "e7a910d531744c1fa5884e63b69e0de3",
            "37d219ff50544173a7d1e081909d4cef",
            "807e36083aea4e9c883e46031ef83b82",
            "608e707293b94936bf3cddecf295c38c",
            "0c1ba02856a542ec990d13c2e47498ea",
            "132582e0dcab47b4ab3e73c9b098b833",
            "355ed904ea21466e8c155723f5cdc311",
            "76782c67287246cd9a562e503ddad9ba",
            "e72423206da64e7c92a713bc07723a6e",
            "01d9567baf934666ac4dffd4db977a93",
            "45a05dc84fdc4e80adabe821b3df0481",
            "fdbc66200d6648dca5d885ce81440b46",
            "18f304a5b8224d788fdbdea6989432b6",
            "0d94b83621cc4decbda7e4f8611d19bc",
            "6f8ea1be3b6049c19210188f70e72a69",
            "8703a3374f5744c181d3e53d4c9000f8",
            "87c59cbb62d74d14bebac4110d468edd",
            "94b626c9e40e441ca04c817ff29177fc",
            "bca6fe1cf4b74881b7d8e544111c14ed",
            "bd506f76c1e540d39da476a48b33c580",
            "24e0ae4b5eec4cbda2b5d2fe4321fe1c",
            "b3e28db757174eb684fd8e7efb08d800",
            "0822ae4ed4004e2eb3b63e703eab355c",
            "df555693bb7543a79c782196e3436646",
            "18f1eb2fe90b4d549a7559db0c306ceb",
            "2e66948afeb5490eb2cfa524dda37440",
            "8c104aab5af4402bbe7c16540d75dcc2",
            "1d5cf23d437b45be86b9f62b04c62a67",
            "0c6479cf0fb645f0b55ff5aace7f5b6d",
            "ced1ebb520fa4c4d89b6bedad107079a",
            "cd692fcfec224a36a15235eb7855a610",
            "67b45ed95f5b4e4da67e188c75744342",
            "ad58218dd13944d6810c3c6ae9dd4155",
            "bacbaa4e2e714e07af6d927f45f024b1",
            "e59b20e5734d438b887cbdbae93f06ec",
            "c32b13f27cf74db393110ea6575dd94c",
            "015bc83851a145adab8632a384ab5d8f",
            "d33463937cff44859dbe569e177ed5d4",
            "67ac20160f9d4861af76fb7b7a712601",
            "9e325ad79ca547f787c0e0af4b6538c0",
            "0f1d9b9c40bb409292955c6e708d3904",
            "7447fb6f6d5b410892fb8c74d8513a12",
            "0285370657fe4681942e71261375fb09",
            "51c20d9ad9434ad2ab399cfb83053924",
            "11c145705dc847b8b42d7811fe010f2a",
            "b0f96627a49240559332ed17c8fa8f7b",
            "64c9231e207946b88339de5709b4b655",
            "16b4517823cc494cb82bace769028153",
            "e56f204259524e42ac5f60bc7f4d5bab",
            "4b9b7bca847b453790a0020e049b6eb0",
            "e4d089d56b7f42e7a163160296b8b3cf"
          ]
        },
        "id": "HweOrN0-KPKE",
        "outputId": "1fac4fa5-7f58-429c-ff35-a7dc1407eaea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6043ffa6f2ab48a5a3ad300068105335"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a8e5618eecd492c808172c6c1c640a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2459babf83b4c84aa7d3c90d627e8c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "639e3877b74d42f59d0b4df122a979bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5acf88c93a748219ac421085c13f913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c122a3b87d3431dad1c6f43efc30322"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65340f070f784a128101dd47ad3ae9c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88f19c79637a443d8a6b790974958a4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76782c67287246cd9a562e503ddad9ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bca6fe1cf4b74881b7d8e544111c14ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ced1ebb520fa4c4d89b6bedad107079a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f1d9b9c40bb409292955c6e708d3904"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install llama-index-embeddings-huggingface\n",
        "!pip -q install llama-index-embeddings-instructor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxYyYuyXO05H",
        "outputId": "8208c073-3825-46ce-d1fa-2c1198a3d387"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model=HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "b3973c01a57447ed9660d7d9bb2413d3",
            "a3a50d230ae941c3885148d6ec2019b2",
            "6353a302641a45e982e3d3848caf4106",
            "9d7d2bcd1b8b4530b29102bb5532d47a",
            "7e12f512d1f94ec2a336814970fa38fd",
            "e48f6246dff34bec83e3df5e24aa027e",
            "5a653789570045e8a03c4ec35da718fa",
            "4484e0facf0e4c50868da6810d6cff84",
            "44f1c86d56514dfa97fe955769d949a5",
            "fb084473efa24e63882e38c0af878695",
            "ed1fc82c50694edab347212c7635f39a",
            "ae26dced3a5548d39ba69e065af441e2",
            "9093c104c06148f5b8b6e4682807f0ea",
            "5c093517caf54a84a02cf95748f4ceea",
            "4f2173651ecd4029bc32d51d781c5348",
            "a0dd545ab44441a28b4ec16c3301eb88",
            "b547134f5e084a38b2f8033586dc7e79",
            "aa8f3bb523b74387b2d585195d919cad",
            "da740c823b0641d1be23bb32d345d419",
            "19e3b79802474bcd86a83e4ff754bca0",
            "687fcafca1bf4ac5b5a0b557b60da8ec",
            "10e9224ffc2e4565bbfa2de9de90c630",
            "3da587300f4940c4a66d7325a5f95b49",
            "bf3b62bae959489aac9793380ba3614a",
            "196904ae6ee840b29e0ebc85bd811659",
            "6a13e8ff6e71499982ca1b9a8672a9bc",
            "aa48eab05038435d806484c99da0d356",
            "6fb3de034dc64f398facaadc3436d798",
            "3338873f604643eb80499c7f9f79fe88",
            "18aa3fee76024a5b9b9869b1cc18b534",
            "a79b5c032c5f4c33b6967d000272170b",
            "5c462fdf61994cd19417b73beb6838f5",
            "d2faac9491514a67abc4743ad38c1f74",
            "cf2ed2cd25854448810707ec7f6d9a40",
            "b0d53cf7229344fd8b792a54216bff2c",
            "76daa0d016ea4ab0a47606b3836ecc63",
            "2a475635a39449069951a013a97146fb",
            "0f8dadcdf43346d4a21b8df8d17d2fcf",
            "bf26e12b02524e6daa7d00e2379c7ceb",
            "cc87fbb9695949c8984b239098744eb7",
            "6e791aeb10404d609dd41225ad66a52d",
            "b6bfdc9a19014270837825474a9976dd",
            "68d8a101980c4884871e6f6fd18a2c85",
            "e46dee9e372142b18b82fd046e0111d6",
            "6c59e0c5252d470abc52ec9a8e8e26d6",
            "bc01d3b85b644983b0299b17dd70245a",
            "e491114d304c416985cc69f82d6ed372",
            "cd8f386f675b490a811433b1ed1f081b",
            "c1d542cbeb55480c909074020c855d53",
            "943dd0e12bf640508e24656ad4c09567",
            "7545e5c277a64343a6887ae7e31755fe",
            "f23b9413d75f438fbe99f994fc5f38f8",
            "43306786899949568fc1f3fab1937a74",
            "999b7dbb1e984bf599b9ca61d544c3b7",
            "86805974a3014142beb8722ee5275f00",
            "8a614ccfa91341e9a84d4eb8eb729e5a",
            "042b01523bf04b0994496851da44fe5d",
            "e5b1325017db46df8bcf7595a7903686",
            "86d2a54faca44cf79704c23c7145ec62",
            "eafd889c6f2345abb97fbbd3bde29fa1",
            "a7bdeabb41b64f2ba8bd09970bc8ee8c",
            "f58b659655864ddab0872529780541dc",
            "2ce9588ea1304a1db8516db0090c63c1",
            "ac1a2d57e5fa478495439b682c00013d",
            "cdcce0b5e1024711b762286a2da83c04",
            "aff19a689acc4c8db3ad7cca00f7f6eb",
            "573273300d724377a36e16212c9cd4cd",
            "82216a2363cb4504a55555787dc8dcb3",
            "db8da59bf83c416c991f43671d781919",
            "94b3d0a1353e49588d3be2c524323877",
            "6f9dbc7474e548b59c901532b30c1e9f",
            "718eb3c2b2a446d084c24164c6259e2f",
            "d240d9d07a774a0d9bb7bbdb046eeb3b",
            "bdf17064a97c47b5b2123392caa0fd98",
            "99ab01ea46c44db4a73542e65b854dae",
            "d79bb992b1c440e4b2c4c72f5b3c411b",
            "dad130c526c4403398ab0a2e5d71f2c2",
            "6f34910d602c479a8484bf1a5d9d0113",
            "6f63e6d06533484d8f76eb42852f2590",
            "4d8d6dfa9bf24b3c9864d1468ad3dd03",
            "da6a65010e2c476cad1456107c132c68",
            "25b1fbda2e5a409ba5acec9489db01d4",
            "50aed7a1ef924745890c7aef53af0981",
            "48b583c01d694eb6b2b6ef3cbebad4d3",
            "fb30dc65ba734064b6178a32e0e33a8b",
            "19f2010b3bc8430cb85a452d9da3226b",
            "df251bbe9cba418982a33242562de55d",
            "1c527b17989f451eb4e1578dbd2f5839",
            "ae4e5e9a193e463482532bb1af8c00bd",
            "c3388d6fad8549fc94dd61a2cfd9045e",
            "183930ba19a64f42837696dddde01018",
            "a2579bf967ee4c53ba11cf67a5893a4a",
            "6c05b55ae017465896ed4bd15ef72add",
            "d918093beabf4f2e84c6e98f49a5c107",
            "eb79ba32b1894c3da7aa07f92b5ab379",
            "329e29298e894a489184b998366b111d",
            "35e427376b6d432296c4f05f98c2ded1",
            "8d2e3a40a0c44d28a4a4f25e6e545c20",
            "34f868c064b34782a7bbbbf626b6797f",
            "11922234783749d7b2a36e925ee2f89d",
            "3989b32b1abe49be9fe4bbbf2e67c2cc",
            "d4829b6ccd504335a49e324c660ca550",
            "2d1e8d6fe526426abb406f94f3cd797a",
            "130490a83e5c44518358ddd13e605516",
            "f0da2dc4edf24c7cb6f22ab5f521ce84",
            "ce61cc1255114d48878004659869c866",
            "01b7cae0ab5b4b11ae01f92677e99f90",
            "9b0369e857524a4c90966366a895d4b7",
            "ce13e5d099114d47806cce68e287480d",
            "007edcf55ea640cb870af9670f13b48e",
            "3ea65a4afc7a46c09efce8543a904bc0",
            "b3b9e3b110c04e33a78a1c7eab875db9",
            "8f10f05b98e140958085970081f9119d",
            "94dab4e7b0434d4cb5434848078d40c2",
            "56107c13ef954b8998184c26c1154af2",
            "0ab46dedc77c4c2d979e47c0e6baf73b",
            "17af80fb096a4b36aca8b214391bcb59",
            "1ede9794af0248aaa799c478b51ecf40",
            "26d839ae9d28467a80b850068fd98e0c",
            "c962bd576d814609b98fb749ff2e74c0",
            "1c31b50964dd4f4d8f90ce72758849f0"
          ]
        },
        "id": "NnbCkNB1O0rL",
        "outputId": "79f662be-c36a-4dc6-8f65-e8806f6dfd23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3973c01a57447ed9660d7d9bb2413d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae26dced3a5548d39ba69e065af441e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3da587300f4940c4a66d7325a5f95b49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf2ed2cd25854448810707ec7f6d9a40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c59e0c5252d470abc52ec9a8e8e26d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a614ccfa91341e9a84d4eb8eb729e5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "573273300d724377a36e16212c9cd4cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f34910d602c479a8484bf1a5d9d0113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae4e5e9a193e463482532bb1af8c00bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11922234783749d7b2a36e925ee2f89d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ea65a4afc7a46c09efce8543a904bc0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "#from llama_index.core.node_parser import SentenceSplitter\n",
        "#from llama_index.llms.openai import OpenAI\n",
        "\n",
        "#Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "#Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "#Settings.num_output = 512\n",
        "#Settings.context_window = 3900"
      ],
      "metadata": {
        "id": "GlPxx0uSRSZT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "Settings.llm = llm#OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = embed_model# OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
        "Settings.num_output = 512\n",
        "Settings.context_window = 3900"
      ],
      "metadata": {
        "id": "HvGfs7BYSq8f"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# old one\n",
        "#from llama_index.core import VectorStoreIndex, ServiceContext\n",
        "# SeerviceContext is a simple class used to initialize variables\n",
        "#service_context=ServiceContext.from_defaults(\n",
        " #   chunk_size=1024,\n",
        "  #  llm=llm,\n",
        "   # embed_model=embed_model\n",
        "#)"
      ],
      "metadata": {
        "id": "1Sk-GZOzO0eb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "3xsjpOtLO0a6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine()"
      ],
      "metadata": {
        "id": "rUGVmsyDO0Xz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_engine.query(\"What is Machine Learning?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAJZko0GWoC3",
        "outputId": "836ad89b-5603-48f2-8faa-1174a0d0241d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning is a broad subfield of artificial intelligence that is concerned with the design and development of algorithms and techniques that allow computer systems to learn and improve automatically through experience. This learning can be based on data, without being explicitly programmed. The context provided suggests that Machine Learning is a topic that has been studied extensively, with various books, journals, and resources available online.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_engine.query(\"Explain why do we have to do feature engineering?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4JcwSriWvxr",
        "outputId": "b8ad5c9d-2b98-4de3-bfd1-9f2feac03c7d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the context provided, feature engineering is necessary to deal with various factors that can affect the performance of machine learning models, such as different lighting conditions, facial expressions, whether a person is wearing glasses, hairstyle, etc., in face recognition applications. By learning which features are relevant for identifying a person, the system can provide more accurate results. Similarly, in named entity recognition, feature engineering helps in identifying entities such as places, titles, names, actions, etc. from documents, which is crucial in the automatic digestion and understanding of documents. In both cases, using examples of marked-up documents or instances to learn such dependencies automatically is more efficient than using hand-crafted rules, especially when deploying the system in many languages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QIYQYfPX1a9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}